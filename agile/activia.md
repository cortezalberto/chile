# Marco de Trabajo IA-Native para el Desarrollo de Software

---

## Índice General

### PARTE I: FUNDAMENTOS Y CONTEXTO
- [Introducción: Una Transformación Estructural](#introducción-una-transformación-estructural)
- [El Impacto Real de la IA Agente en el Desarrollo de Software](#el-impacto-real-de-la-ia-agente-en-el-desarrollo-de-software)
- [Del Agile Clásico a los Entornos IA-Native](#del-agile-clásico-a-los-entornos-ia-native)
- [Por Qué Agile No Alcanza en Entornos IA-Native](#por-qué-agile-no-alcanza-en-entornos-ia-native)
- [De Tareas a Decisiones: Un Cambio de Unidad Fundamental](#de-tareas-a-decisiones-un-cambio-de-unidad-fundamental)
- [El Policy Ticket como Instrumento Central](#el-policy-ticket-como-instrumento-central)
- [Del Uso de Herramientas al Trabajo con Agentes](#del-uso-de-herramientas-al-trabajo-con-agentes)
- [La Realidad de las Software Factories en 2026](#-la-realidad-de-las-software-factories-en-2026-)

### PARTE II: AGENTES DE IA - ANATOMÍA Y ORQUESTACIÓN
- [Anatomía de los Agentes de IA: Comprendiendo lo que Orquestamos](#-anatomía-de-los-agentes-de-ia-comprendiendo-lo-que-orquestamos-)
- [Orquestación de Agentes: Herramientas y Frameworks para el Programador de 2026](#-orquestación-de-agentes-herramientas-y-frameworks-para-el-programador-de-2026-)

### PARTE III: MARCO DE GOBERNANZA
- [Principios Fundamentales del Marco IA-Native](#-principios-fundamentales-del-marco-ia-native-)
- [De Tareas a Decisiones Gobernadas: El Cambio Conceptual](#de-tareas-a-decisiones-gobernadas-el-cambio-conceptual)
- [El Policy Ticket: Instrumento Operativo Central](#el-policy-ticket-instrumento-operativo-central)
- [POLICY-TICKET-TEMPLATE: Template Operativo Estandarizado](#-policy-ticket-template-template-operativo-estandarizado-)
- [Sistema de Skills de Gobernanza](#-sistema-de-skills-de-gobernanza-)
- [Arquitectura de Documentación por Capas](#-arquitectura-de-documentación-por-capas-)
- [Meta-Instrumentos de Gobernanza](#-meta-instrumentos-de-gobernanza-)
- [Sincronización Automática de Gobernanza](#-sincronización-automática-de-gobernanza-)
- [Un Marco para Empresas Reales](#un-marco-para-empresas-reales)
- [Matriz de Overhead Operativo por Nivel de Riesgo](#-matriz-de-overhead-operativo-por-nivel-de-riesgo-)

### PARTE IV: TRANSFORMACIÓN DE ROLES
- [Transformación de Roles y Responsabilidades](#transformación-de-roles-y-responsabilidades)
- [Transformación de Roles en la Práctica Diaria](#transformación-de-roles-en-la-práctica-diaria)
- [El Product Owner en el Marco IA-Native: De Priorizador de Valor a Gobernador de Riesgo](#-el-product-owner-en-el-marco-ia-native-de-priorizador-de-valor-a-gobernador-de-riesgo-)
- [El Tech Lead en el Marco IA-Native: De Revisor de Código a Diseñador de Límites Cognitivos](#-el-tech-lead-en-el-marco-ia-native-de-revisor-de-código-a-diseñador-de-límites-cognitivos-)
- [El Scrum Master en el Marco IA-Native: De Facilitador de Ceremonias a Guardián del Sistema Socio-Técnico](#-el-scrum-master-en-el-marco-ia-native-de-facilitador-de-ceremonias-a-guardián-del-sistema-socio-técnico-)
- [El QA Lead en el Marco IA-Native: De Detector de Bugs a Diseñador de Sistemas de Verificación](#-el-qa-lead-en-el-marco-ia-native-de-detector-de-bugs-a-diseñador-de-sistemas-de-verificación-)
- [El Programador en 2026: De Escritor de Código a Arquitecto de Decisiones](#-el-programador-en-2026-de-escritor-de-código-a-arquitecto-de-decisiones-)
- [El DBA en el Marco IA-Native: De Administrador de Bases de Datos a Arquitecto de Integridad de Información](#-el-dba-en-el-marco-ia-native-de-administrador-de-bases-de-datos-a-arquitecto-de-integridad-de-información-)

### PARTE V: ADOPCIÓN Y VALIDACIÓN
- [Caso de Uso: Adopción en una Software Factory Real](#caso-de-uso-adopción-en-una-software-factory-real)
- [Transformación del Backlog: De User Stories a Policy Tickets](#transformación-del-backlog-de-user-stories-a-policy-tickets)
- [Simulación de un Sprint IA-Native Completo](#simulación-de-un-sprint-ia-native-completo)
- [De la Teoría a la Operación: Guías Prácticas](#de-la-teoría-a-la-operación-guías-prácticas)
- [Validación y Evaluación del Marco en la Práctica](#validación-y-evaluación-del-marco-en-la-práctica)
- [Discusión Crítica: Riesgos, Límites y Trade-offs](#discusión-crítica-riesgos-límites-y-trade-offs)
- [Proyección en Contextos Educativos y Formación Profesional](#proyección-en-contextos-educativos-y-formación-profesional)
- [Casos de Estudio: Implementaciones Reales del Framework](#-casos-de-estudio-implementaciones-reales-del-framework-)
- [Plan de Rollout Detallado: 90 Días hacia la Adopción](#-plan-de-rollout-detallado-90-días-hacia-la-adopción-)
- [Gestión de la Resistencia al Cambio](#-gestión-de-la-resistencia-al-cambio-)
- [Dashboard y Métricas de Adopción del Framework](#-dashboard-y-métricas-de-adopción-del-framework-)

### PARTE VI: CONTEXTO ORGANIZACIONAL Y LEGAL
- [Marco Regulatorio y Compliance](#-marco-regulatorio-y-compliance-)
- [Propiedad Intelectual del Código Generado por IA](#-propiedad-intelectual-del-código-generado-por-ia-)
- [Ética, Sesgo y Fairness en el Desarrollo IA-Native](#-ética-sesgo-y-fairness-en-el-desarrollo-ia-native-)
- [Economía de Tokens y FinOps para IA](#-economía-de-tokens-y-finops-para-ia-)
- [Modelo Económico del Framework IA-Native: ROI, TCO y Business Case](#-modelo-económico-del-framework-ia-native-roi-tco-y-business-case-)

### PARTE VII: CALIDAD Y MÉTRICAS
- [Testing y QA de Código Generado por IA](#-testing-y-qa-de-código-generado-por-ia-)
- [KPIs y Métricas para Entornos IA-Native](#-kpis-y-métricas-para-entornos-ia-native-)
- [Casos de Fallo y Anti-patrones](#-casos-de-fallo-y-anti-patrones-)

### PARTE VIII: CONFIGURACIÓN Y HERRAMIENTAS
- [Configuración Multi-Agente bajo Marco IA-Native](#-configuración-multi-agente-bajo-marco-ia-native-)
- [Integración con Herramientas Existentes](#-integración-con-herramientas-existentes-)
- [GitHub como Sistema Nervioso del Framework](#github-como-sistema-nervioso-del-framework)

### PARTE IX: INGENIERÍA DE PROMPTS
- [Ingeniería de Prompts: El Lenguaje de la Delegación Cognitiva](#-ingeniería-de-prompts-el-lenguaje-de-la-delegación-cognitiva-)

### PARTE X: CICLO DE VIDA DE DESARROLLO IA-NATIVE (SDLC)
- [Visión General del SDLC IA-Native](#-visión-general-del-sdlc-ia-native-)
- [Análisis de Requisitos con IA](#-análisis-de-requisitos-con-ia-)
- [Planificación y Estimación con IA](#-planificación-y-estimación-con-ia-)
- [Arquitectura de Software IA-Native](#-arquitectura-de-software-ia-native-)
- [Desarrollo Frontend con IA](#-desarrollo-frontend-con-ia-)
- [Desarrollo Backend con IA](#-desarrollo-backend-con-ia-)
- [Aplicaciones Móviles con IA](#-aplicaciones-móviles-con-ia-)
- [Optimización de APIs y Performance](#-optimización-de-apis-y-performance-)
- [DevOps y Despliegue IA-Native](#-devops-y-despliegue-ia-native-)
- [Buenas Prácticas y Patrones de Diseño](#-buenas-prácticas-y-patrones-de-diseño-)
- [Pull Requests y Code Review IA-Native](#-pull-requests-y-code-review-ia-native-)

### PARTE XI: WORKFLOW COMPLETO DE IMPLEMENTACIÓN IA-NATIVE
- [Guía Maestra: De Requisitos a Producción](#guía-maestra-de-requisitos-a-producción)
- [Fase 1: Captura y Validación de Requisitos](#fase-1-captura-y-validación-de-requisitos)
- [Fase 2: Análisis, Planificación y Estimación](#fase-2-análisis-planificación-y-estimación)
- [Fase 3: Arquitectura y Diseño Técnico](#fase-3-arquitectura-y-diseño-técnico)
- [Fase 4: Desarrollo](#fase-4-desarrollo)
- [Fase 5: Testing e Integración](#fase-5-testing-e-integración)
- [Fase 6: Despliegue y Operación](#fase-6-despliegue-y-operación)
- [Resumen: Documentos por Fase](#resumen-documentos-por-fase)

### PARTE XII: CONCLUSIONES
- [Conclusiones Generales](#conclusiones-generales)
- [Regla Final del Framework](#regla-final-del-framework)

---

# PARTE I: FUNDAMENTOS Y CONTEXTO

## Introducción: Una Transformación Estructural

El desarrollo de software atraviesa en la actualidad una transformación que no puede describirse adecuadamente como una simple evolución tecnológica. No se trata de mejores lenguajes, frameworks más expresivos o herramientas de automatización más eficientes. El cambio que se está produciendo es de naturaleza estructural: por primera vez desde el origen de la disciplina, una parte sustantiva del trabajo intelectual involucrado en la construcción de software deja de ser ejecutada exclusivamente por personas y comienza a ser realizada por sistemas artificiales con capacidad de acción.

Durante décadas, la ingeniería de software se organizó alrededor de un supuesto implícito pero central: el software es diseñado, escrito, modificado y validado por seres humanos. Todas las metodologías, prácticas y marcos de trabajo, desde los modelos más rígidos hasta las metodologías ágiles, descansan sobre ese supuesto. Incluso cuando se incorporaron herramientas de automatización, estas siempre actuaron como extensiones pasivas del programador: compiladores, linters, generadores de código, frameworks, pipelines de integración continua. En todos los casos, la decisión última y la responsabilidad recaían en una persona.

Ese supuesto comienza a romperse cuando la inteligencia artificial deja de ser una herramienta asistencial y pasa a comportarse como un agente. Un agente no solo sugiere: ejecuta. No solo completa líneas de código: modifica estructuras, refactoriza módulos completos, genera tests, produce documentación, propone cambios arquitectónicos y, en algunos casos, integra directamente sus resultados en repositorios productivos. Esta capacidad de acción altera profundamente el equilibrio tradicional entre velocidad, control y responsabilidad.

En este nuevo escenario, el cuello de botella del desarrollo ya no está en la escritura del código. La velocidad de generación se ha acelerado artificialmente hasta un punto en el que el problema principal pasa a ser otro: cómo controlar lo que se genera, cómo verificarlo, cómo integrarlo de forma segura y, sobre todo, cómo asumir responsabilidad por decisiones que ya no fueron tomadas línea por línea por un humano.

Este marco de trabajo surge precisamente para responder a ese problema.

---

## Del Agile Clásico a los Entornos IA-Native

Las metodologías ágiles fueron una respuesta brillante a un problema real de su tiempo: cómo organizar equipos humanos para construir software en contextos de cambio, incertidumbre y presión por entregar valor de manera incremental. Scrum, Kanban, XP y sus múltiples derivaciones introdujeron conceptos que hoy resultan casi obvios, pero que en su momento fueron disruptivos: ciclos cortos, feedback frecuente, colaboración interdisciplinaria, adaptación continua.

Sin embargo, todas estas metodologías comparten una característica fundamental: están diseñadas para coordinar personas. Las historias de usuario son relatos aspiracionales escritos para humanos. Las tareas se asignan a individuos. Las ceremonias están pensadas como espacios de sincronización cognitiva y social. El control de calidad ocurre, en gran medida, a posteriori: revisiones de código, testing manual o semiautomático, retrospectivas narrativas.

Cuando se introduce inteligencia artificial con capacidad de ejecución directa, este esquema comienza a mostrar sus límites. No porque Agile sea incorrecto, sino porque responde a una ontología distinta del trabajo. Agile asume que el error humano es inevitable pero gestionable, que la velocidad está limitada por la capacidad cognitiva de las personas y que la responsabilidad puede distribuirse colectivamente dentro de un equipo.

Los agentes de IA rompen estos supuestos. No se cansan, no dudan, no comprenden el dominio en sentido humano y pueden producir grandes volúmenes de cambios en muy poco tiempo. Esto genera una asimetría peligrosa: la capacidad de ejecutar crece mucho más rápido que la capacidad de comprender, verificar y asumir consecuencias.

Hablar simplemente de "usar IA en Agile" resulta insuficiente. No alcanza con agregar una herramienta nueva a un proceso viejo. Lo que se requiere es un cambio en la forma misma de concebir el trabajo.

---

## De Tareas a Decisiones: Un Cambio de Unidad Fundamental

Uno de los cambios más profundos que introduce este marco es la redefinición de la unidad básica del trabajo. En los enfoques tradicionales, esa unidad es la tarea o la historia de usuario. Ambas describen algo que debe hacerse, generalmente por una persona o un equipo.

En entornos IA-Native, esa lógica deja de ser suficiente. Cuando una IA puede generar código, refactorizar, escribir tests o modificar configuraciones, el problema ya no es "qué tarea hacer", sino "qué está autorizado a hacer este agente".

La unidad fundamental deja de ser la tarea y pasa a ser la decisión de delegación.

Cada intervención de la IA debe entenderse como una delegación cognitiva explícita: alguien autoriza a un sistema artificial a realizar cierto tipo de acciones, en cierto contexto, bajo ciertos límites y con determinadas obligaciones de evidencia. Esa autorización no puede ser implícita ni informal, porque el costo del error es demasiado alto.

De esta necesidad surge el concepto central del framework: el cambio gobernado.

Un cambio gobernado no es simplemente una modificación del código. Es una modificación que cumple condiciones explícitas de autorización, control y trazabilidad. Si una acción no está gobernada, no es válida, independientemente de que funcione técnicamente.

---

## El Policy Ticket como Instrumento Central

Para materializar este cambio de enfoque, el framework introduce una nueva pieza operativa: el Policy Ticket.

El Policy Ticket no es una tarea ni una historia de usuario en el sentido clásico. Es un contrato operativo que define, de manera explícita, qué se le permite hacer a la inteligencia artificial y bajo qué condiciones. En lugar de describir un resultado deseado, describe un comportamiento autorizado.

Un Policy Ticket incluye, como mínimo: quién asume la responsabilidad humana, cuál es la intención de la intervención, qué acciones están permitidas, qué acciones están explícitamente prohibidas, qué nivel de autonomía se concede, qué evidencias deben producirse, y qué criterios determinan si el cambio puede integrarse.

Este instrumento cumple una función crucial: desplaza el control desde el "después" hacia el "antes". En lugar de confiar en que un problema será detectado en una review tardía, se gobierna la intervención desde el momento en que se autoriza.

---

## Transformación de Roles y Responsabilidades

La adopción de un enfoque IA-Native no solo modifica artefactos y procesos, sino que transforma profundamente los roles dentro de una organización de desarrollo.

El Product Owner, por ejemplo, deja de ser únicamente un priorizador de valor funcional y pasa a desempeñar un rol clave en la gestión del riesgo. Debe decidir no solo qué se construye, sino qué nivel de autonomía se concede a la IA en cada dominio del producto.

El Scrum Master o Agile Coach deja de ser un facilitador de ceremonias y se convierte en un guardián del sistema socio-técnico. Su responsabilidad ya no es solo que el equipo "funcione", sino que el conjunto de humanos, herramientas, agentes y procesos mantenga un equilibrio saludable.

El Tech Lead evoluciona hacia un diseñador de límites cognitivos. Su trabajo consiste en definir guardrails, estándares de evidencia, políticas de integración y criterios técnicos que permitan aprovechar la velocidad de la IA sin perder control.

El rol de QA también se transforma radicalmente. En lugar de detectar errores al final del proceso, pasa a diseñar mecanismos de verificación automática, gates de calidad y sistemas de observabilidad que operan en tiempo real.

Junto a estos roles transformados, comienzan a aparecer otros nuevos: responsables de gobierno de IA, diseñadores de políticas, especialistas en riesgo de modelos, perfiles híbridos entre ingeniería, seguridad y compliance.

---

## Un Marco para Empresas Reales

Este framework no está pensado para entornos ideales ni para equipos experimentales. Está diseñado para software factories reales, con presión comercial, plazos, clientes, deuda técnica y restricciones organizacionales.

Por eso, su adopción no se plantea como un cambio abrupto, sino como un proceso progresivo. Las organizaciones pueden comenzar aplicándolo en dominios de bajo riesgo, como refactors internos, generación de tests o documentación. A medida que adquieren experiencia, pueden extenderlo a áreas más sensibles.

El objetivo no es frenar el uso de la inteligencia artificial, sino hacerlo sostenible. La experiencia demuestra que la adopción descontrolada de IA genera, a mediano plazo, más problemas de los que resuelve: deuda técnica acelerada, pérdida de comprensión del sistema, errores difíciles de rastrear y responsabilidades difusas.

Este marco propone un camino intermedio: aprovechar la potencia de la IA sin renunciar a los principios fundamentales de la ingeniería de software.

---

## El Impacto Real de la IA Agente en el Desarrollo de Software

La incorporación de inteligencia artificial en el desarrollo de software suele describirse, de manera superficial, como una mejora de productividad. Sin embargo, esta caracterización resulta insuficiente para explicar lo que está ocurriendo en la práctica cotidiana de las organizaciones. El cambio que introduce la IA agente no es cuantitativo, sino cualitativo: altera la naturaleza misma del trabajo de desarrollo.

En los modelos tradicionales, incluso en aquellos altamente automatizados, la inteligencia artificial o las herramientas de apoyo operaban como extensiones del programador. Autocompletaban líneas, sugerían fragmentos de código o detectaban errores sintácticos. El control de la acción seguía siendo humano. La IA no ejecutaba cambios relevantes sin mediación explícita.

Los entornos actuales rompen ese límite. Editores AI-first, agentes de refactorización, generadores automáticos de tests, asistentes capaces de recorrer repositorios completos y proponer cambios coherentes en múltiples capas del sistema configuran un escenario nuevo. En este contexto, la IA ya no asiste: actúa.

Esa capacidad de acción tiene consecuencias profundas. La velocidad de producción deja de estar acotada por la escritura manual de código. Un agente puede generar decenas o cientos de modificaciones en el tiempo que antes insumía una sola. Este aumento de velocidad no es neutro: acelera también la propagación de errores, inconsistencias y malas decisiones si no existe un marco de control adecuado.

Otro aspecto crítico es la pérdida progresiva de comprensión global del sistema. Cuando una parte significativa del código es producido o modificado por agentes artificiales, los desarrolladores humanos dejan de tener una relación directa y continua con cada decisión. Esto no implica necesariamente una degradación de la calidad, pero sí introduce una distancia cognitiva que debe ser gestionada.

Finalmente, emerge un problema que las metodologías tradicionales no resuelven: la responsabilidad. Cuando un agente introduce un cambio que genera un fallo en producción, ¿quién es responsable? ¿El programador que aprobó el merge? ¿El líder técnico que habilitó el uso del agente? ¿La organización que definió el proceso? Estas preguntas no pueden responderse de manera informal; requieren un marco explícito.

Esta sección establece el punto de partida del marco de trabajo: la IA agente transforma el desarrollo de software en un sistema socio-técnico más complejo, donde la gobernanza del proceso es tan importante como la calidad del código producido.

---

## Por Qué Agile No Alcanza en Entornos IA-Native

Las metodologías ágiles surgieron como una respuesta pragmática a los límites de los enfoques predictivos y excesivamente planificados del desarrollo de software. Su énfasis en ciclos cortos, feedback temprano, colaboración y adaptación continua permitió a las organizaciones lidiar mejor con la incertidumbre inherente a los productos digitales. Durante años, este enfoque demostró ser eficaz para coordinar equipos humanos y mejorar la calidad de las entregas.

Sin embargo, Agile fue concebido bajo un conjunto de supuestos que hoy comienzan a tensionarse. El más importante de ellos es que el trabajo es ejecutado por personas. Las historias de usuario, las tareas, las estimaciones, las ceremonias y los mecanismos de control están diseñados para gestionar capacidades humanas: atención limitada, comprensión contextual, necesidad de comunicación y responsabilidad individual y colectiva.

Cuando se incorporan agentes de inteligencia artificial con capacidad de acción directa, estos supuestos dejan de ser válidos de manera uniforme. La IA no participa de las ceremonias, no comprende el dominio en términos humanos y no asume responsabilidad. Sin embargo, puede ejecutar trabajo real a una velocidad muy superior a la de cualquier equipo. Esto genera una asimetría que Agile, en su formulación clásica, no modela.

Un ejemplo claro aparece en la historia de usuario tradicional. La fórmula "como usuario quiero X para lograr Y" funciona como un artefacto de comunicación entre personas. No está diseñada para gobernar el comportamiento de un agente artificial. No especifica límites operativos, no clasifica riesgo, no exige evidencia previa ni define quién autoriza explícitamente la acción. En un entorno IA-Native, esta ambigüedad deja de ser tolerable.

Algo similar ocurre con las ceremonias. El planning distribuye tareas humanas; la daily sincroniza estados subjetivos; la review valida resultados visibles; la retrospectiva reflexiona sobre el proceso. Todas estas prácticas asumen que el ritmo de producción está acotado por la capacidad humana y que los errores pueden ser discutidos y corregidos en ciclos posteriores. Cuando la IA introduce decenas de cambios entre una ceremonia y otra, el control a posteriori se vuelve insuficiente.

No se trata de afirmar que Agile "está mal" o que debe ser descartado. Por el contrario, muchas de sus intuiciones siguen siendo valiosas. El problema es que Agile no fue diseñado para gobernar agentes, sino para coordinar personas. Pretender resolver esta brecha simplemente "agregando IA" al proceso conduce, en la práctica, a una pérdida de control progresiva.

En muchos equipos se observa un patrón recurrente: al principio, la IA acelera el desarrollo; luego, comienzan a aparecer inconsistencias, refactors difíciles de entender, tests frágiles y decisiones arquitectónicas poco justificadas. El equipo sigue usando Scrum o Kanban, pero los artefactos dejan de reflejar lo que realmente ocurre. El proceso formal dice una cosa; la práctica real, otra.

Esta brecha entre proceso declarado y proceso efectivo es el síntoma más claro de que el marco metodológico ya no alcanza. No basta con "hacer Agile con IA". Es necesario repensar qué se gobierna, cómo se autoriza el trabajo y dónde se inscribe la responsabilidad cuando la ejecución deja de ser exclusivamente humana.

Esta constatación no invalida las metodologías ágiles, pero sí exige su refactorización profunda. El marco de trabajo IA-Native se apoya en los valores de Agile, pero redefine sus instrumentos fundamentales para hacerlos compatibles con un entorno donde la inteligencia artificial actúa como agente.

---

## Del Uso de Herramientas al Trabajo con Agentes

Durante gran parte de la historia reciente del desarrollo de software, la incorporación de nuevas herramientas no alteró de manera sustantiva la estructura del trabajo. Los lenguajes evolucionaron, los frameworks se volvieron más expresivos y las plataformas de automatización redujeron tareas repetitivas, pero el núcleo del proceso permaneció estable: una persona tomaba decisiones, escribía código y asumía la responsabilidad por los cambios introducidos.

La inteligencia artificial comenzó ingresando a este ecosistema como una herramienta más. Los primeros asistentes de código sugerían fragmentos, completaban líneas o ayudaban a recordar APIs. Su impacto era significativo en términos de productividad individual, pero no cuestionaba la organización del trabajo ni los marcos metodológicos existentes. La IA era, en esencia, un acelerador del programador humano.

Ese escenario cambia de forma radical cuando la IA deja de limitarse a sugerir y pasa a operar como agente. Un agente no espera instrucciones detalladas paso a paso. Recibe una intención general, analiza un contexto más amplio y ejecuta una secuencia de acciones para alcanzar un objetivo. En el desarrollo de software, esto se traduce en sistemas capaces de recorrer un repositorio completo, identificar patrones, proponer refactors, generar tests, modificar configuraciones y producir cambios coherentes en múltiples archivos.

En la práctica diaria, este cambio se manifiesta de varias maneras. En primer lugar, el volumen de cambios crece de forma abrupta. Un desarrollador puede solicitar a un agente que "unifique componentes duplicados" o que "genere tests de regresión", y recibir como resultado decenas de modificaciones distribuidas en distintas capas del sistema. El esfuerzo ya no está en escribir el código, sino en entender qué se cambió y por qué.

En segundo lugar, se modifica la relación temporal con el trabajo. En los flujos tradicionales, los cambios se producen de manera relativamente lineal y predecible. Con agentes de IA, pueden aparecer ráfagas de actividad intensa seguidas de períodos de análisis y validación. Esto desajusta prácticas como la estimación clásica o el seguimiento por horas o puntos de historia, que pierden relevancia frente a métricas de impacto y riesgo.

Otro cambio importante es la naturaleza del error. Cuando una persona se equivoca, el error suele ser local, entendible y contextualizado. Cuando un agente se equivoca, el error puede ser sistemático, replicado en múltiples lugares y difícil de rastrear hasta una decisión concreta. La velocidad que hace a la IA valiosa es la misma que amplifica sus fallos si no existen mecanismos de contención.

Finalmente, se produce un desplazamiento del rol del desarrollador humano. Su trabajo deja de consistir principalmente en producir código y pasa a centrarse en definir intenciones, establecer límites, evaluar resultados y tomar decisiones de integración. El desarrollador se convierte en un supervisor cognitivo, responsable de guiar y controlar la acción de sistemas que operan a una escala distinta.

Este conjunto de transformaciones explica por qué el simple uso de IA como "herramienta más" resulta insuficiente. En un entorno agent-first, el desarrollo de software ya no puede organizarse únicamente alrededor de tareas humanas. Requiere un marco que permita gobernar la acción de los agentes, gestionar el riesgo asociado a su autonomía y mantener la trazabilidad de las decisiones.

La transición de herramientas a agentes marca, así, un punto de no retorno. Ignorar este cambio conduce a procesos desalineados con la realidad operativa. Reconocerlo es el primer paso para construir un marco de trabajo adecuado a los entornos IA-Native.

---

## ★ La Realidad de las Software Factories en 2026: Problemática, Perfiles y Producción ★

> **"Si 2025 fue el año de descubrir que la IA generativa tiene un problema de realización de valor, 2026 es el año de hacer algo al respecto."**

El ecosistema de las software factories atraviesa en 2026 una transformación sin precedentes. Lo que comenzó como la adopción entusiasta de herramientas de asistencia de código se ha convertido en un replanteamiento fundamental del modelo de negocio, las competencias requeridas y la propia naturaleza de la producción de software. Esta sección examina la problemática actual basándose en datos de investigación recientes y proyecciones de la industria.

### La Crisis de Adopción: Entre la Promesa y la Realidad

Los números iniciales parecían prometedores. Para finales de 2025, el 84% de los desarrolladores ya utilizaban o planeaban utilizar herramientas de codificación con IA, y el 51% las usaba diariamente. Microsoft reporta que la IA escribe hasta el 30% de su código, mientras que Google indica proporciones similares del 25%. Sin embargo, detrás de estas cifras de adopción se esconde una realidad más compleja.

La investigación de CodeRabbit reveló una paradoja inquietante: aunque los desarrolladores se mueven más rápido y mejoran su productividad con IA, estos beneficios se ven compensados por el tiempo dedicado a corregir código defectuoso o abordar problemas de seguridad. Los estudios tempranos de GitHub, Google y Microsoft encontraron que los desarrolladores completaban tareas entre un 20% y un 55% más rápido. Sin embargo, un informe de Bain & Company describió los ahorros reales en entornos empresariales como "poco destacables".

El problema central no es la tecnología en sí, sino la brecha entre la capacidad de generación y la capacidad de gobernanza. Las herramientas de codificación con IA están aquí; la gobernanza empresarial no lo está.

### Los Cinco Desafíos Críticos de las Software Factories

**1. La Deuda de Comprensión**

Un fenómeno nuevo emerge con fuerza en 2026: la *deuda de comprensión* (comprehension debt). Se define como el costo futuro que los desarrolladores pagarán por entender, modificar y depurar código que no escribieron, que fue generado por una máquina. La preocupación central es que las ganancias de velocidad inmediatas y medibles a nivel individual están creando un pasivo oculto y compuesto a nivel de sistema y organización.

Las software factories que adoptaron IA agresivamente sin marcos de gobierno están descubriendo que sus bases de código se han convertido en lo que algunos expertos llaman "codebases embrujados": sistemas que funcionan pero que nadie comprende completamente, donde cada modificación introduce riesgos impredecibles.

**2. La Crisis de Seguridad**

Los datos son contundentes: aproximadamente el 45% del código generado por IA contiene vulnerabilidades de seguridad que requieren revisión humana. Investigaciones sugieren que el código generado por IA tiene una tasa de vulnerabilidad cercana al 50%, comparado con el 15-20% del código tradicionalmente escrito por humanos.

Para las software factories, especialmente aquellas que trabajan en sectores regulados como fintech o salud, esta realidad representa un riesgo existencial. El tiempo ahorrado en generación se consume en auditorías, remediación y gestión de incidentes de seguridad.

**3. El Burnout y la Presión Incrementada**

Las expectativas empresariales sobre los desarrolladores han aumentado junto con la adopción creciente de IA. Más de dos tercios de los desarrolladores reportan que la presión para entregar proyectos más rápido se ha intensificado. La paradoja es cruel: la IA se introdujo para aliviar la carga de trabajo, pero ha elevado las expectativas hasta el punto de generar niveles récord de agotamiento profesional.

El burnout se ha convertido en un problema crítico que afecta a los equipos de ingeniería de software en 2026. Las software factories enfrentan tasas de rotación elevadas y dificultades para retener talento senior, precisamente el perfil más necesario para supervisar el trabajo de la IA.

**4. La Erosión de Habilidades Fundamentales**

Las empresas enfrentan dos prioridades contradictorias: innovar con IA y fortalecer sus prácticas de ingeniería de software. Mientras persiguen la ingeniería de software IA-nativa, los equipos enfrentan dificultades como el fracaso en cambiar mentalidades, la erosión de habilidades críticas y fundamentales, y la naturaleza no determinista de los modelos de lenguaje grandes.

Las habilidades que separaban a los grandes desarrolladores en 2025 ahora son obligatorias en 2026: pensamiento de diseño de sistemas, habilidades de depuración que incluyan entender por qué la IA eligió un enfoque particular, y juicio arquitectónico.

**5. La Crisis del Pipeline de Desarrolladores Junior**

Quizás el desafío más preocupante para el futuro de la industria: la contratación de desarrolladores junior ha caído dramáticamente y el aprendizaje tradicional está roto. Los puestos de nivel inicial ahora requieren el nivel de habilidad que solía esperarse de perfiles mid-level.

La investigación de Harvard muestra que las empresas que adoptan IA reducen la contratación de desarrolladores junior entre un 9% y un 10% en seis trimestres. Solo el 7% de las nuevas contrataciones en grandes empresas tecnológicas son graduados recientes, una caída desde el 9.3% en 2023. Las ofertas de pasantías tecnológicas han disminuido un 30% desde 2023. Las grandes empresas tecnológicas contrataron un 50% menos de graduados recientes en los últimos tres años.

Este fenómeno crea un problema sistémico: ¿quién supervisará a la IA en diez años si hoy no se está formando la próxima generación de desarrolladores senior?

### Perfiles Competitivos: Quién Sobrevive en 2026

La industria está experimentando una especialización acelerada. El antiguo perfil de "ingeniero de software genérico" está dando paso a perfiles más definidos. Las software factories competitivas están contratando tres categorías de perfiles:

**Categoría 1: Especialistas con Profundidad Técnica**

Los perfiles en T ganan: experiencia profunda en una especialidad más capacidades amplias en muchas áreas. La IA acelera el aprendizaje horizontal mientras los humanos mantienen profundidad vertical irreemplazable en diseño de sistemas, arquitectura, y saber cuándo la IA está equivocada.

Los roles específicos más demandados incluyen:
- Ingeniero backend cloud-native
- Especialista en plataformas y DevOps
- Ingeniero de IA/ML con capacidad de deployment
- Desarrollador full-stack orientado a producto

**Categoría 2: Ingenieros de IA con Capacidades de Producción**

El 53% de las ofertas de trabajo tecnológico en Estados Unidos requieren habilidades de IA/ML, un aumento desde el 29% en 2024. Las empresas que contratan IA en 2026 no buscan experimentadores; buscan profesionales con capacidades de deployment a nivel de producción.

Las habilidades técnicas clave incluyen:
- Prompt Engineering, LangChain, bases de datos vectoriales
- Diseño de APIs y fine-tuning de LLMs
- Proficiencia en PyTorch, C++, Python y CUDA
- MLOps: el backbone de la IA en producción
- Experiencia con TensorRT, ONNX, TVM para deployment en edge

**Categoría 3: Supervisores Cognitivos y Arquitectos de Decisiones**

Este es el perfil emergente que el marco IA-Native anticipa: profesionales cuyo valor no reside en escribir código sino en:
- Definir políticas y límites para la IA
- Evaluar y validar resultados generados
- Diseñar guardrails y mecanismos de verificación
- Asumir responsabilidad por las decisiones de integración
- Auditar comportamiento de agentes

El 65% de los desarrolladores espera que su rol sea redefinido en 2026, moviéndose de la codificación rutinaria hacia arquitectura, integración y toma de decisiones habilitada por IA.

### Lo que los Empleadores Exigen de los Juniors

Los empleadores ahora esperan que los juniors sean IA-nativos. Los mejores juniors usan la IA como herramienta de aprendizaje, no como muleta: verifican su output, entienden por qué funciona, y aplican ese conocimiento de manera independiente.

Las habilidades que se vuelven obsoletas: codificación mecánica, dominio de sintaxis de lenguajes, depuración rutinaria. Las habilidades que ganan valor: diseño de sistemas, arquitectura, análisis de seguridad, revisión de código, comunicación, expertise de dominio y, crucialmente, entender cuándo la IA está equivocada.

### LinkedIn Jobs on the Rise 2026: Los 25 Empleos de Mayor Crecimiento

El reporte anual de LinkedIn "Jobs on the Rise 2026" proporciona una radiografía precisa de hacia dónde se dirige el mercado laboral tecnológico. Basado en el análisis de millones de empleos iniciados por miembros de LinkedIn entre enero de 2023 y julio de 2025, el informe revela tendencias que confirman y amplifican las transformaciones descritas en este marco.

**El Dominio Absoluto de la IA**

Los empleos relacionados con inteligencia artificial ocupan tres de las cinco primeras posiciones del ranking. Esto indica un momento sostenido tanto en roles técnicos como estratégicos de IA. La economía global ha añadido 1.3 millones de nuevos empleos relacionados con IA en solo dos años, mientras que la demanda de ingenieros de IA y roles centrados en datos continúa dominando las contrataciones.

**El Top 10 de LinkedIn para 2026**

| Ranking | Rol | Habilidades Clave | Modalidad |
|---------|-----|-------------------|-----------|
| **#1** | **Ingeniero de IA** | LangChain, RAG, PyTorch | 26% remoto, 27% híbrido |
| **#2** | **Consultor y Estratega de IA** | Evaluación de casos de uso, MLOps, gestión de riesgo | 30% remoto, 33% híbrido |
| **#3** | Especialista en Ventas de Viviendas Nuevas | — | Principalmente presencial |
| **#4** | **Anotador de Datos** | Etiquetado de datos, preparación de datasets para entrenamiento | 28% remoto, 29% híbrido |
| **#5** | **Investigador de IA/ML** | Deep Learning, PyTorch, Computer Vision | 16% remoto, 24% híbrido |
| **#6** | Especialista en Reembolsos de Salud | — | 42% remoto |
| **#7** | **Fundadores** | Emprendimiento, liderazgo | 51% remoto, 30% híbrido |
| **#8** | Ejecutivos de Ventas | Estrategia comercial | — |
| **#9** | Gerentes de Commissioning | Validación de proyectos de ingeniería | 18% remoto |
| **#10** | Venture Partners | Inversión, asesoría | 8% remoto, 80% híbrido |

**Análisis de los Roles Tecnológicos Clave**

*Ingeniero de IA (#1)*: El rol de mayor crecimiento a nivel global. Los ingenieros de IA desarrollan e implementan modelos que realizan tareas complejas que típicamente requieren toma de decisiones humana, como resolución de problemas y predicción. Las industrias que más contratan son Tecnología, Servicios de IT y Consultoría de Negocios. Los hotspots de contratación son San Francisco, Nueva York y Dallas. Un dato revelador: los principales roles desde los cuales se transiciona hacia Ingeniero de IA son Software Engineer, Data Scientist y Full Stack Engineer.

*Consultor y Estratega de IA (#2)*: Estos expertos ayudan a las organizaciones en evaluación de casos de uso, implementación de pilotos y gestión de MLOps, balanceando riesgo con ROI. El perfil típico tiene 8.2 años de experiencia, indicando que este no es un rol para principiantes sino para profesionales senior que combinan expertise técnica con visión de negocio.

*Anotador de Datos (#4)*: Un rol frecuentemente subestimado pero crítico para el pipeline de IA. Los anotadores etiquetan y revisan datos según guías detalladas para entrenar modelos. Son empleados principalmente en sectores de tecnología, staffing y educación superior, con concentración en Austin, Nueva York y San Francisco. Un 62% son mujeres, representando una de las pocas áreas de IA con alta participación femenina.

*Investigador de IA/ML (#5)*: Diseñan, prueban y refinan modelos de IA para avanzar capacidades. Sus habilidades clave incluyen Deep Learning, PyTorch y Computer Vision. Trabajan principalmente en Tecnología, Educación Superior y Servicios de Investigación.

**Otros Roles Tecnológicos en el Top 25**

| Ranking | Rol | Descripción |
|---------|-----|-------------|
| #15 | **Técnico de Datacenter** | Instalación y mantenimiento de servidores; 4% remoto, 32% híbrido |
| #20 | **Investigador Cuantitativo** | Modelos matemáticos/estadísticos; trading algorítmico, backtesting |

**La Demanda de Infraestructura Impulsada por IA**

Los empleos de infraestructura y operaciones —incluyendo ingenieros cloud, ingenieros de confiabilidad del sitio (SRE) y ingenieros de plataforma— siguen muy demandados, ya que la complejidad en el backend ha aumentado con la llegada de la IA. Las aplicaciones desarrolladas con IA son intensivas en cómputo, costosas y propensas a indisponibilidad. Los empleadores buscan personas con expertise en "hacerlas funcionar bien, económicamente y de manera confiable".

**El Auge del Forward Deployed Engineer**

Incluyendo IA pero más amplio, el "forward deployed engineer" (FDE) es probablemente el empleo más caliente en tecnología en este momento. Estos son los ingenieros que entienden los modelos fluidamente, y también comprenden datos empresariales, cómo hacer fine-tuning, y cómo crear loops de aprendizaje por refuerzo. Representan el puente entre la capacidad técnica de la IA y las necesidades reales del negocio.

**El Fenómeno del Emprendimiento**

Un dato llamativo: "Fundadores" aparece en el #7, con 51% de modalidad remota. El auge de fundadores y consultores independientes señala un cambio hacia el autoempleo y el trabajo gig, mientras los profesionales se adaptan a la incertidumbre del mercado laboral tradicional.

**Implicaciones para la Contratación en 2026**

El reporte confirma varias tendencias críticas:

1. **8 de cada 10 líderes globales** reportan mayor probabilidad de contratar candidatos cómodos con herramientas de IA comparados con candidatos más experimentados pero menos familiarizados con la tecnología.

2. **El 41% de ofertas de trabajo tecnológico activas** ahora requieren habilidades de IA o son específicas de IA. La demanda de habilidades relacionadas con IA ha crecido aproximadamente 7 veces en solo dos años.

3. **Los títulos importan menos que las habilidades demostradas**. Un portafolio sólido, certificaciones relevantes y experiencia real en proyectos frecuentemente superan un título en ciencias de la computación.

4. **Los roles que mencionan IA pagan aproximadamente 28% más**, mientras que roles no técnicos con fluidez en IA pueden ver aumentos salariales del 35% al 43%.

**La "Ola de Recontratación"**

Un fenómeno emergente que el reporte captura: "Las empresas que intentaron operar con personal mínimo basándose solo en automatización ahora se dan cuenta de que todavía necesitan humanos con las habilidades especializadas correctas. Estamos viendo lo que llamamos una 'ola de recontratación' mientras las organizaciones corrigen el curso."

Esta ola valida el argumento central del marco IA-Native: la IA no reemplaza la supervisión humana, la hace más crítica. Las organizaciones que eliminaron roles humanos prematuramente están descubriendo que la gobernanza, el juicio y la responsabilidad requieren profesionales capacitados que no pueden ser automatizados.

### El Reset de la Producción de Código en 2026

El término *vibe coding*, coronado como palabra del año por Collins en 2025, describe el enfoque donde las personas describen software en lenguaje natural y dejan que la IA escriba, refine y depure el código. Este enfoque dominó 2025, pero 2026 marca un punto de inflexión.

Las herramientas de generación de código con IA enfrentan un reset en 2026. Proveedores y empresas están cambiando el foco desde el uso experimental y el crecimiento temprano de ingresos hacia arquitectura, gobernanza y mantenibilidad a largo plazo. Se espera un movimiento alejándose del "vibe coding" hacia herramientas que incorporen guardrails y respeten patrones de software existentes dentro de las grandes organizaciones.

Los líderes de la industria predicen que en 2026, las herramientas de desarrollo impulsadas por IA madurarán mucho más allá del vibe coding o la asistencia básica de prueba de concepto. La próxima ola se enfocará en generar código de grado productivo que encaje sin problemas en los estándares de arquitectura empresarial.

### Trazabilidad, Confianza y Gobernanza

Mientras la IA tiene gran potencial, la confianza se convierte nuevamente en una preocupación mayor. Las organizaciones necesitarán fuerte trazabilidad, controles de proveniencia y mecanismos de aseguramiento automatizados para garantizar seguridad.

Los líderes de ingeniería en sectores regulados dicen que quieren trails de auditoría, comportamiento consistente y alineación con controles de cumplimiento de cualquier herramienta que toque sistemas de producción.

Este requisito alinea directamente con el marco IA-Native: la exigencia de Policy Tickets, evidencia explícita y responsabilidad humana identificable no es una burocracia innecesaria, sino una respuesta directa a las demandas del mercado empresarial.

### El Mercado Latinoamericano: Oportunidad y Transformación

El mercado de servicios IT en Latinoamérica alcanza los $107.23 mil millones en 2025, con proyección de $242.1 mil millones para 2030 (crecimiento CAGR del 17.69%). La región cuenta con aproximadamente 2 millones de desarrolladores, con México liderando con 800,000 expertos tecnológicos.

Para las software factories latinoamericanas, la IA representa tanto amenaza como oportunidad. La ventaja de costos tradicional (ingenieros senior que ganan aproximadamente la mitad que sus contrapartes estadounidenses) se erosiona si las empresas locales no pueden ofrecer valor agregado más allá de la ejecución de código.

El outsourcing ha evolucionado de una táctica de reducción de costos a un impulsor estratégico de crecimiento. Las software factories que prosperan son aquellas que se especializan en nichos como fintech, health tech, desarrollo de productos de IA y ciberseguridad, donde la expertise de dominio y la capacidad de gobernanza superan la simple producción de código.

### Proyección: El Timeline de Gartner

Gartner proporciona un timeline de tres fases que sirve como guía para la planificación estratégica:
- **Ahora**: Complementariedad con IA
- **2027**: Flujos de trabajo IA-nativos
- **2028**: Disciplina de ingeniería de IA

El 80% de los ingenieros debe upskilling para 2027. La no adopción significa riesgo de carrera. Los roles que mencionan IA pagan aproximadamente un 28% más, mientras que los roles no técnicos con fluidez en IA pueden ver aumentos salariales del 35% al 43%.

### Implicaciones para las Software Factories

Las software factories que sobrevivirán y prosperarán en 2026 y más allá son aquellas que:

1. **Adoptan gobernanza explícita** - Implementan marcos como IA-Native con Policy Tickets, niveles de autonomía y requisitos de evidencia.

2. **Invierten en talento senior** - Reconocen que la IA no reemplaza la supervisión humana sino que la hace más crítica.

3. **Mantienen el pipeline de juniors** - Desarrollan programas de formación que preparen a la próxima generación para trabajar con IA, no ser reemplazados por ella.

4. **Especializan su oferta** - Se mueven de "fábrica de código" a "fábrica de soluciones gobernadas" donde el valor está en la expertise de dominio y la capacidad de control.

5. **Priorizan la seguridad** - Asumen que el código generado por IA requiere validación humana exhaustiva, especialmente en contextos regulados.

6. **Miden lo que importa** - Abandonan métricas de velocidad bruta por métricas de impacto, riesgo y calidad a largo plazo.

El mensaje es claro: la era del código como commodity ha terminado. El futuro pertenece a las organizaciones que pueden gobernar la producción de software con la misma rigurosidad con la que lo producen.

---

# PARTE II: AGENTES DE IA - ANATOMÍA Y ORQUESTACIÓN

## ★ Anatomía de los Agentes de IA: Comprendiendo lo que Orquestamos ★

> **"Un agente no genera respuestas; genera resultados. El usuario define el qué, el agente descubre el cómo."**

Para gobernar efectivamente la acción de la inteligencia artificial en el desarrollo de software, es imprescindible comprender qué son exactamente los agentes de IA, cómo funcionan, y en qué se diferencian de las herramientas tradicionales de asistencia. Sin esta comprensión, cualquier marco de gobernanza se construye sobre fundamentos inestables.

### ¿Qué es un Agente de IA?

Un agente de IA (o LLM agéntico) es un modelo de lenguaje que opera con intención, planificación y acción, en lugar de simplemente producir respuestas de un solo turno. A diferencia de un chatbot tradicional que responde preguntas, un agente persigue objetivos incluso cuando el camino no es explícito.

Los agentes representan un cambio de paradigma: de interacciones simples de pregunta-respuesta hacia sistemas autónomos que pueden planificar, razonar, usar herramientas y lograr objetivos complejos. A diferencia de los chatbots tradicionales, los agentes mantienen estado a través de las interacciones, descomponen problemas en pasos, y adaptan su enfoque basándose en resultados intermedios.

### Las Cuatro Dimensiones que Definen a un Agente

Los agentes de codificación se diferencian de los asistentes integrados en IDEs a lo largo de cuatro dimensiones clave:

**1. Autonomía**
Capacidad de completar tareas sin guía humana continua. Un agente puede recibir una instrucción como "refactoriza este módulo para mejorar la legibilidad" y ejecutar múltiples acciones —analizar el código, identificar patrones problemáticos, proponer cambios, ejecutar tests— sin intervención humana entre cada paso.

**2. Alcance**
Operación a través de múltiples archivos y bases de código completas. Mientras un asistente de autocompletado trabaja línea por línea, un agente puede entender y modificar repositorios enteros, comprendiendo las relaciones entre componentes.

**3. Planificación**
Descomposición de requerimientos en subtareas y ejecución de soluciones multi-paso. El agente no solo ejecuta; primero analiza, planifica, y luego procede de manera estructurada, ajustando su plan según los resultados intermedios.

**4. Interacción**
Contribución principalmente a través de pull requests. Los agentes modernos no solo sugieren código; crean branches, generan commits, abren PRs, y pueden iterar basándose en feedback de revisores humanos o sistemas de CI/CD.

### Taxonomía de Agentes de Codificación en 2026

El ecosistema de agentes de codificación ha madurado significativamente. Para finales de 2025, aproximadamente el 85% de los desarrolladores utilizaban regularmente herramientas de IA para codificar. Los principales tipos de agentes incluyen:

**Agentes Integrados en IDE**

*Cursor*: Representa lo que sucede cuando reconstruyes un IDE desde cero con la IA como ciudadano de primera clase. Basado en VS Code, la IA ve lo que el desarrollador ve y puede realizar cambios directamente, no solo sugerirlos. Su filosofía: la IA debe mejorar el flujo de trabajo existente mientras el desarrollador mantiene control sobre cada decisión.

*GitHub Copilot*: El pionero de la asistencia de código, ahora evolucionado hacia capacidades agénticas con Copilot Workspace, permitiendo planificación y ejecución de cambios multi-archivo.

**Agentes Autónomos de Terminal**

*Claude Code*: Lidera en precisión (80.9% en SWE-bench con Opus 4.5). Incluye sub-agentes, hooks personalizados, y amplia configurabilidad. Reportes indican que Claude Code usa 5.5x menos tokens que alternativas para la misma tarea, con menos errores y retrabajo. Su filosofía: la IA debe ser capaz de trabajo autónomo en tareas bien definidas, liberando al desarrollador para enfocarse en arquitectura y toma de decisiones de alto nivel.

*OpenAI Codex*: Ha resurgido en 2025 como una herramienta seria de codificación agent-first. Es open source, permitiendo personalización completa. Destaca en eficiencia de tokens: tareas idénticas en TypeScript consumen 72,579 tokens con Codex versus 234,772 con Claude—una diferencia de 3x que se acumula a escala.

**Agentes Completamente Autónomos**

*Devin*: El "ingeniero de software completamente autónomo" de Cognition Labs. Opera en su propio entorno sandboxed con shell, editor de código y navegador. Se le asigna una tarea via Slack o web, y Devin trabaja independientemente. Ideal para migraciones y refactoring masivo cuando la tarea puede definirse claramente. Ha reducido precios de $500/mes a $20/mes, democratizando el acceso.

### Capacidades Actuales: Lo que los Agentes Pueden Hacer

Los agentes de 2026 han alcanzado capacidades que habrían parecido ciencia ficción hace pocos años:

- **Sesiones autónomas de semanas**: Claude Code y OpenAI Codex ahora ejecutan sesiones autónomas que abarcan semanas. Cursor ha documentado agentes operando por "3+ semanas", escribiendo más de 1 millón de líneas de código en un solo proyecto.

- **Migraciones completas de codebase**: Un agente migró una base de código entera de Solid a React—266,000 líneas añadidas, 193,000 eliminadas—sin intervención humana entre checkpoints.

- **Multi-archivo y multi-repositorio**: Los agentes entienden contexto a nivel de repositorio completo, pueden hacer cambios coordinados a través de múltiples archivos, y mantener coherencia arquitectónica.

### Los Riesgos Documentados

Un estudio reciente de arXiv reveló hallazgos críticos sobre el impacto de los agentes:

- Las ganancias de velocidad son grandes pero frontloaded: solo son significativas cuando los agentes son la primera herramienta de IA observable en un proyecto. Repositorios con uso previo de IDEs con IA experimentan beneficios mínimos o de corta duración.

- Los riesgos de calidad son persistentes: las advertencias de análisis estático y la complejidad cognitiva aumentan aproximadamente 18% y 35% respectivamente, indicando una deuda de complejidad inducida por agentes sostenida incluso cuando las ventajas de velocidad desaparecen.

Estos efectos heterogéneos sugieren rendimientos decrecientes de la asistencia de IA y destacan la necesidad de salvaguardas de calidad—precisamente lo que el marco IA-Native proporciona.

### El Mercado de Agentes: Escala y Proyección

El mercado de agentes de IA ha explotado de $5.40 mil millones en 2024 a $7.63 mil millones en 2025, con proyecciones de alcanzar $50.31 mil millones para 2030 a un notable CAGR del 45.8%. Este crecimiento refleja un cambio fundamental de herramientas de IA pasivas hacia sistemas autónomos capaces de razonar, planificar y ejecutar tareas complejas.

La IA ha creado demanda a escala, incluyendo más de 600,000 nuevos empleos en datacenters habilitados por IA y 1.3 millones de nuevos roles como Ingenieros de IA, Forward-Deployed Engineers y Anotadores de Datos.

---

## ★ Orquestación de Agentes: Herramientas y Frameworks para el Programador de 2026 ★

> **"El campo de la IA agéntica está atravesando su revolución de microservicios. Así como las aplicaciones monolíticas dieron paso a arquitecturas de servicios distribuidos, los agentes de propósito único están siendo reemplazados por equipos orquestados de agentes especializados."**

El programador de 2026 no solo utiliza agentes individuales; los orquesta. Esta sección examina los frameworks y herramientas que permiten construir, coordinar y gobernar sistemas multi-agente.

### Frameworks de Orquestación de Agentes

#### LangChain: El Framework Universal

LangChain, lanzado en 2022, evolucionó rápidamente hasta convertirse en el framework más ampliamente adoptado para construir aplicaciones impulsadas por LLMs. Su arquitectura modular proporciona bloques de construcción para cadenas, agentes, memoria e integración de herramientas.

**Características clave:**
- Comunidad masiva (80K+ estrellas en GitHub)
- Documentación exhaustiva y adopción empresarial probada
- Integraciones con prácticamente todo proveedor de LLM, base de datos vectorial y servicio externo
- "Navaja suiza" de frameworks de agentes, aunque sin flujos de trabajo prescritos—el desarrollador diseña la lógica del agente

**Ideal para:** Construir lógica personalizada profunda con múltiples modelos y herramientas.

#### CrewAI: Equipos de Agentes Basados en Roles

CrewAI es un framework de orquestación de agentes orientado a equipos, basado en roles, diseñado para flujos de trabajo estructurados tipo SOP (Standard Operating Procedures).

**Arquitectura de dos capas:**
- *Crews*: Colaboración dinámica de agentes basada en roles
- *Flows*: Orquestación de tareas determinista y orientada a eventos

Esta estructura balancea autonomía de alto nivel con control de bajo nivel. CrewAI ha ganado tracción gracias a una curva de aprendizaje más baja y documentación extensiva.

**Ideal para:** Diseñar equipos basados en roles de forma visual; casos de uso de experiencia del cliente.

#### AutoGen: Razonamiento Colaborativo de Microsoft

AutoGen, desarrollado por Microsoft Research, está construido alrededor del razonamiento colaborativo a través de conversación. Los agentes interactúan mediante mensajes estructurados en lenguaje natural en una arquitectura tipo chat grupal.

**Características distintivas:**
- Cada agente recibe un rol, objetivo y herramientas opcionales
- Excele en escenarios que requieren múltiples agentes especializados trabajando juntos
- Interfaz low-code para prototipado de agentes
- Trata los flujos de trabajo como conversaciones entre agentes

**Ideal para:** Investigar loops de auto-reflexión multi-agente; sistemas de razonamiento colaborativo.

#### La Analogía que lo Resume Todo

> "AutoGen te da los ladrillos, LangChain pone un kit de herramientas sobre la mesa, y CrewAI te presta la tripulación y una misión."

### n8n: La Capa de Control para IA Agéntica

n8n representa un nuevo paradigma en la orquestación de agentes: una plataforma de automatización de flujos de trabajo que combina de manera única capacidades de IA con automatización de procesos de negocio.

**¿Qué es n8n?**

n8n es una plataforma de automatización fair-code que da a equipos técnicos la flexibilidad del código con la velocidad del no-code. Con más de 400 integraciones y capacidades nativas de IA, permite construir automatizaciones potentes manteniendo control total sobre datos y despliegues.

**Agentes de IA en n8n:**

Los agentes en n8n son flujos de trabajo autónomos impulsados por IA que pueden tomar decisiones, interactuar con aplicaciones y ejecutar tareas sin input humano constante. Usan una combinación de memoria, objetivos y herramientas (como búsqueda web o acceso a bases de datos) para razonar a través de tareas paso a paso.

**Arquitectura Técnica:**

- Sistema de nodos powered by LangChain con soporte para OpenAI, Anthropic y HuggingFace
- Gestión de memoria y capacidades de tool-calling para agentes context-aware
- Despliegue production-ready usando Docker Compose con PostgreSQL y Redis en modo cola para escalado horizontal
- Hasta 220 ejecuciones de workflow por segundo en una sola instancia

**Lo que Distingue a n8n:**

Lo que diferencia a n8n es cómo trata la automatización como una preocupación de desarrollo de primera clase en lugar de solo un hack de productividad. Los desarrolladores pueden escribir JavaScript y Python real, gestionar flujos de trabajo como código, y extender la plataforma via CLI, SDK y APIs.

**Orquestación Multi-Agente:**

n8n soporta orquestación multi-agente: gestionar múltiples agentes de IA, cada uno manejando tareas especializadas, mientras coordina sus acciones hacia objetivos de negocio complejos. Se puede comenzar a construir sistemas multi-agente con una UI declarativa y añadir complejidad con Python o JavaScript cuando tenga sentido.

### Otras Herramientas de Orquestación

| Herramienta | Tipo | Fortaleza Principal |
|-------------|------|---------------------|
| **LangGraph** | Framework | Representación visual de flujos como grafos; workflows estructurados |
| **DSPy** | Framework | Programación declarativa de prompts; optimización automática |
| **Haystack** | Framework | Pipelines de búsqueda y RAG; procesamiento de documentos |
| **Microsoft Semantic Kernel** | SDK | Integración empresarial; ecosistema Microsoft |
| **Make** | No-code | Automatización visual; ideal para no-técnicos |
| **Zapier** | No-code | Integraciones masivas; simplicidad de uso |

### El Stack de IA Agéntica en 2026

Un nuevo paradigma ha emergido: sistemas de IA Agéntica capaces de tomar decisiones autónomas, orquestar flujos de trabajo complejos, e impulsar procesos de negocio con mínima intervención humana.

El stack típico de un programador orquestando agentes en 2026 incluye:

**Capa 1: Modelos Base**
- GPT-4/5 (OpenAI)
- Claude Opus/Sonnet (Anthropic)
- Gemini (Google)
- Modelos open source (Llama, Mistral)

**Capa 2: Frameworks de Agentes**
- LangChain/LangGraph para lógica compleja
- CrewAI para equipos de agentes
- AutoGen para razonamiento colaborativo

**Capa 3: Orquestación y Automatización**
- n8n como capa de control conectando agentes, APIs y lógica de negocio
- Temporal para workflows de larga duración
- Apache Airflow para pipelines de datos

**Capa 4: Infraestructura**
- Bases de datos vectoriales (Pinecone, Weaviate, Chroma)
- Sistemas de memoria y estado
- Monitoreo y observabilidad (LangSmith, Weights & Biases)

**Capa 5: Gobernanza**
- Policy Tickets del marco IA-Native
- Controles de branch protection en GitHub
- Sistemas de auditoría y trazabilidad

### Cómo el Programador de 2026 Orquesta Agentes

El programador moderno no escribe código directamente; diseña sistemas de agentes. Su trabajo diario incluye:

**1. Definición de Arquitectura de Agentes**
Determinar qué agentes se necesitan, qué rol cumple cada uno, y cómo se comunican entre sí. Esto requiere pensamiento sistémico más que habilidad de codificación.

**2. Configuración de Herramientas y Permisos**
Cada agente necesita acceso a herramientas específicas (APIs, bases de datos, sistemas de archivos). El programador define qué puede y qué no puede hacer cada agente—los guardrails del marco IA-Native.

**3. Diseño de Flujos de Trabajo**
Usando herramientas como n8n o LangGraph, el programador diseña cómo fluye el trabajo entre agentes, qué triggers inician procesos, y qué condiciones determinan bifurcaciones.

**4. Implementación de Memoria y Estado**
Los agentes necesitan recordar contexto entre interacciones. El programador configura sistemas de memoria apropiados—desde simple caché hasta bases de datos vectoriales complejas.

**5. Monitoreo y Ajuste**
Una vez desplegado, el sistema requiere observabilidad continua. El programador analiza métricas, identifica cuellos de botella, y ajusta configuraciones para optimizar rendimiento y calidad.

**6. Gobernanza y Cumplimiento**
Cada acción de agente debe ser trazable a un Policy Ticket, un responsable humano, y evidencia de verificación. El programador asegura que el sistema cumple con los requisitos del marco IA-Native.

### El Futuro: Equipos de Agentes Especializados

La tendencia es clara: los agentes de propósito general están dando paso a equipos orquestados de agentes especializados. Un proyecto típico podría incluir:

- Un agente de análisis de requerimientos
- Un agente de diseño arquitectónico
- Múltiples agentes de implementación (frontend, backend, base de datos)
- Un agente de testing y QA
- Un agente de documentación
- Un agente de seguridad y compliance

El programador de 2026 es el director de orquesta de este ensemble. No toca cada instrumento; coordina la sinfonía completa.

---

# PARTE III: MARCO DE GOBERNANZA

## ★ Principios Fundamentales del Marco IA-Native ★

> **"Un marco sin principios explícitos es un marco sin criterio de decisión. Los principios formalizados permiten resolver ambigüedades y mantener coherencia a escala."**

El marco IA-Native se sustenta en un conjunto de principios fundamentales que guían todas las decisiones operativas. Estos principios no son sugerencias; son la base sobre la cual se construye la coherencia del sistema.

### Los Seis Principios Cardinales

| Principio | Descripción | Implicación Práctica |
|-----------|-------------|---------------------|
| **Responsabilidad Inalienable** | Delegar ejecución no transfiere responsabilidad. La IA ejecuta; el humano responde. | Todo Policy Ticket tiene un responsable humano identificado y contactable |
| **Control Preventivo** | El gobierno ocurre antes de la ejecución, no después. Las condiciones se establecen ex-ante. | Las políticas definen límites antes de que la IA actúe, no en la review |
| **Evidencia sobre Confianza** | Las decisiones se basan en artefactos verificables, no en supuestos o reputación. | Sin evidencia documentada, el cambio no es válido—aunque funcione |
| **Autonomía Proporcional** | Mayor riesgo implica menor autonomía de la IA. La libertad se gradúa según el dominio. | Dominios críticos requieren supervisión humana en cada paso |
| **Trazabilidad Completa** | Cada cambio puede rastrearse hasta una decisión explícita y un responsable identificable. | Vínculo obligatorio PR ↔ Policy Ticket; auditoría completa |
| **Modularidad de Políticas** | Las políticas son unidades independientes, reutilizables y componibles. | Sistema de skills de gobernanza; templates estandarizados |

### Aplicación de los Principios

**En la Práctica Diaria:**

```
┌─────────────────────────────────────────────────────────────┐
│                    DECISIÓN DE MERGE                        │
├─────────────────────────────────────────────────────────────┤
│ ¿Hay Policy Ticket vinculado?                               │
│   NO → Rechazar (viola Trazabilidad Completa)               │
│   SÍ ↓                                                      │
│ ¿Hay responsable humano identificado?                       │
│   NO → Rechazar (viola Responsabilidad Inalienable)         │
│   SÍ ↓                                                      │
│ ¿Existe evidencia requerida?                                │
│   NO → Rechazar (viola Evidencia sobre Confianza)           │
│   SÍ ↓                                                      │
│ ¿El nivel de autonomía es apropiado para el dominio?        │
│   NO → Escalar (viola Autonomía Proporcional)               │
│   SÍ ↓                                                      │
│ ¿Los límites fueron definidos antes de la ejecución?        │
│   NO → Documentar (viola Control Preventivo)                │
│   SÍ ↓                                                      │
│ APROBAR MERGE                                               │
└─────────────────────────────────────────────────────────────┘
```

### Jerarquía de Principios

Cuando dos principios entran en conflicto, se aplica la siguiente jerarquía:

1. **Responsabilidad Inalienable** - Siempre tiene precedencia
2. **Control Preventivo** - El gobierno ex-ante antes que la velocidad
3. **Evidencia sobre Confianza** - Sin pruebas, no hay validez
4. **Autonomía Proporcional** - El riesgo determina la libertad
5. **Trazabilidad Completa** - Todo debe poder reconstruirse
6. **Modularidad de Políticas** - Organización eficiente del conocimiento

---

## De Tareas a Decisiones Gobernadas: El Cambio Conceptual

El cambio más profundo que introduce el trabajo con agentes de inteligencia artificial no es técnico, sino conceptual. No se limita a nuevas herramientas ni a una mayor velocidad de ejecución, sino que afecta la forma en que se define, organiza y controla el trabajo de desarrollo. Para comprender este cambio, es necesario revisar uno de los supuestos más arraigados de la ingeniería de software: la noción de tarea como unidad básica del proceso.

En los enfoques tradicionales, la tarea representa una porción de trabajo que debe ser realizada por una persona o un equipo. Puede ser pequeña o grande, pero siempre presupone una ejecución humana. Incluso cuando se automatizan partes del proceso, la tarea sigue siendo el marco de referencia para planificar, estimar y evaluar el avance. La responsabilidad se asocia de manera directa con quien ejecuta la tarea.

En entornos IA-Native, este modelo se vuelve insuficiente. Cuando un agente puede generar, modificar o refactorizar grandes volúmenes de código en muy poco tiempo, la pregunta relevante deja de ser "qué tarea hay que hacer" y pasa a ser "qué está autorizado a hacer este agente". El foco se desplaza de la ejecución al gobierno de la acción.

Este desplazamiento implica redefinir la unidad básica del trabajo. En lugar de tareas, el marco propone pensar en decisiones de delegación. Cada intervención de la IA debe ser entendida como una decisión explícita de delegar parte del trabajo cognitivo a un sistema artificial. Esa delegación no puede ser implícita ni genérica; debe estar acotada, justificada y gobernada.

A partir de esta lógica, el concepto central deja de ser la tarea y pasa a ser el cambio gobernado. Un cambio gobernado es una modificación del sistema que cumple con un conjunto de condiciones previas: ha sido autorizada por un responsable humano, se encuentra delimitada en su alcance, incorpora mecanismos de verificación y produce evidencia suficiente para su evaluación. Si una modificación no cumple estas condiciones, no es considerada válida, aun cuando el código funcione correctamente.

Este enfoque altera también la noción de control. En los modelos tradicionales, el control es mayormente reactivo. Se revisa el código después de que fue escrito, se detectan errores en testing y se corrigen en ciclos posteriores. En un contexto de alta automatización y velocidad, este control a posteriori resulta insuficiente. El marco IA-Native propone un control predominantemente preventivo, donde las condiciones de validez se establecen antes de que la IA actúe.

Otro aspecto conceptual relevante es la trazabilidad. Cuando el trabajo es ejecutado por personas, la trazabilidad suele ser informal: conversaciones, comentarios en commits, decisiones implícitas. En un entorno donde la IA produce cambios, esta informalidad se vuelve riesgosa. El marco introduce la necesidad de trazabilidad explícita, donde cada cambio puede ser rastreado hasta una decisión de delegación concreta y a un responsable identificable.

Finalmente, esta transformación obliga a repensar la noción de responsabilidad. Delegar no implica transferir responsabilidad. Aunque la IA ejecute el cambio, la responsabilidad última sigue siendo humana. El marco se construye sobre esta premisa: la inteligencia artificial no asume responsabilidad; la organización y las personas sí.

Esta sección establece, así, el fundamento conceptual del marco de trabajo. Al redefinir la unidad básica del proceso y desplazar el foco hacia decisiones gobernadas, se sientan las bases para un modelo que puede escalar el uso de IA sin perder control, coherencia ni responsabilidad profesional.

---

## El Policy Ticket: Instrumento Operativo Central

Una vez que se redefine la unidad básica del trabajo como una decisión de delegación gobernada, surge de manera natural la necesidad de un instrumento que permita materializar esa decisión de forma clara, operativa y verificable. El Policy Ticket cumple exactamente esa función dentro del marco de trabajo IA-Native.

A diferencia de una tarea o una historia de usuario tradicional, el Policy Ticket no describe principalmente un resultado esperado, sino un comportamiento autorizado. Su propósito no es indicar qué debe hacerse, sino establecer qué está permitido hacer a la inteligencia artificial en un contexto determinado, bajo qué condiciones y con qué responsabilidades asociadas.

Este cambio de enfoque es fundamental. En los modelos clásicos, gran parte del control se ejerce a posteriori: se confía en que el equipo humano tomará buenas decisiones durante la implementación y se corrigen los errores detectados en instancias posteriores. En un entorno donde la IA puede ejecutar cambios a gran velocidad, ese esquema resulta frágil. El Policy Ticket desplaza el control hacia el momento previo a la ejecución.

Un Policy Ticket define, en primer lugar, un responsable humano explícito. No se trata de un rol genérico, sino de una persona o función concreta que asume la responsabilidad por autorizar la intervención de la IA. Esta explicitación evita la dilución de responsabilidades, un problema frecuente cuando los cambios son generados por sistemas automáticos.

En segundo lugar, el Policy Ticket establece la intención de la intervención. Esta intención debe ser clara y acotada. No se autoriza a la IA a "mejorar el sistema" de manera genérica, sino a realizar acciones específicas, como refactorizar componentes duplicados, generar tests de regresión o analizar consistencia entre fuentes de datos. La claridad de la intención es clave para reducir comportamientos no deseados.

Otro componente central es la delimitación del alcance. El Policy Ticket especifica qué tipos de acciones están permitidas y cuáles están explícitamente prohibidas. Esta delimitación actúa como un conjunto de guardrails que acotan el espacio de acción de la IA. Todo lo que no está expresamente permitido se considera fuera de alcance.

El nivel de autonomía constituye otro eje importante. No todas las intervenciones requieren el mismo grado de libertad. En algunos casos, la IA puede limitarse a generar propuestas que luego serán evaluadas por humanos. En otros, puede ejecutar cambios de manera automática siempre que se cumplan ciertos controles. El Policy Ticket permite graduar esta autonomía en función del riesgo asociado.

La exigencia de evidencia completa el instrumento. Cada intervención autorizada debe producir artefactos verificables: diffs de código, resultados de tests, reportes de análisis, métricas de impacto. La evidencia reemplaza a la confianza implícita y se convierte en el criterio principal para decidir si un cambio puede integrarse.

Finalmente, el Policy Ticket define criterios claros de aceptación y rechazo. Un cambio puede ser técnicamente correcto y, aun así, ser rechazado si no cumple con las condiciones establecidas. Esta distinción refuerza la idea de que el marco no se centra únicamente en la funcionalidad, sino en la gobernanza del proceso.

En conjunto, el Policy Ticket opera como un contrato operativo entre humanos y sistemas artificiales. Formaliza la delegación cognitiva, establece límites claros y proporciona un mecanismo auditable para gestionar el uso de la IA en el desarrollo de software. Sin este instrumento, el uso de agentes tiende a volverse informal, opaco y difícil de controlar a escala.

---

## ★ Matriz de Overhead Operativo por Nivel de Riesgo ★

> **"La gobernanza efectiva no significa aplicar el mismo rigor a todo. Significa aplicar el rigor apropiado al riesgo real."**

Uno de los principales riesgos en la adopción del marco IA-Native es la burocratización excesiva: aplicar el mismo nivel de control a una modificación cosmética en CSS que a un cambio en la lógica de procesamiento de pagos. Esta sección establece parámetros concretos para graduar el overhead de gobernanza de manera proporcional al riesgo.

### Clasificación de Dominios por Nivel de Riesgo

La clasificación de riesgo no es arbitraria. Se basa en tres factores principales:

| Factor | Descripción | Peso |
|--------|-------------|------|
| **Impacto en usuarios** | ¿Una falla afecta directamente a usuarios finales? | 40% |
| **Impacto financiero** | ¿Una falla puede causar pérdidas económicas directas? | 35% |
| **Reversibilidad** | ¿Un error puede corregirse fácilmente o es permanente? | 25% |

**Matriz de Clasificación de Dominios:**

| Dominio | Riesgo | Justificación |
|---------|--------|---------------|
| Documentación, README, comentarios | **Bajo** | Sin impacto funcional, fácilmente reversible |
| Tests unitarios, tests de integración | **Bajo** | Protegen contra errores, no introducen riesgo |
| Estilos CSS, UI cosmética | **Bajo** | Impacto visual limitado, fácilmente corregible |
| Refactoring interno (sin cambio de API) | **Bajo-Medio** | Puede introducir bugs sutiles |
| Frontend lógica de presentación | **Medio** | Afecta UX pero raramente datos |
| Backend endpoints no críticos | **Medio** | Puede afectar funcionalidad parcial |
| APIs públicas, contratos de interfaz | **Medio-Alto** | Cambios pueden romper integraciones |
| Lógica de negocio core | **Alto** | Errores afectan directamente el valor |
| Autenticación, autorización | **Alto** | Vulnerabilidades = brechas de seguridad |
| Procesamiento de pagos | **Crítico** | Impacto financiero directo |
| Datos personales (PII, GDPR) | **Crítico** | Implicaciones legales y regulatorias |
| Infraestructura de seguridad | **Crítico** | Exposición total del sistema |

### Matriz de Overhead por Nivel de Riesgo

Esta matriz define el esfuerzo operativo esperado para cada nivel de riesgo:

```
┌─────────────────────────────────────────────────────────────────────────────────────┐
│                    MATRIZ DE OVERHEAD OPERATIVO                                      │
├──────────┬───────────────┬─────────────┬────────────────┬──────────────────────────┤
│ NIVEL    │ TIEMPO PT     │ APROBADORES │ EVIDENCIAS     │ AUTONOMÍA IA             │
├──────────┼───────────────┼─────────────┼────────────────┼──────────────────────────┤
│ BAJO     │ 5-10 min      │ Self-       │ • Tests pasan  │ Completa con auto-merge  │
│          │               │ approval    │ • Lint OK      │ si checks pasan          │
├──────────┼───────────────┼─────────────┼────────────────┼──────────────────────────┤
│ MEDIO    │ 15-30 min     │ 1 peer      │ • Tests pasan  │ Genera PR, espera        │
│          │               │ reviewer    │ • Lint OK      │ aprobación humana        │
│          │               │             │ • SAST limpio  │                          │
├──────────┼───────────────┼─────────────┼────────────────┼──────────────────────────┤
│ ALTO     │ 30-60 min     │ Tech Lead + │ • Tests pasan  │ Solo propuesta,          │
│          │               │ 1 domain    │ • SAST limpio  │ humano implementa        │
│          │               │ expert      │ • Security     │ o valida línea por       │
│          │               │             │   scan         │ línea                    │
├──────────┼───────────────┼─────────────┼────────────────┼──────────────────────────┤
│ CRÍTICO  │ 1-2 horas     │ Tech Lead + │ • Todo lo      │ IA solo análisis,        │
│          │               │ Security +  │   anterior +   │ humano escribe código    │
│          │               │ Product     │ • Penetration  │ con IA como consultor    │
│          │               │ Owner       │   test         │                          │
│          │               │             │ • Audit trail  │                          │
│          │               │             │   completo     │                          │
└──────────┴───────────────┴─────────────┴────────────────┴──────────────────────────┘
```

### Detalle de Tiempos por Actividad

**Desglose del tiempo de Policy Ticket por nivel:**

#### Nivel Bajo (5-10 minutos totales)

| Actividad | Tiempo | Responsable |
|-----------|--------|-------------|
| Seleccionar template de PT bajo riesgo | 1 min | Desarrollador |
| Completar campos obligatorios | 2-3 min | Desarrollador |
| Verificar que checks automáticos pasan | 2-3 min | Automático |
| Auto-aprobación y merge | 1-2 min | Sistema |

*Ejemplo de Policy Ticket nivel bajo:*
```yaml
PT-ID: PT-2026-0142
Dominio: testing
Riesgo: bajo
Intención: Agregar tests unitarios para módulo de validación de email
Alcance-Permitido:
  - Crear archivos en /tests/unit/validation/
  - Modificar /tests/conftest.py para fixtures
Alcance-Prohibido:
  - Modificar código de producción
  - Cambiar configuración de CI/CD
Autonomía: completa
Aprobación: self-approval
Evidencia: tests pasan, cobertura >= 80%
```

#### Nivel Medio (15-30 minutos totales)

| Actividad | Tiempo | Responsable |
|-----------|--------|-------------|
| Completar Policy Ticket detallado | 5-8 min | Desarrollador |
| Análisis de impacto (qué puede afectar) | 3-5 min | Desarrollador |
| Revisión por peer | 5-10 min | Reviewer |
| Verificación de evidencias | 2-5 min | Automático + Reviewer |
| Aprobación y merge | 2 min | Reviewer |

*Ejemplo de Policy Ticket nivel medio:*
```yaml
PT-ID: PT-2026-0143
Dominio: backend
Riesgo: medio
Intención: Optimizar queries de listado de productos para reducir tiempo de respuesta
Alcance-Permitido:
  - Modificar /src/repositories/product_repository.py
  - Agregar índices en migrations
  - Modificar tests relacionados
Alcance-Prohibido:
  - Cambiar estructura de tablas existentes
  - Modificar contratos de API
  - Tocar código de pricing o inventario
Autonomía: con-revisión
Aprobación: 1 peer reviewer con experiencia en DB
Evidencia:
  - Tests de integración pasan
  - Benchmark antes/después documentado
  - EXPLAIN ANALYZE de queries optimizadas
  - No hay degradación en otros endpoints
```

#### Nivel Alto (30-60 minutos totales)

| Actividad | Tiempo | Responsable |
|-----------|--------|-------------|
| Completar Policy Ticket exhaustivo | 10-15 min | Desarrollador |
| Documentar análisis de riesgo | 5-10 min | Desarrollador + Tech Lead |
| Revisión técnica profunda | 10-15 min | Tech Lead |
| Revisión de dominio | 5-10 min | Domain Expert |
| Verificación de evidencias de seguridad | 5-10 min | Automático + Humano |
| Aprobación multi-parte | 5 min | Tech Lead + Expert |

*Ejemplo de Policy Ticket nivel alto:*
```yaml
PT-ID: PT-2026-0144
Dominio: autenticación
Riesgo: alto
Responsable: @maria.garcia (Tech Lead)
Intención: Implementar rotación automática de refresh tokens
Contexto: |
  Auditoría de seguridad identificó que tokens no rotan,
  permitiendo uso indefinido si son comprometidos.
Alcance-Permitido:
  - Modificar /src/auth/token_service.py
  - Agregar campo 'generation' a tabla refresh_tokens
  - Modificar endpoint /auth/refresh
  - Actualizar tests de autenticación
Alcance-Prohibido:
  - Cambiar algoritmo de firma de JWT
  - Modificar flujo de login inicial
  - Tocar integración con SSO externo
  - Cambiar tiempos de expiración sin aprobación adicional
Autonomía: propuesta-solamente
  La IA genera propuesta de implementación.
  Humano revisa línea por línea antes de crear PR.
Aprobación:
  - Tech Lead (obligatorio)
  - Security Champion (obligatorio)
  - Backend Lead (opcional pero recomendado)
Evidencia-Requerida:
  - Tests unitarios con cobertura >90% del nuevo código
  - Tests de integración del flujo completo
  - Escaneo SAST sin vulnerabilidades críticas o altas
  - Escaneo de dependencias actualizado
  - Documentación de comportamiento ante edge cases:
    * Token expirado durante rotación
    * Múltiples requests simultáneos de refresh
    * Token revocado manualmente
Criterios-Rechazo:
  - Cualquier vulnerabilidad de seguridad identificada
  - Degradación de performance >10% en /auth/refresh
  - Incompatibilidad con clientes móviles existentes
```

#### Nivel Crítico (1-2 horas totales)

| Actividad | Tiempo | Responsable |
|-----------|--------|-------------|
| Completar Policy Ticket con análisis completo | 15-20 min | Desarrollador + Tech Lead |
| Threat modeling del cambio | 15-20 min | Security + Tech Lead |
| Revisión de impacto en compliance | 10-15 min | Compliance/Legal |
| Desarrollo supervisado (IA como consultor) | Variable | Desarrollador Senior |
| Revisión de código exhaustiva | 15-20 min | Tech Lead + Security |
| Penetration testing del cambio | 10-20 min | Security |
| Aprobación ejecutiva | 5-10 min | PO + Tech Lead + Security |

*Ejemplo de Policy Ticket nivel crítico:*
```yaml
PT-ID: PT-2026-0145
Dominio: pagos
Riesgo: crítico
Responsable-Principal: @carlos.mendez (Tech Lead)
Responsable-Seguridad: @ana.security (Security Lead)
Responsable-Producto: @jose.po (Product Owner)

Intención: |
  Implementar soporte para nuevo procesador de pagos (Stripe Connect)
  para habilitar marketplace de vendedores.

Contexto-Negocio: |
  - Fecha límite contractual: 2026-03-15
  - Impacto estimado: $2M ARR en comisiones
  - Regulaciones aplicables: PCI-DSS, PSD2

Análisis-de-Riesgo:
  Probabilidad-Error: Media (nuevo procesador, código complejo)
  Impacto-Error: Crítico (pérdida financiera directa, daño reputacional)
  Riesgo-Residual: Alto (requiere máxima supervisión)

Alcance-Permitido:
  - Crear módulo /src/payments/stripe_connect/
  - Integrar con Stripe Connect API
  - Crear tablas para connected_accounts y transfers
  - Implementar webhooks para eventos de Stripe
  - Crear endpoints admin para gestión de cuentas

Alcance-Explícitamente-Prohibido:
  - Modificar procesador de pagos existente (legacy)
  - Almacenar datos de tarjeta (solo tokens)
  - Implementar lógica de retry automático sin aprobación
  - Desplegar a producción sin período de shadow mode
  - Procesar pagos reales en ambiente de prueba

Autonomía: análisis-solamente
  La IA puede:
  - Analizar documentación de Stripe
  - Sugerir estructura de código
  - Generar tests unitarios
  - Identificar edge cases

  La IA NO puede:
  - Escribir código de producción directamente
  - Generar PRs automáticamente
  - Modificar configuración de seguridad

Aprobación-Requerida:
  Fase-Diseño:
    - Tech Lead (obligatorio)
    - Security Lead (obligatorio)
    - Arquitecto (obligatorio)
  Fase-Implementación:
    - Code review por 2 seniors (obligatorio)
    - Security review por Security Lead (obligatorio)
  Fase-Despliegue:
    - Product Owner (obligatorio)
    - Tech Lead (obligatorio)
    - Security Lead (obligatorio)
    - CTO (notificación)

Evidencia-Requerida:
  Pre-Desarrollo:
    - [ ] Threat model documentado
    - [ ] Diseño técnico aprobado
    - [ ] Checklist PCI-DSS revisado

  Pre-Merge:
    - [ ] Tests unitarios >95% cobertura
    - [ ] Tests de integración con Stripe sandbox
    - [ ] SAST sin vulnerabilidades (ninguna)
    - [ ] DAST ejecutado y limpio
    - [ ] Dependency scan actualizado
    - [ ] Secrets scan ejecutado

  Pre-Producción:
    - [ ] Penetration test ejecutado
    - [ ] Load test: 1000 TPS sin degradación
    - [ ] Runbook de rollback documentado
    - [ ] Alertas configuradas
    - [ ] Shadow mode por 72h sin errores

Criterios-Rechazo-Absolutos:
  - Cualquier vulnerabilidad de seguridad
  - Almacenamiento de datos de tarjeta
  - Falla en tests de integración con Stripe
  - Ausencia de cualquier evidencia requerida
  - Objeción de Security Lead

Plan-Rollback:
  Trigger: Error rate >0.1% o latencia p99 >500ms
  Acción: Feature flag off + notificación inmediata
  Responsable: On-call engineer + Tech Lead
  Tiempo-Máximo-Rollback: 5 minutos
```

### Reglas de Escalación de Riesgo

El nivel de riesgo puede **escalar automáticamente** bajo ciertas condiciones:

| Condición | Escalación |
|-----------|------------|
| Cambio toca >500 líneas de código | +1 nivel |
| Cambio afecta >3 módulos diferentes | +1 nivel |
| Cambio modifica configuración de seguridad | Mínimo Alto |
| Cambio afecta datos de usuario | Mínimo Alto |
| Cambio involucra dinero/transacciones | Automáticamente Crítico |
| Cambio en producción fuera de horario | +1 nivel |
| Desarrollador tiene <6 meses en el proyecto | +1 nivel |

### Métricas de Eficiencia del Overhead

Para asegurar que el overhead se mantiene proporcional, monitorear:

| Métrica | Target | Alarma |
|---------|--------|--------|
| Tiempo promedio PT nivel bajo | <10 min | >15 min |
| Tiempo promedio PT nivel medio | <30 min | >45 min |
| % de PTs rechazados por burocracia | <5% | >10% |
| % de PTs correctamente clasificados | >90% | <80% |
| Ratio cambios/PT (bajo riesgo) | >3 | <1 |

### Automatización del Overhead

Para reducir fricción sin perder control, automatizar:

**Nivel Bajo:**
```yaml
# .github/workflows/auto-approve-low-risk.yml
name: Auto-approve Low Risk PRs
on:
  pull_request:
    types: [opened, synchronize]

jobs:
  classify-and-approve:
    runs-on: ubuntu-latest
    steps:
      - name: Check if low-risk domain
        id: risk
        run: |
          # Analizar archivos modificados
          MODIFIED=$(gh pr view ${{ github.event.pull_request.number }} --json files -q '.files[].path')

          # Dominios bajo riesgo
          LOW_RISK_PATTERNS="^tests/|^docs/|\.md$|^\.github/|^styles/"

          if echo "$MODIFIED" | grep -vE "$LOW_RISK_PATTERNS" | grep -q .; then
            echo "risk=higher" >> $GITHUB_OUTPUT
          else
            echo "risk=low" >> $GITHUB_OUTPUT
          fi

      - name: Auto-approve if low risk
        if: steps.risk.outputs.risk == 'low'
        run: gh pr review ${{ github.event.pull_request.number }} --approve
```

**Nivel Medio y Superior:**
```yaml
# Template automático de PR según riesgo detectado
name: Generate Risk-Appropriate Checklist
on:
  pull_request:
    types: [opened]

jobs:
  add-checklist:
    runs-on: ubuntu-latest
    steps:
      - name: Determine risk level
        id: risk
        run: |
          # Lógica de clasificación basada en archivos modificados
          # ...

      - name: Add checklist to PR
        run: |
          RISK="${{ steps.risk.outputs.level }}"

          if [ "$RISK" == "high" ]; then
            gh pr comment ${{ github.event.pull_request.number }} --body "
            ## Checklist Obligatorio - Riesgo Alto

            ### Pre-Review
            - [ ] Policy Ticket vinculado: PT-____
            - [ ] Tech Lead notificado: @____
            - [ ] Análisis de impacto documentado

            ### Evidencias
            - [ ] Tests pasan (>90% cobertura nuevo código)
            - [ ] SAST ejecutado y limpio
            - [ ] Security scan completado

            ### Aprobaciones
            - [ ] Tech Lead
            - [ ] Domain Expert
            "
          fi
```

Esta matriz operacionaliza el concepto de "fricción proporcional al riesgo" y proporciona parámetros concretos que los equipos pueden usar inmediatamente. El objetivo no es eliminar el overhead, sino asegurar que cada minuto invertido en gobernanza produce valor proporcional en reducción de riesgo.

---

# PARTE IV: TRANSFORMACIÓN DE ROLES

## Transformación de Roles en la Práctica Diaria

La adopción de un marco de trabajo IA-Native no se limita a introducir nuevos artefactos o a redefinir unidades de trabajo. Su impacto más profundo se manifiesta en la transformación de los roles y responsabilidades dentro de la organización. Cuando una parte significativa de la ejecución es delegada a agentes de inteligencia artificial, los roles tradicionales dejan de operar bajo los mismos supuestos que les dieron origen.

Uno de los cambios más visibles ocurre en el rol del Product Owner. En los modelos clásicos, su función principal consiste en maximizar el valor del producto priorizando funcionalidades y gestionando el backlog. En un entorno IA-Native, esta responsabilidad se amplía. El Product Owner debe considerar no solo qué se construye, sino cómo se construye y bajo qué nivel de riesgo. Autorizar o no ciertas intervenciones de la IA se convierte en una decisión estratégica, especialmente en dominios sensibles como pagos, seguridad o información crítica para el usuario.

El rol del Scrum Master o Agile Coach también se redefine de manera sustantiva. Su trabajo ya no se limita a facilitar ceremonias o a remover impedimentos organizacionales. En un sistema donde humanos y agentes coexisten, este rol se orienta a mantener la coherencia del sistema socio-técnico. Esto implica velar por la calidad de las políticas, detectar patrones de uso riesgoso de la IA y promover ajustes en los procesos antes de que los problemas se materialicen.

El Tech Lead experimenta quizás una de las transformaciones más profundas. Tradicionalmente, este rol se centra en decisiones técnicas, revisión de código y orientación al equipo. En un entorno IA-Native, el Tech Lead pasa a desempeñar un papel clave en el diseño de límites cognitivos. Define qué puede y qué no puede hacer la IA, establece estándares de evidencia, configura pipelines de verificación y decide en qué contextos la autonomía automática es aceptable. Su responsabilidad se desplaza de la supervisión del código a la supervisión del comportamiento del sistema.

El rol de QA deja de ser un eslabón final del proceso. En lugar de detectar errores después de que el código fue escrito, QA se convierte en un diseñador de mecanismos de verificación. Su foco está en construir tests automáticos, gates de calidad, análisis estáticos y sistemas de monitoreo que permitan detectar problemas de manera temprana y sistemática. En este contexto, la calidad deja de ser una actividad puntual y pasa a ser una propiedad emergente del sistema.

Junto a estos roles transformados, comienzan a emerger otros nuevos que no tienen un equivalente directo en los marcos tradicionales. Aparecen perfiles orientados al gobierno de la IA, responsables de definir políticas, evaluar riesgos asociados a modelos y auditar el comportamiento de los agentes. Estos roles combinan conocimientos técnicos, criterio organizacional y sensibilidad ética.

Es importante destacar que esta transformación no implica la desaparición de los roles existentes, sino su evolución. Las personas siguen siendo centrales, pero su trabajo se desplaza desde la ejecución directa hacia la toma de decisiones, la definición de límites y la asunción explícita de responsabilidades. El marco IA-Native no reduce la importancia del factor humano; por el contrario, lo vuelve más crítico.

Esta sección deja en claro que adoptar IA en el desarrollo de software no es una decisión puramente técnica. Es una decisión organizacional que redefine quién decide, quién autoriza y quién responde cuando las cosas salen bien o mal. Ignorar esta dimensión conduce a una adopción superficial y, en muchos casos, a problemas difíciles de revertir.

---

## ★ El Product Owner en el Marco IA-Native: De Priorizador de Valor a Gobernador de Riesgo ★

> **"En el desarrollo tradicional, el Product Owner decide qué construir. En el desarrollo IA-Native, decide qué construir, quién lo construye, bajo qué límites y asumiendo qué consecuencias."**

El Product Owner experimenta una de las transformaciones más profundas y complejas del marco IA-Native. Su rol, tradicionalmente centrado en maximizar el valor del producto mediante la priorización del backlog, se expande hacia una función de gobernanza que requiere nuevas competencias, nuevos instrumentos y una relación fundamentalmente diferente con el equipo de desarrollo.

Esta sección desarrolla en profundidad cómo el Product Owner inicia, ejecuta y evoluciona su rol dentro del paradigma IA-Native, proporcionando guías prácticas, ejemplos detallados y herramientas concretas para la transición.

### La Naturaleza del Cambio: Por Qué el PO No Puede Seguir Igual

En el modelo tradicional, el Product Owner opera con un supuesto implícito: todo el trabajo es realizado por humanos. Este supuesto simplifica enormemente su función. Las historias de usuario describen resultados deseados, el equipo las ejecuta, y el PO valida que el resultado coincide con la intención. La responsabilidad se distribuye de manera relativamente clara: el PO define el "qué", el equipo define el "cómo".

Cuando la inteligencia artificial entra como ejecutor, este modelo se quiebra. La IA puede producir resultados técnicamente correctos pero contextualmente inapropiados. Puede resolver el problema planteado introduciendo efectos secundarios no anticipados. Puede operar a velocidades que superan la capacidad humana de supervisión. Y, crucialmente, no puede asumir responsabilidad por sus acciones.

En este nuevo contexto, el Product Owner ya no puede limitarse a priorizar y validar. Debe gobernar: establecer límites antes de la ejecución, definir condiciones de aceptabilidad, clasificar riesgos y asumir responsabilidad explícita por las decisiones de delegación.

### Las Cinco Dimensiones del PO IA-Native

El rol del Product Owner IA-Native se articula en cinco dimensiones complementarias que expanden su función tradicional:

**Dimensión 1: Estratega de Valor (tradicional ampliada)**

El PO sigue siendo responsable de maximizar el valor del producto. Sin embargo, el concepto de "valor" se complejiza. Ya no se trata solo de funcionalidades entregadas, sino del balance entre velocidad de entrega, calidad del resultado, riesgo asumido y costo de supervisión.

Un Product Owner IA-Native debe preguntarse: ¿Vale la pena acelerar esta funcionalidad con IA si el costo de supervisión supera el beneficio? ¿El riesgo de un error en este dominio justifica la velocidad ganada? ¿Estamos generando deuda técnica de gobernanza al delegar sin controles adecuados?

**Dimensión 2: Clasificador de Riesgo**

Esta dimensión es completamente nueva. El PO debe clasificar cada área funcional del producto según su nivel de riesgo, considerando:

- **Impacto de un error**: ¿Qué sucede si la IA se equivoca aquí?
- **Reversibilidad**: ¿Podemos deshacer el daño fácilmente?
- **Visibilidad**: ¿Un error sería detectado rápidamente?
- **Regulación**: ¿Hay implicaciones legales o de compliance?
- **Sensibilidad de datos**: ¿Se manejan datos personales, financieros o de salud?

**Dimensión 3: Diseñador de Autonomía**

Para cada dominio de riesgo, el PO define qué nivel de autonomía es apropiado para la IA. Esta decisión no es puramente técnica; es estratégica. Implica balancear la velocidad de entrega contra el control, la innovación contra la seguridad.

**Dimensión 4: Definidor de Evidencias**

El PO tradicional define criterios de aceptación. El PO IA-Native define además qué evidencias debe producir la IA para demostrar que su trabajo es válido. Esto incluye tests, métricas, diffs de código, análisis de seguridad y cualquier otro artefacto que permita tomar una decisión informada.

**Dimensión 5: Responsable Último**

Quizás el cambio más importante: el PO asume responsabilidad explícita por las decisiones de delegación. Cuando autoriza a la IA a trabajar en un dominio, está firmando un contrato implícito: si algo sale mal, la responsabilidad es suya.

### El Ciclo de Trabajo del Product Owner IA-Native

El trabajo del PO se organiza en un ciclo continuo que integra sus nuevas responsabilidades:

```
┌─────────────────────────────────────────────────────────────────────────────┐
│              CICLO DE TRABAJO DEL PRODUCT OWNER IA-NATIVE                   │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                             │
│  ┌──────────────┐                                                          │
│  │   DESCUBRIR  │  Identificar necesidades de negocio y usuarios           │
│  │              │  (igual que antes)                                       │
│  └──────┬───────┘                                                          │
│         │                                                                   │
│         ▼                                                                   │
│  ┌──────────────┐                                                          │
│  │  CLASIFICAR  │  NUEVO: Determinar dominio de riesgo de cada necesidad   │
│  │              │  ¿Bajo? ¿Medio? ¿Alto? ¿Crítico?                         │
│  └──────┬───────┘                                                          │
│         │                                                                   │
│         ▼                                                                   │
│  ┌──────────────┐                                                          │
│  │   DEFINIR    │  NUEVO: Establecer nivel de autonomía para la IA         │
│  │  AUTONOMÍA   │  ¿Completa? ¿Con checkpoints? ¿Supervisada? ¿Prohibida?  │
│  └──────┬───────┘                                                          │
│         │                                                                   │
│         ▼                                                                   │
│  ┌──────────────┐                                                          │
│  │   REDACTAR   │  TRANSFORMADO: Crear Policy Ticket en lugar de User Story│
│  │    POLICY    │  Incluir límites, prohibiciones, evidencias requeridas   │
│  │    TICKET    │                                                          │
│  └──────┬───────┘                                                          │
│         │                                                                   │
│         ▼                                                                   │
│  ┌──────────────┐                                                          │
│  │  SUPERVISAR  │  NUEVO: Monitorear ejecución según nivel de autonomía    │
│  │  EJECUCIÓN   │  Revisar checkpoints, validar evidencias parciales       │
│  └──────┬───────┘                                                          │
│         │                                                                   │
│         ▼                                                                   │
│  ┌──────────────┐                                                          │
│  │   VALIDAR    │  TRANSFORMADO: Aceptar/rechazar basándose en evidencias  │
│  │  EVIDENCIAS  │  No solo funcionalidad, también gobernanza               │
│  └──────┬───────┘                                                          │
│         │                                                                   │
│         ▼                                                                   │
│  ┌──────────────┐                                                          │
│  │   APRENDER   │  NUEVO: Ajustar clasificaciones y niveles de autonomía   │
│  │   Y AJUSTAR  │  basándose en resultados reales                          │
│  └──────┬───────┘                                                          │
│         │                                                                   │
│         └──────────────────────────────────────────────────────────────────┘│
│                                                                             │
└─────────────────────────────────────────────────────────────────────────────┘
```

### Primer Día del PO IA-Native: Guía Paso a Paso

Cuando un Product Owner asume su rol bajo el marco IA-Native, debe realizar una serie de actividades fundacionales que establecen las bases para todo el trabajo posterior.

#### Paso 1: Auditoría de Dominios del Producto

El primer ejercicio consiste en mapear exhaustivamente todas las áreas funcionales del producto y clasificarlas según su nivel de riesgo. Este mapeo debe ser explícito, documentado y compartido con el equipo.

**Ejemplo: Sistema de Gestión de Restaurante**

```yaml
# MAPA DE DOMINIOS DE RIESGO - Sistema RestaurantePro
# Product Owner: María García
# Fecha: 2026-01-24
# Versión: 1.0

dominios:

  # ═══════════════════════════════════════════════════════════════
  # DOMINIO DE RIESGO BAJO (🟢)
  # Errores son visibles, reversibles y no afectan datos críticos
  # ═══════════════════════════════════════════════════════════════

  presentacion_menu:
    nombre: "Visualización del Menú"
    descripcion: "Componentes de UI para mostrar platos, categorías, fotos"
    nivel_riesgo: bajo
    justificacion:
      - "Errores son inmediatamente visibles para QA y usuarios"
      - "No hay persistencia de datos sensibles"
      - "Reversión es trivial (redeploy)"
      - "Impacto de error: estético, no funcional"
    autonomia_ia: completa
    evidencias_minimas:
      - "Tests de componentes pasando"
      - "Screenshots de renderizado"
      - "Validación de accesibilidad básica"
    responsable_dominio: "Tech Lead Frontend"

  documentacion_interna:
    nombre: "Documentación Técnica"
    descripcion: "READMEs, comentarios de código, guías de desarrollo"
    nivel_riesgo: bajo
    justificacion:
      - "No afecta comportamiento del sistema"
      - "Errores son fácilmente corregibles"
      - "Revisión humana es rápida"
    autonomia_ia: completa
    evidencias_minimas:
      - "Markdown válido"
      - "Links funcionando"
    responsable_dominio: "Tech Lead"

  tests_unitarios:
    nombre: "Generación de Tests Unitarios"
    descripcion: "Tests para funciones y componentes existentes"
    nivel_riesgo: bajo
    justificacion:
      - "Tests incorrectos fallan o dan falsos positivos detectables"
      - "No modifican código de producción"
      - "Mejoran la cobertura sin riesgo"
    autonomia_ia: completa
    evidencias_minimas:
      - "Tests ejecutando sin errores"
      - "Cobertura incrementada"
      - "No tests vacíos o triviales"
    responsable_dominio: "QA Lead"

  # ═══════════════════════════════════════════════════════════════
  # DOMINIO DE RIESGO MEDIO (🟡)
  # Errores pueden afectar experiencia pero son detectables
  # ═══════════════════════════════════════════════════════════════

  carrito_compras:
    nombre: "Carrito de Compras"
    descripcion: "Agregar/quitar items, calcular totales, persistir estado"
    nivel_riesgo: medio
    justificacion:
      - "Afecta experiencia de usuario directamente"
      - "Cálculos incorrectos son detectables en QA"
      - "No procesa pagos, solo prepara la orden"
      - "Datos no son permanentes hasta confirmar"
    autonomia_ia: con_checkpoints
    frecuencia_checkpoints: "cada 2 horas de trabajo"
    evidencias_minimas:
      - "Tests de integración pasando"
      - "Validación de cálculos con casos edge"
      - "Review de lógica de negocio"
    responsable_dominio: "Product Owner + Tech Lead"

  busqueda_filtros:
    nombre: "Búsqueda y Filtros"
    descripcion: "Sistema de búsqueda de platos, filtros por categoría, precio"
    nivel_riesgo: medio
    justificacion:
      - "Afecta encontrabilidad de productos"
      - "Errores no causan pérdida de datos"
      - "Impacto en conversión pero no en integridad"
    autonomia_ia: con_checkpoints
    frecuencia_checkpoints: "cada feature completado"
    evidencias_minimas:
      - "Tests de búsqueda con datasets reales"
      - "Métricas de relevancia"
      - "Tests de performance"
    responsable_dominio: "Product Owner"

  notificaciones:
    nombre: "Sistema de Notificaciones"
    descripcion: "Emails, push notifications, SMS para pedidos"
    nivel_riesgo: medio
    justificacion:
      - "Afecta comunicación con cliente"
      - "Errores pueden causar spam o silencio"
      - "No hay datos financieros involucrados"
    autonomia_ia: con_checkpoints
    frecuencia_checkpoints: "antes de cada integración"
    evidencias_minimas:
      - "Tests con mocks de servicios externos"
      - "Validación de templates"
      - "Rate limiting verificado"
    responsable_dominio: "Tech Lead Backend"

  # ═══════════════════════════════════════════════════════════════
  # DOMINIO DE RIESGO ALTO (🟠)
  # Errores pueden causar daño significativo, requieren supervisión
  # ═══════════════════════════════════════════════════════════════

  informacion_alergenos:
    nombre: "Información de Alérgenos"
    descripcion: "Datos de alérgenos por plato, advertencias, filtros especiales"
    nivel_riesgo: alto
    justificacion:
      - "CRÍTICO: Impacto directo en salud del usuario"
      - "Errores pueden causar reacciones alérgicas graves"
      - "Implicaciones legales significativas"
      - "Datos deben ser 100% precisos"
    autonomia_ia: supervisada
    restricciones:
      - "IA NO puede modificar datos de alérgenos en base de datos"
      - "IA NO puede inferir o completar información faltante"
      - "IA puede SOLO mostrar datos validados por humanos"
    actividades_permitidas:
      - "Crear componentes de visualización"
      - "Implementar filtros por tipo de alérgeno"
      - "Generar tests de consistencia"
      - "Reportar inconsistencias encontradas"
    evidencias_minimas:
      - "Validación de que NO se modificaron datos fuente"
      - "Tests de consistencia contra fuente de verdad"
      - "Review de seguridad"
      - "Aprobación explícita de Product Owner"
    responsable_dominio: "Product Owner (responsabilidad directa)"

  datos_usuario:
    nombre: "Gestión de Datos de Usuario"
    descripcion: "Perfiles, preferencias, historial de pedidos, direcciones"
    nivel_riesgo: alto
    justificacion:
      - "Datos personales protegidos por GDPR/LGPD"
      - "Errores pueden causar exposición de información"
      - "Implicaciones legales y de compliance"
    autonomia_ia: supervisada
    restricciones:
      - "IA NO puede acceder a datos de producción"
      - "IA NO puede modificar esquemas de datos personales"
      - "IA NO puede implementar lógica de exportación de datos"
    evidencias_minimas:
      - "Análisis de seguridad completado"
      - "Tests de privacidad"
      - "Review de compliance"
      - "Aprobación de Security Lead"
    responsable_dominio: "Security Lead + Product Owner"

  integraciones_externas:
    nombre: "Integraciones con Terceros"
    descripcion: "APIs de delivery, sistemas de inventario, proveedores"
    nivel_riesgo: alto
    justificacion:
      - "Errores pueden afectar operación de partners"
      - "Costos potenciales por uso incorrecto de APIs"
      - "Datos compartidos con terceros"
    autonomia_ia: supervisada
    restricciones:
      - "IA NO puede modificar credenciales"
      - "IA NO puede cambiar endpoints de producción"
      - "IA debe usar siempre ambientes de sandbox"
    evidencias_minimas:
      - "Tests contra sandbox verificados"
      - "Validación de manejo de errores"
      - "Review de contratos de API"
    responsable_dominio: "Tech Lead + Partner Manager"

  # ═══════════════════════════════════════════════════════════════
  # DOMINIO DE RIESGO CRÍTICO (🔴)
  # La IA solo puede asistir, nunca ejecutar autónomamente
  # ═══════════════════════════════════════════════════════════════

  procesamiento_pagos:
    nombre: "Procesamiento de Pagos"
    descripcion: "Integración con pasarelas, transacciones, reembolsos"
    nivel_riesgo: critico
    justificacion:
      - "CRÍTICO: Dinero real involucrado"
      - "Regulaciones financieras estrictas (PCI-DSS)"
      - "Errores pueden causar pérdidas financieras directas"
      - "Fraude potencial si hay vulnerabilidades"
    autonomia_ia: solo_asistencia
    la_ia_puede:
      - "Sugerir estructura de código"
      - "Revisar código existente y señalar problemas"
      - "Generar tests (que serán revisados manualmente)"
      - "Documentar flujos existentes"
    la_ia_no_puede:
      - "Escribir código que se integre directamente"
      - "Modificar configuraciones de pasarelas"
      - "Acceder a credenciales de pago"
      - "Tocar lógica de transacciones existente"
    proceso_obligatorio:
      - "Todo código escrito por humano senior"
      - "Code review por dos personas mínimo"
      - "Auditoría de seguridad externa"
      - "Tests en sandbox con datos sintéticos"
      - "Aprobación de Finance + Security + PO"
    responsable_dominio: "CTO + Finance Lead + Product Owner"

  autenticacion_seguridad:
    nombre: "Autenticación y Seguridad"
    descripcion: "Login, tokens, permisos, sesiones, 2FA"
    nivel_riesgo: critico
    justificacion:
      - "CRÍTICO: Puerta de entrada al sistema"
      - "Vulnerabilidades exponen todo el sistema"
      - "Implicaciones legales severas por brechas"
    autonomia_ia: solo_asistencia
    la_ia_puede:
      - "Analizar código existente por vulnerabilidades"
      - "Sugerir mejoras de seguridad"
      - "Documentar flujos de autenticación"
    la_ia_no_puede:
      - "Modificar lógica de autenticación"
      - "Cambiar políticas de permisos"
      - "Tocar manejo de tokens o sesiones"
    responsable_dominio: "Security Lead + CTO"

  migraciones_datos:
    nombre: "Migraciones de Base de Datos"
    descripcion: "Cambios de esquema, migraciones de datos existentes"
    nivel_riesgo: critico
    justificacion:
      - "CRÍTICO: Cambios potencialmente irreversibles"
      - "Afecta integridad de todos los datos"
      - "Errores pueden corromper información histórica"
    autonomia_ia: solo_asistencia
    la_ia_puede:
      - "Sugerir estructura de migraciones"
      - "Generar scripts de rollback"
      - "Analizar impacto potencial"
    la_ia_no_puede:
      - "Ejecutar migraciones"
      - "Modificar datos de producción"
      - "Crear migraciones destructivas sin revisión"
    responsable_dominio: "DBA + Tech Lead + CTO"

# ═══════════════════════════════════════════════════════════════════════════
# RESUMEN DE DISTRIBUCIÓN DE RIESGO
# ═══════════════════════════════════════════════════════════════════════════

resumen:
  total_dominios: 12
  distribucion:
    bajo: 3      # 25% - Autonomía completa de IA
    medio: 3     # 25% - IA con checkpoints
    alto: 3      # 25% - IA supervisada
    critico: 3   # 25% - Solo asistencia de IA

  capacidad_supervision_equipo:
    dominios_alto_simultaneos: 2
    dominios_critico_simultaneos: 1

  nota: |
    Esta distribución indica que ~50% del producto puede beneficiarse
    de alta autonomía de IA, mientras que el otro 50% requiere
    supervisión significativa. El equipo debe dimensionar su capacidad
    de supervisión acorde a esta realidad.
```

#### Paso 2: Crear la Matriz de Decisión de Autonomía

Una vez mapeados los dominios, el PO debe crear una matriz que guíe las decisiones de autonomía para cada tipo de trabajo dentro de cada dominio.

```
┌─────────────────────────────────────────────────────────────────────────────┐
│           MATRIZ DE DECISIÓN DE AUTONOMÍA - Product Owner                   │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                             │
│  DOMINIO          │ ANÁLISIS │ DISEÑO │ CÓDIGO │ TESTS │ DEPLOY │ HOTFIX  │
│  ─────────────────┼──────────┼────────┼────────┼───────┼────────┼─────────│
│  🟢 Bajo          │   IA     │   IA   │   IA   │  IA   │  Auto  │   IA    │
│  🟡 Medio         │   IA     │  IA+H  │  IA+H  │  IA   │  H+IA  │  IA+H   │
│  🟠 Alto          │  IA+H    │   H    │  IA+H  │ IA+H  │   H    │   H     │
│  🔴 Crítico       │   H+IA   │   H    │   H    │ H+IA  │   H    │   H     │
│                                                                             │
│  Leyenda:                                                                   │
│  IA    = IA ejecuta autónomamente                                          │
│  IA+H  = IA ejecuta, humano revisa antes de integrar                       │
│  H+IA  = Humano ejecuta, IA asiste                                         │
│  H     = Solo humano, IA no participa                                      │
│  Auto  = Automatizado sin intervención                                      │
│                                                                             │
└─────────────────────────────────────────────────────────────────────────────┘
```

#### Paso 3: Establecer el Catálogo de Evidencias por Dominio

El PO define qué evidencias son necesarias para aceptar trabajo en cada nivel de riesgo.

```yaml
# CATÁLOGO DE EVIDENCIAS POR NIVEL DE RIESGO
# Product Owner: María García

evidencias_por_nivel:

  bajo:
    obligatorias:
      - tipo: "tests_pasando"
        descripcion: "Todos los tests unitarios y de componentes pasan"
        automatizable: true
      - tipo: "lint_clean"
        descripcion: "Sin errores de linter"
        automatizable: true
    opcionales:
      - tipo: "screenshots"
        descripcion: "Capturas de cambios visuales"
        cuando: "Cambios de UI"
    tiempo_revision_estimado: "15 minutos"
    puede_aprobar: ["Tech Lead", "Senior Dev", "PO"]

  medio:
    obligatorias:
      - tipo: "tests_pasando"
        descripcion: "Tests unitarios + integración pasan"
        automatizable: true
      - tipo: "code_review"
        descripcion: "Al menos un reviewer aprobó"
        automatizable: false
      - tipo: "no_regression"
        descripcion: "Tests de regresión pasando"
        automatizable: true
    opcionales:
      - tipo: "metricas_performance"
        descripcion: "No degradación de performance"
        cuando: "Cambios en queries o lógica pesada"
    tiempo_revision_estimado: "30-60 minutos"
    puede_aprobar: ["Tech Lead", "Senior Dev"]
    requiere_ademas: ["PO para funcionalidades de negocio"]

  alto:
    obligatorias:
      - tipo: "tests_completos"
        descripcion: "Unitarios + integración + E2E relevantes"
        automatizable: true
      - tipo: "code_review_senior"
        descripcion: "Review por senior del dominio"
        automatizable: false
      - tipo: "security_check"
        descripcion: "Análisis de seguridad automatizado"
        automatizable: true
      - tipo: "validacion_datos"
        descripcion: "Verificación de que datos sensibles no fueron alterados"
        automatizable: parcial
      - tipo: "approval_po"
        descripcion: "Aprobación explícita del Product Owner"
        automatizable: false
    opcionales:
      - tipo: "audit_trail"
        descripcion: "Log de todas las acciones de la IA"
        cuando: "Siempre recomendado"
    tiempo_revision_estimado: "2-4 horas"
    puede_aprobar: ["Tech Lead + PO conjuntamente"]

  critico:
    obligatorias:
      - tipo: "tests_exhaustivos"
        descripcion: "Cobertura completa incluyendo edge cases"
        automatizable: true
      - tipo: "code_review_multiple"
        descripcion: "Mínimo 2 reviewers senior"
        automatizable: false
      - tipo: "security_audit"
        descripcion: "Auditoría de seguridad manual o externa"
        automatizable: false
      - tipo: "compliance_check"
        descripcion: "Verificación de cumplimiento regulatorio"
        automatizable: parcial
      - tipo: "approval_chain"
        descripcion: "Aprobación de PO + Tech Lead + Security/Finance según dominio"
        automatizable: false
      - tipo: "rollback_plan"
        descripcion: "Plan de rollback documentado y probado"
        automatizable: false
    tiempo_revision_estimado: "1-2 días"
    puede_aprobar: ["Cadena de aprobación completa"]
```

### Las Ceremonias Ágiles Transformadas: Perspectiva del Product Owner

Cada ceremonia ágil tradicional se transforma cuando se adopta el marco IA-Native. A continuación se detalla cómo cambia el rol del PO en cada una.

#### Sprint Planning IA-Native

**Antes (tradicional):**
- PO presenta historias priorizadas
- Equipo estima en story points
- Se seleccionan historias según velocidad
- PO responde preguntas sobre requisitos

**Ahora (IA-Native):**

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                    SPRINT PLANNING - ROL DEL PO IA-NATIVE                   │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                             │
│  FASE 1: PRESENTACIÓN DE INTENCIONES (30 min)                              │
│  ┌─────────────────────────────────────────────────────────────────────┐   │
│  │ El PO presenta:                                                     │   │
│  │ • Policy Tickets priorizados (no User Stories)                     │   │
│  │ • Clasificación de riesgo de cada uno                              │   │
│  │ • Nivel de autonomía propuesto para la IA                          │   │
│  │ • Evidencias que requerirá para aceptar                            │   │
│  └─────────────────────────────────────────────────────────────────────┘   │
│                                                                             │
│  FASE 2: NEGOCIACIÓN DE AUTONOMÍA (45 min)                                 │
│  ┌─────────────────────────────────────────────────────────────────────┐   │
│  │ El equipo técnico puede:                                            │   │
│  │ • Proponer mayor autonomía si tienen controles adecuados           │   │
│  │ • Solicitar menor autonomía si ven riesgos no considerados         │   │
│  │ • Sugerir evidencias adicionales o diferentes                      │   │
│  │                                                                     │   │
│  │ El PO decide finalmente, asumiendo la responsabilidad              │   │
│  └─────────────────────────────────────────────────────────────────────┘   │
│                                                                             │
│  FASE 3: CÁLCULO DE CAPACIDAD DE SUPERVISIÓN (20 min)                      │
│  ┌─────────────────────────────────────────────────────────────────────┐   │
│  │ Nueva métrica: ¿Cuánta supervisión requiere este sprint?           │   │
│  │                                                                     │   │
│  │ Capacidad del equipo = Σ(horas disponibles para revisión)          │   │
│  │ Demanda de supervisión = Σ(horas de revisión por Policy Ticket)    │   │
│  │                                                                     │   │
│  │ Si Demanda > Capacidad:                                            │   │
│  │   → Reducir Policy Tickets de alto riesgo                          │   │
│  │   → O aumentar autonomía donde sea seguro                          │   │
│  │   → O traer más capacidad de supervisión                           │   │
│  └─────────────────────────────────────────────────────────────────────┘   │
│                                                                             │
│  FASE 4: COMPROMISO Y RESPONSABILIDADES (10 min)                           │
│  ┌─────────────────────────────────────────────────────────────────────┐   │
│  │ Se registra explícitamente:                                         │   │
│  │ • Qué Policy Tickets entran al sprint                              │   │
│  │ • Quién es responsable humano de cada uno                          │   │
│  │ • Qué nivel de autonomía tiene la IA en cada uno                   │   │
│  │ • El PO firma su aceptación de la estrategia de riesgo             │   │
│  └─────────────────────────────────────────────────────────────────────┘   │
│                                                                             │
└─────────────────────────────────────────────────────────────────────────────┘
```

#### Daily Standup IA-Native

**Antes:** El PO a veces asiste, escucha updates de progreso.

**Ahora:** El PO tiene un rol activo en el monitoreo de riesgos.

```yaml
# GUÍA DE DAILY PARA EL PRODUCT OWNER IA-NATIVE

duracion: "15 minutos estrictos"

estructura:

  minutos_1_a_5:
    nombre: "Estado de Policy Tickets"
    preguntas_clave:
      - "¿Qué Policy Tickets están en progreso?"
      - "¿Alguno cambió de nivel de riesgo?"
      - "¿Hay bloqueos de supervisión?"
    rol_po: "Escuchar, tomar nota de riesgos emergentes"

  minutos_5_a_10:
    nombre: "Alertas de Gobernanza"
    preguntas_clave:
      - "¿La IA intentó hacer algo fuera de su alcance autorizado?"
      - "¿Alguna evidencia falló o fue insuficiente?"
      - "¿Hay checkpoints pendientes de revisión?"
    rol_po: "Decidir si ajustar autonomía o prioridades"

  minutos_10_a_15:
    nombre: "Decisiones del PO"
    acciones_posibles:
      - "Autorizar continuación de Policy Tickets"
      - "Pausar trabajo que excede riesgo aceptable"
      - "Reasignar capacidad de supervisión"
      - "Escalar issues a stakeholders"

preguntas_que_el_po_debe_hacer:
  - "¿Cuántos diffs están pendientes de mi revisión?"
  - "¿Hay algún dominio alto/crítico con trabajo en progreso?"
  - "¿La IA produjo algo inesperado que necesito evaluar?"
  - "¿Tenemos capacidad de supervisión para lo planeado hoy?"
```

#### Sprint Review IA-Native

**Antes:** Demostración de funcionalidades completadas.

**Ahora:** Demostración + evaluación de gobernanza.

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                 SPRINT REVIEW - AGENDA DEL PO IA-NATIVE                     │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                             │
│  PARTE 1: DEMOSTRACIÓN FUNCIONAL (40% del tiempo)                          │
│  ──────────────────────────────────────────────────────────────────────────│
│  • Mostrar funcionalidades completadas (igual que siempre)                 │
│  • Feedback de stakeholders sobre el producto                              │
│                                                                             │
│  PARTE 2: REPORTE DE GOBERNANZA (30% del tiempo) ← NUEVO                   │
│  ──────────────────────────────────────────────────────────────────────────│
│  El PO presenta:                                                            │
│                                                                             │
│  📊 Métricas del Sprint:                                                   │
│  ┌─────────────────────────────────────────────────────────────────────┐   │
│  │ Policy Tickets completados: 12                                      │   │
│  │ - Con autonomía completa: 5 (42%)                                  │   │
│  │ - Con checkpoints: 4 (33%)                                         │   │
│  │ - Supervisados: 2 (17%)                                            │   │
│  │ - Solo asistencia: 1 (8%)                                          │   │
│  │                                                                     │   │
│  │ Incidentes de gobernanza: 2                                        │   │
│  │ - IA excedió alcance: 1 (detectado en checkpoint)                  │   │
│  │ - Evidencia insuficiente: 1 (se solicitó más)                      │   │
│  │                                                                     │   │
│  │ Rechazos por gobernanza: 1                                         │   │
│  │ - Motivo: Cambio tocaba dominio crítico sin autorización           │   │
│  └─────────────────────────────────────────────────────────────────────┘   │
│                                                                             │
│  🔄 Ajustes de Clasificación:                                              │
│  ┌─────────────────────────────────────────────────────────────────────┐   │
│  │ Dominio "notificaciones": medio → bajo                              │   │
│  │ Razón: 3 sprints sin incidentes, controles maduros                 │   │
│  │                                                                     │   │
│  │ Dominio "reportes": bajo → medio                                   │   │
│  │ Razón: Nuevo requisito de compliance requiere más supervisión      │   │
│  └─────────────────────────────────────────────────────────────────────┘   │
│                                                                             │
│  PARTE 3: APRENDIZAJES Y AJUSTES (30% del tiempo)                          │
│  ──────────────────────────────────────────────────────────────────────────│
│  • ¿Las clasificaciones de riesgo fueron correctas?                        │
│  • ¿Los niveles de autonomía fueron apropiados?                            │
│  • ¿Las evidencias requeridas fueron suficientes?                          │
│  • ¿Qué ajustaríamos para el próximo sprint?                               │
│                                                                             │
└─────────────────────────────────────────────────────────────────────────────┘
```

### Ejemplos Prácticos Detallados

#### Ejemplo 1: El PO Recibe una Solicitud de Nueva Funcionalidad

**Situación:** El área comercial solicita agregar "pago con QR" al sistema de restaurante.

**Proceso del PO IA-Native:**

```yaml
# ANÁLISIS DE SOLICITUD - Product Owner
# Fecha: 2026-01-24
# Solicitante: Área Comercial
# Funcionalidad: Pago con código QR

paso_1_clasificacion_de_riesgo:
  dominio_primario: "procesamiento_pagos"
  nivel_riesgo_heredado: critico

  analisis_detallado:
    impacto_de_error:
      financiero: "Alto - transacciones reales"
      reputacional: "Alto - confianza del usuario"
      legal: "Alto - regulaciones PCI-DSS"
      operacional: "Medio - se puede revertir con soporte"

    reversibilidad: "Parcial - transacciones no son reversibles automáticamente"

    visibilidad_de_errores: "Media - algunos errores son silenciosos"

    datos_sensibles: "Sí - información de pago"

  decision_final: "CRÍTICO - La IA solo puede asistir"

paso_2_descomposicion_en_policy_tickets:

  # Separo la funcionalidad en partes con diferentes niveles de riesgo

  pt_001:
    nombre: "UI del escáner de QR"
    riesgo: bajo
    autonomia: completa
    justificacion: "Solo UI, no toca lógica de pagos"

  pt_002:
    nombre: "Integración visual con flujo de checkout"
    riesgo: medio
    autonomia: con_checkpoints
    justificacion: "Afecta UX de pago pero no procesa transacciones"

  pt_003:
    nombre: "Generación de código QR con datos de pago"
    riesgo: alto
    autonomia: supervisada
    justificacion: "Maneja datos de transacción aunque no los procesa"

  pt_004:
    nombre: "Integración con pasarela de pago QR"
    riesgo: critico
    autonomia: solo_asistencia
    justificacion: "Transacciones financieras reales"

paso_3_comunicacion_a_stakeholders:
  mensaje: |
    Hemos analizado la solicitud de "Pago con QR".

    La funcionalidad se desarrollará en 4 fases:
    1. UI del escáner (IA autónoma) - Sprint 1
    2. Integración visual (IA con supervisión) - Sprint 1
    3. Generación de QR (IA supervisada) - Sprint 2
    4. Integración con pasarela (desarrollo humano) - Sprint 2-3

    El tiempo total estimado considera que las fases 3 y 4
    requieren supervisión intensiva y no pueden acelerarse
    con más autonomía de IA debido al nivel de riesgo.

    ¿Aceptan esta estrategia de implementación?

paso_4_registro_de_decision:
  decision: "Aprobar desarrollo con restricciones de autonomía"
  responsable: "María García (Product Owner)"
  fecha: "2026-01-24"
  revision_programada: "2026-02-15"
```

#### Ejemplo 2: El PO Debe Rechazar un Cambio Técnicamente Correcto

**Situación:** La IA completó un refactor del módulo de carrito que incluye optimizaciones en el cálculo de descuentos. El código funciona perfectamente, los tests pasan, pero...

```yaml
# EVALUACIÓN DE EVIDENCIAS - Product Owner
# Policy Ticket: PT-2026-089
# Descripción: Refactor de CarritoService

evidencias_recibidas:
  tests_unitarios: "✅ 47/47 pasando"
  tests_integracion: "✅ 12/12 pasando"
  cobertura: "✅ 94%"
  lint: "✅ Sin errores"
  performance: "✅ 15% más rápido"

revision_de_alcance:
  archivos_autorizados:
    - "src/services/CarritoService.ts"
    - "src/services/CarritoService.test.ts"

  archivos_modificados:
    - "src/services/CarritoService.ts" ✅
    - "src/services/CarritoService.test.ts" ✅
    - "src/services/DescuentosService.ts" ❌ # NO AUTORIZADO
    - "src/utils/PrecioCalculator.ts" ❌ # NO AUTORIZADO

problema_detectado: |
  La IA optimizó el cálculo de descuentos modificando archivos
  fuera de su alcance autorizado. Aunque el código es correcto
  y mejora el performance, el Policy Ticket solo autorizaba
  cambios en CarritoService.

  DescuentosService está clasificado como dominio de riesgo MEDIO
  y requiere checkpoints que no se realizaron.

decision_del_po:
  accion: "RECHAZAR"
  motivo: "Cambios fuera del alcance autorizado"

  instrucciones:
    - "Revertir cambios en DescuentosService.ts"
    - "Revertir cambios en PrecioCalculator.ts"
    - "Re-enviar solo con cambios en CarritoService"
    - "Si la optimización de descuentos es valiosa, crear nuevo Policy Ticket"

nuevo_policy_ticket_sugerido:
  id: "PT-2026-090"
  nombre: "Optimización de DescuentosService"
  riesgo: medio
  autonomia: con_checkpoints
  nota: |
    Extraído de PT-2026-089 porque la IA identificó
    una oportunidad de mejora legítima, pero fuera
    del alcance original.

aprendizaje_registrado:
  tipo: "IA excedió alcance"
  causa: "Optimización oportunista"
  accion_preventiva: |
    Agregar al prompt del agente: "Si identificas mejoras
    fuera de tu alcance, repórtalas pero no las implementes."
```

#### Ejemplo 3: El PO Ajusta Niveles de Autonomía Basándose en Resultados

**Situación:** Después de 3 sprints, el PO revisa el historial de un dominio para decidir si ajustar su clasificación.

```yaml
# REVISIÓN TRIMESTRAL DE CLASIFICACIÓN DE RIESGO
# Product Owner: María García
# Dominio: "Sistema de Notificaciones"
# Clasificación actual: MEDIO (🟡)

historial_ultimos_3_sprints:

  sprint_2026_S01:
    policy_tickets: 4
    incidentes: 0
    autonomia_usada: "con_checkpoints"
    tiempo_supervision: "8 horas"
    resultado: "100% exitoso"

  sprint_2026_S02:
    policy_tickets: 3
    incidentes: 1
    descripcion_incidente: "Rate limit mal configurado"
    causa: "Test no cubría escenario de alto volumen"
    impacto: "Bajo - detectado en staging"
    autonomia_usada: "con_checkpoints"
    tiempo_supervision: "6 horas"
    resultado: "Incidente contenido"

  sprint_2026_S03:
    policy_tickets: 5
    incidentes: 0
    autonomia_usada: "con_checkpoints"
    tiempo_supervision: "5 horas"
    resultado: "100% exitoso"
    mejora: "Se agregó test de rate limit"

analisis:
  total_policy_tickets: 12
  total_incidentes: 1
  tasa_exito: "91.7%"
  tendencia: "Mejorando"
  controles_maduros:
    - "Tests de rate limit agregados"
    - "Monitoreo de envíos en producción"
    - "Alertas de anomalías configuradas"

  riesgos_residuales:
    - "Integraciones con servicios externos (email, SMS)"
    - "Costos por uso excesivo de APIs"

decision:
  cambio_clasificacion: "NO por ahora"
  justificacion: |
    Aunque los resultados son buenos, el incidente del S02
    mostró que los tests no cubrían todos los escenarios.
    Mantendré la clasificación MEDIO por un trimestre más.

    Si el próximo trimestre continúa sin incidentes y se
    demuestra robustez en las integraciones externas,
    consideraré bajar a BAJO.

  ajuste_menor:
    frecuencia_checkpoints: "cada 4 horas → cada feature completo"
    justificacion: "Los checkpoints cada 4 horas resultaron excesivos"

proxima_revision: "2026-04-24"
```

### Anti-patrones del Product Owner IA-Native

El PO debe evitar caer en estos patrones disfuncionales:

```
┌─────────────────────────────────────────────────────────────────────────────┐
│              ANTI-PATRONES DEL PRODUCT OWNER IA-NATIVE                      │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                             │
│  ❌ ANTI-PATRÓN 1: "El PO Ausente"                                         │
│  ─────────────────────────────────────────────────────────────────────────│
│  Descripción: Delegar la clasificación de riesgo al equipo técnico         │
│  Síntoma: "Ustedes saben mejor que yo qué es riesgoso"                     │
│  Problema: La responsabilidad de negocio se diluye                         │
│  Solución: El PO DEBE estar presente en decisiones de autonomía            │
│                                                                             │
│  ❌ ANTI-PATRÓN 2: "El PO Paranoico"                                       │
│  ─────────────────────────────────────────────────────────────────────────│
│  Descripción: Clasificar todo como riesgo crítico                          │
│  Síntoma: "Mejor prevenir, pongamos todo en supervisión máxima"            │
│  Problema: Se pierde el beneficio de la IA, el equipo se frustra          │
│  Solución: Clasificar con criterio, no con miedo                           │
│                                                                             │
│  ❌ ANTI-PATRÓN 3: "El PO Optimista"                                       │
│  ─────────────────────────────────────────────────────────────────────────│
│  Descripción: Dar autonomía máxima para acelerar entregas                  │
│  Síntoma: "La IA sabe lo que hace, confiemos"                              │
│  Problema: Incidentes graves cuando algo sale mal                          │
│  Solución: Recordar que la responsabilidad es del PO, no de la IA         │
│                                                                             │
│  ❌ ANTI-PATRÓN 4: "El PO Inflexible"                                      │
│  ─────────────────────────────────────────────────────────────────────────│
│  Descripción: Nunca ajustar clasificaciones después de establecerlas       │
│  Síntoma: "Así lo definimos y así se queda"                                │
│  Problema: No se aprende de la experiencia                                 │
│  Solución: Revisiones trimestrales obligatorias de clasificaciones         │
│                                                                             │
│  ❌ ANTI-PATRÓN 5: "El PO Desconectado"                                    │
│  ─────────────────────────────────────────────────────────────────────────│
│  Descripción: No revisar las evidencias, solo aprobar lo que llega         │
│  Síntoma: "Si pasaron los tests, está bien"                                │
│  Problema: Se pierde el control de gobernanza                              │
│  Solución: El PO debe entender qué está aprobando, no solo que funciona   │
│                                                                             │
│  ❌ ANTI-PATRÓN 6: "El PO Técnico"                                         │
│  ─────────────────────────────────────────────────────────────────────────│
│  Descripción: Clasificar riesgo solo por complejidad técnica               │
│  Síntoma: "Es un CRUD simple, bajo riesgo"                                 │
│  Problema: El riesgo de negocio ≠ complejidad técnica                      │
│  Ejemplo: Un CRUD de datos de salud es técnicamente simple pero            │
│           tiene riesgo CRÍTICO por el tipo de datos                        │
│  Solución: Evaluar impacto de negocio, no solo dificultad técnica         │
│                                                                             │
└─────────────────────────────────────────────────────────────────────────────┘
```

### Métricas de Éxito del Product Owner IA-Native

El PO debe trackear métricas específicas que reflejen su nueva responsabilidad:

```yaml
# DASHBOARD DE MÉTRICAS DEL PRODUCT OWNER IA-NATIVE

metricas_de_gobernanza:

  cobertura_de_policy_tickets:
    definicion: "% de cambios que tienen Policy Ticket asociado"
    objetivo: ">95%"
    alerta: "<90%"
    importancia: "Crítica - cambios sin PT son ingobernables"

  precision_de_clasificacion:
    definicion: "% de Policy Tickets cuya clasificación de riesgo fue correcta"
    medicion: "Retrospectiva: ¿hubo incidentes en dominios clasificados como bajo?"
    objetivo: ">90%"
    alerta: "<80%"
    importancia: "Alta - mide la calidad de las decisiones del PO"

  tasa_de_rechazos_por_gobernanza:
    definicion: "% de PRs rechazados por exceder alcance o falta de evidencia"
    objetivo: "5-15%"
    alerta_bajo: "<5% puede indicar falta de control"
    alerta_alto: ">20% puede indicar políticas muy restrictivas"
    importancia: "Media - balance entre control y productividad"

  tiempo_de_supervision_por_sprint:
    definicion: "Horas dedicadas a revisar evidencias y aprobar cambios"
    objetivo: "Proporcional a Policy Tickets de alto riesgo"
    alerta: "Crecimiento sin aumento de Policy Tickets críticos"
    importancia: "Media - eficiencia del proceso"

metricas_de_valor:

  throughput_por_nivel_autonomia:
    definicion: "Policy Tickets completados, segmentados por nivel de autonomía"
    uso: "Entender cuánto trabajo se acelera con IA vs supervisado"

  ratio_autonomia_efectiva:
    definicion: "Policy Tickets con autonomía alta que completaron sin incidentes"
    objetivo: ">95%"
    importancia: "Alta - valida que las clasificaciones de bajo riesgo son correctas"

  incidentes_por_origen:
    definicion: "Incidentes en producción categorizados por fuente"
    categorias:
      - "Código de IA con autonomía completa"
      - "Código de IA supervisado"
      - "Código humano asistido por IA"
      - "Código puramente humano"
    uso: "Identificar si la IA introduce más o menos riesgos"

metricas_de_aprendizaje:

  ajustes_de_clasificacion:
    definicion: "Número de dominios reclasificados por trimestre"
    objetivo: "1-3 por trimestre"
    interpretacion: |
      0 = Posible rigidez, no se está aprendiendo
      >5 = Posible inestabilidad en las clasificaciones iniciales

  evolucion_de_autonomia:
    definicion: "Tendencia del nivel promedio de autonomía"
    objetivo: "Crecimiento gradual a medida que el equipo madura"
    alerta: "Decrecimiento sostenido indica problemas sistémicos"
```

### Checklist de Adopción para el Product Owner

Para facilitar la transición, el PO puede usar este checklist:

```
┌─────────────────────────────────────────────────────────────────────────────┐
│          CHECKLIST DE ADOPCIÓN - PRODUCT OWNER IA-NATIVE                    │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                             │
│  SEMANA 1: FUNDAMENTOS                                                      │
│  □ Leer y comprender el marco IA-Native completo                           │
│  □ Identificar todos los dominios funcionales del producto                  │
│  □ Realizar clasificación inicial de riesgo de cada dominio                │
│  □ Documentar justificación de cada clasificación                          │
│  □ Compartir clasificación con Tech Lead para validación técnica           │
│                                                                             │
│  SEMANA 2: INSTRUMENTACIÓN                                                  │
│  □ Crear template de Policy Ticket adaptado al producto                    │
│  □ Definir catálogo de evidencias por nivel de riesgo                      │
│  □ Establecer matriz de decisión de autonomía                              │
│  □ Configurar campos adicionales en herramienta de gestión (Jira, etc.)   │
│  □ Entrenar al equipo en el nuevo formato de Policy Ticket                 │
│                                                                             │
│  SEMANA 3: PILOTO                                                           │
│  □ Seleccionar 3-5 items del backlog para convertir en Policy Tickets     │
│  □ Ejecutar primer sprint con formato híbrido (PT + User Stories)         │
│  □ Participar activamente en checkpoints de dominios medio/alto           │
│  □ Documentar fricciones y ajustes necesarios                              │
│                                                                             │
│  SEMANA 4: RETROSPECTIVA Y AJUSTE                                          │
│  □ Evaluar resultados del piloto                                           │
│  □ Ajustar clasificaciones si fueron incorrectas                           │
│  □ Refinar catálogo de evidencias basándose en experiencia                 │
│  □ Decidir ritmo de migración del backlog completo                         │
│                                                                             │
│  MES 2: EXPANSIÓN                                                           │
│  □ Migrar progresivamente todo el backlog a Policy Tickets                 │
│  □ Establecer rutina de revisión de clasificaciones                        │
│  □ Implementar métricas de gobernanza en dashboard                         │
│  □ Capacitar a stakeholders en el nuevo modelo de reporting                │
│                                                                             │
│  MES 3+: MADUREZ                                                            │
│  □ Realizar primera revisión trimestral de clasificaciones                 │
│  □ Documentar casos de éxito y aprendizajes                                │
│  □ Proponer ajustes al marco basados en experiencia real                   │
│  □ Mentorear a otros POs en la adopción del marco                          │
│                                                                             │
└─────────────────────────────────────────────────────────────────────────────┘
```

### Conclusión: El Nuevo Contrato del Product Owner

El Product Owner IA-Native firma un nuevo contrato implícito con su organización:

> **"Acepto que mi rol ya no es solo decidir qué construir, sino gobernar cómo se construye cuando la ejecución es delegada a sistemas artificiales. Asumo la responsabilidad de clasificar riesgos, definir límites de autonomía, establecer evidencias requeridas y responder por las consecuencias de las delegaciones que autorizo. Entiendo que la velocidad que ofrece la IA viene acompañada de nuevas responsabilidades que no puedo eludir."**

Este nuevo contrato no disminuye la importancia del Product Owner; por el contrario, lo eleva a una posición más estratégica donde sus decisiones tienen impacto directo en el balance entre innovación y seguridad, entre velocidad y control, entre valor entregado y riesgo asumido.

---

## ★ El Tech Lead en el Marco IA-Native: De Revisor de Código a Diseñador de Límites Cognitivos ★

> **"El Tech Lead del futuro no será quien mejor escriba código, sino quien mejor diseñe las fronteras dentro de las cuales la IA puede operar de manera segura y efectiva."**

El Tech Lead experimenta quizás la transformación más técnicamente profunda del marco IA-Native. Su rol, tradicionalmente centrado en decisiones arquitectónicas, revisión de código y mentoría técnica del equipo, se expande hacia una función de ingeniería de restricciones donde debe diseñar los guardrails, políticas de integración y mecanismos de verificación que permiten aprovechar la velocidad de la IA sin sacrificar la integridad del sistema.

Esta sección desarrolla en profundidad cómo el Tech Lead inicia, ejecuta y evoluciona su rol dentro del paradigma IA-Native, proporcionando guías técnicas, ejemplos de configuración y herramientas concretas para la transición.

### La Naturaleza del Cambio: Por Qué el Tech Lead No Puede Seguir Igual

En el modelo tradicional, el Tech Lead opera como el guardián de la calidad técnica. Revisa pull requests, toma decisiones arquitectónicas, resuelve problemas complejos y mentora a desarrolladores junior. Su autoridad proviene de su expertise técnico y su conocimiento profundo del sistema.

Cuando la inteligencia artificial entra como ejecutor de código, este modelo se quiebra de formas sutiles pero críticas:

1. **Volumen de revisión**: La IA puede generar en horas lo que antes tomaba días. El Tech Lead no puede revisar todo con el mismo nivel de detalle.

2. **Naturaleza del código**: El código generado por IA tiene patrones diferentes al humano. Puede ser técnicamente correcto pero estilísticamente inconsistente, o seguir convenciones que el Tech Lead no reconoce.

3. **Velocidad de cambio**: Los refactors que antes tomaban sprints ahora pueden completarse en horas. El sistema evoluciona más rápido de lo que el Tech Lead puede asimilar cognitivamente.

4. **Responsabilidad difusa**: Cuando la IA introduce un bug, ¿quién es responsable? El Tech Lead debe establecer mecanismos que prevengan problemas antes de que ocurran.

En este nuevo contexto, el Tech Lead ya no puede limitarse a revisar y aprobar. Debe **diseñar sistemas de restricción** que operen automáticamente, definir **políticas de integración** que filtren cambios problemáticos, y establecer **estándares de evidencia** que permitan tomar decisiones informadas sin revisar cada línea de código.

### Las Seis Dimensiones del Tech Lead IA-Native

El rol del Tech Lead IA-Native se articula en seis dimensiones complementarias:

**Dimensión 1: Arquitecto de Sistema (tradicional ampliada)**

El Tech Lead sigue siendo responsable de la arquitectura del sistema. Sin embargo, ahora debe considerar cómo la IA interactúa con cada componente. ¿Qué partes del sistema pueden ser modificadas con alta autonomía? ¿Cuáles requieren restricciones estrictas? La arquitectura ya no es solo técnica; es también una arquitectura de permisos cognitivos.

**Dimensión 2: Diseñador de Guardrails**

Esta dimensión es completamente nueva. El Tech Lead debe diseñar las barreras que limitan lo que la IA puede hacer:

- **Guardrails de alcance**: Qué archivos/módulos puede tocar la IA
- **Guardrails de patrón**: Qué patrones de código son aceptables
- **Guardrails de dependencia**: Qué librerías puede agregar o modificar
- **Guardrails de seguridad**: Qué operaciones están prohibidas

**Dimensión 3: Ingeniero de Pipelines de Verificación**

El Tech Lead debe diseñar y mantener los pipelines de CI/CD que verifican automáticamente el trabajo de la IA:

- Tests automatizados estratificados por nivel de riesgo
- Análisis estático adaptado a código generado por IA
- Gates de calidad que bloquean cambios problemáticos
- Métricas de cobertura y complejidad

**Dimensión 4: Definidor de Estándares de Evidencia**

Para cada tipo de cambio, el Tech Lead define qué evidencias son necesarias para considerar el trabajo válido. Esto incluye no solo tests, sino también métricas de performance, análisis de seguridad, y validaciones de consistencia arquitectónica.

**Dimensión 5: Curador de Contexto para IA**

El Tech Lead es responsable de que la IA tenga el contexto necesario para producir código de calidad:

- Mantener actualizada la documentación técnica que la IA consume
- Definir ejemplos de código que sirvan como referencia
- Establecer convenciones explícitas en archivos de configuración
- Crear y mantener AGENTS.md y archivos de skills

**Dimensión 6: Mentor de Supervisión**

El Tech Lead debe enseñar al equipo cómo supervisar efectivamente el trabajo de la IA:

- Qué buscar en un diff generado por IA
- Cómo detectar patrones problemáticos
- Cuándo rechazar y cuándo solicitar regeneración
- Cómo dar feedback que mejore futuros resultados

### El Ciclo de Trabajo del Tech Lead IA-Native

```
┌─────────────────────────────────────────────────────────────────────────────┐
│              CICLO DE TRABAJO DEL TECH LEAD IA-NATIVE                       │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                             │
│  ┌──────────────┐                                                          │
│  │   DISEÑAR    │  Definir arquitectura considerando zonas de autonomía    │
│  │ ARQUITECTURA │  ¿Qué módulos pueden ser modificados por IA?             │
│  └──────┬───────┘                                                          │
│         │                                                                   │
│         ▼                                                                   │
│  ┌──────────────┐                                                          │
│  │   DEFINIR    │  Establecer restricciones técnicas por dominio           │
│  │  GUARDRAILS  │  Archivos protegidos, patrones prohibidos, límites       │
│  └──────┬───────┘                                                          │
│         │                                                                   │
│         ▼                                                                   │
│  ┌──────────────┐                                                          │
│  │  CONFIGURAR  │  Diseñar pipelines de CI/CD con gates automáticos        │
│  │   PIPELINES  │  Tests, linting, análisis de seguridad, métricas         │
│  └──────┬───────┘                                                          │
│         │                                                                   │
│         ▼                                                                   │
│  ┌──────────────┐                                                          │
│  │   CURAR      │  Mantener documentación y ejemplos para la IA            │
│  │   CONTEXTO   │  AGENTS.md, skills, convenciones, templates              │
│  └──────┬───────┘                                                          │
│         │                                                                   │
│         ▼                                                                   │
│  ┌──────────────┐                                                          │
│  │  SUPERVISAR  │  Revisar cambios de alto riesgo, validar gates           │
│  │   EJECUCIÓN  │  Aprobar/rechazar basándose en evidencias                │
│  └──────┬───────┘                                                          │
│         │                                                                   │
│         ▼                                                                   │
│  ┌──────────────┐                                                          │
│  │   AJUSTAR    │  Refinar guardrails basándose en incidentes              │
│  │   Y MEJORAR  │  Mejorar pipelines, actualizar documentación             │
│  └──────┬───────┘                                                          │
│         │                                                                   │
│         └──────────────────────────────────────────────────────────────────┘│
│                                                                             │
└─────────────────────────────────────────────────────────────────────────────┘
```

### Primer Día del Tech Lead IA-Native: Guía Paso a Paso

#### Paso 1: Auditoría Técnica del Sistema

El primer ejercicio del Tech Lead es mapear el sistema desde la perspectiva de la autonomía de IA.

```yaml
# MAPA TÉCNICO DE AUTONOMÍA - Sistema RestaurantePro
# Tech Lead: Carlos Rodríguez
# Fecha: 2026-01-24
# Versión: 1.0

arquitectura_sistema:
  tipo: "Monorepo con Clean Architecture"
  frontend: "React 19 + TypeScript + Tailwind"
  backend: "Python/FastAPI + SQLAlchemy"
  base_datos: "PostgreSQL"
  cache: "Redis"
  mensajería: "RabbitMQ"

# ═══════════════════════════════════════════════════════════════════════════
# CLASIFICACIÓN DE MÓDULOS POR AUTONOMÍA TÉCNICA
# ═══════════════════════════════════════════════════════════════════════════

modulos:

  # ─────────────────────────────────────────────────────────────────────────
  # CAPA DE PRESENTACIÓN (Frontend)
  # ─────────────────────────────────────────────────────────────────────────

  frontend_components:
    path: "src/components/"
    autonomia_tecnica: alta
    justificacion_tecnica:
      - "Componentes UI son aislados y testeables"
      - "Errores son visibles inmediatamente"
      - "No hay lógica de negocio crítica"
      - "Snapshot tests proveen red de seguridad"
    guardrails:
      - "No importar directamente de @/services/api"
      - "Usar solo hooks del directorio @/hooks"
      - "Seguir nomenclatura: PascalCase para componentes"
      - "Máximo 200 líneas por componente"
    tests_requeridos:
      - unit: true
      - snapshot: true
      - accessibility: "componentes interactivos"
    auto_merge: true
    condicion_auto_merge: "CI verde + coverage > 80%"

  frontend_hooks:
    path: "src/hooks/"
    autonomia_tecnica: media
    justificacion_tecnica:
      - "Hooks encapsulan lógica reutilizable"
      - "Pueden afectar múltiples componentes"
      - "Requieren testing más riguroso"
    guardrails:
      - "No llamadas directas a API (usar services)"
      - "No estado global (usar context providers)"
      - "Documentar dependencias y efectos"
    tests_requeridos:
      - unit: true
      - integration: "hooks que usan otros hooks"
    auto_merge: false
    requiere_review: "Tech Lead o Senior Frontend"

  frontend_pages:
    path: "src/pages/"
    autonomia_tecnica: media
    justificacion_tecnica:
      - "Pages orquestan componentes"
      - "Definen rutas y navegación"
      - "Errores pueden romper flujos completos"
    guardrails:
      - "No lógica de negocio en pages"
      - "Delegar a hooks y componentes"
      - "Mantener pages como orquestadores"
    tests_requeridos:
      - integration: true
      - e2e: "flujos críticos"

  # ─────────────────────────────────────────────────────────────────────────
  # CAPA DE APLICACIÓN (Backend)
  # ─────────────────────────────────────────────────────────────────────────

  backend_use_cases:
    path: "src/application/use_cases/"
    autonomia_tecnica: media
    justificacion_tecnica:
      - "Use cases contienen lógica de negocio"
      - "Son el corazón de la aplicación"
      - "Errores tienen impacto directo en comportamiento"
    guardrails:
      - "No acceso directo a base de datos"
      - "Usar solo repositorios inyectados"
      - "No dependencias de infraestructura"
      - "Seguir principio de responsabilidad única"
    tests_requeridos:
      - unit: true
      - integration: "con repositorios mock"
    patron_obligatorio: |
      class XxxUseCase:
          def __init__(self, repository: XxxRepository):
              self._repository = repository

          async def execute(self, request: XxxRequest) -> XxxResponse:
              # Lógica aquí

  backend_domain:
    path: "src/domain/"
    autonomia_tecnica: baja
    justificacion_tecnica:
      - "Dominio contiene reglas de negocio críticas"
      - "Entidades definen invariantes del sistema"
      - "Errores pueden corromper datos"
    guardrails:
      - "NO dependencias externas (solo stdlib)"
      - "NO imports de infrastructure o application"
      - "Entidades deben ser inmutables o con validación"
      - "Value Objects para datos compuestos"
    tests_requeridos:
      - unit: "100% de reglas de negocio"
    requiere_review: "Tech Lead obligatorio"

  backend_repositories:
    path: "src/infrastructure/repositories/"
    autonomia_tecnica: media
    justificacion_tecnica:
      - "Repositorios acceden a datos"
      - "Errores pueden causar pérdida de datos"
      - "Pero están bien aislados por interfaces"
    guardrails:
      - "Implementar interface del dominio"
      - "No lógica de negocio (solo persistencia)"
      - "Usar transacciones para operaciones múltiples"
      - "Logging obligatorio para operaciones de escritura"
    tests_requeridos:
      - integration: "con base de datos de test"

  # ─────────────────────────────────────────────────────────────────────────
  # CAPA DE INFRAESTRUCTURA
  # ─────────────────────────────────────────────────────────────────────────

  infrastructure_api:
    path: "src/infrastructure/api/"
    autonomia_tecnica: media
    justificacion_tecnica:
      - "Endpoints exponen la aplicación"
      - "Errores pueden exponer información sensible"
      - "Validación de entrada es crítica"
    guardrails:
      - "Usar Pydantic para validación de entrada"
      - "No lógica de negocio en endpoints"
      - "Manejo explícito de errores HTTP"
      - "Rate limiting en endpoints públicos"
    tests_requeridos:
      - integration: true
      - security: "endpoints con autenticación"

  infrastructure_database:
    path: "src/infrastructure/database/"
    autonomia_tecnica: muy_baja
    justificacion_tecnica:
      - "Modelos definen esquema de datos"
      - "Migraciones son irreversibles en producción"
      - "Errores pueden corromper toda la base"
    guardrails:
      - "NO modificar modelos sin migración"
      - "NO eliminar campos (solo deprecar)"
      - "Migraciones deben ser reversibles"
      - "Index obligatorio para campos de búsqueda"
    tests_requeridos:
      - migration: "verificar up y down"
    requiere_review: "Tech Lead + DBA"
    la_ia_puede:
      - "Sugerir estructura de migración"
      - "Generar modelo inicial"
      - "Documentar esquema existente"
    la_ia_no_puede:
      - "Ejecutar migraciones"
      - "Modificar modelos en producción"
      - "Eliminar campos o tablas"

  # ─────────────────────────────────────────────────────────────────────────
  # ARCHIVOS CRÍTICOS (Autonomía Mínima)
  # ─────────────────────────────────────────────────────────────────────────

  archivos_criticos:
    archivos:
      - path: "src/infrastructure/security/"
        autonomia: prohibida
        razon: "Autenticación y autorización"
      - path: "src/infrastructure/payments/"
        autonomia: prohibida
        razon: "Procesamiento de pagos"
      - path: ".env*"
        autonomia: prohibida
        razon: "Configuración sensible"
      - path: "alembic/versions/"
        autonomia: asistencia_solo
        razon: "Migraciones de base de datos"
      - path: "docker-compose.prod.yml"
        autonomia: prohibida
        razon: "Configuración de producción"

# ═══════════════════════════════════════════════════════════════════════════
# RESUMEN DE DISTRIBUCIÓN
# ═══════════════════════════════════════════════════════════════════════════

resumen:
  modulos_alta_autonomia: 1     # 11%
  modulos_media_autonomia: 5    # 56%
  modulos_baja_autonomia: 2     # 22%
  modulos_autonomia_prohibida: 1 # 11%

  estrategia: |
    El 67% del código puede beneficiarse de autonomía media-alta
    de IA con los guardrails adecuados. El 33% restante requiere
    supervisión estricta o está prohibido para modificación por IA.
```

#### Paso 2: Diseñar Sistema de Guardrails

El Tech Lead debe implementar guardrails técnicos que se apliquen automáticamente.

```yaml
# CONFIGURACIÓN DE GUARDRAILS TÉCNICOS
# Tech Lead: Carlos Rodríguez

# ═══════════════════════════════════════════════════════════════════════════
# GUARDRAILS DE ESTRUCTURA (Validación de archivos y paths)
# ═══════════════════════════════════════════════════════════════════════════

structure_guardrails:

  protected_paths:
    description: "Archivos que la IA no puede modificar directamente"
    paths:
      - "src/infrastructure/security/**"
      - "src/infrastructure/payments/**"
      - "alembic/versions/**"
      - ".env*"
      - "docker-compose.prod.yml"
      - "kubernetes/**"
      - ".github/workflows/**"
    enforcement: "pre-commit hook + CI check"
    mensaje_error: |
      ❌ GUARDRAIL VIOLATION: Intento de modificar archivo protegido
      Path: {path}
      Razón: Este archivo requiere modificación manual por Tech Lead
      Acción: Crear issue para revisión humana

  naming_conventions:
    components:
      pattern: "^[A-Z][a-zA-Z]+\\.tsx$"
      example: "UserProfile.tsx"
      error: "Componentes deben usar PascalCase"
    hooks:
      pattern: "^use[A-Z][a-zA-Z]+\\.ts$"
      example: "useUserData.ts"
      error: "Hooks deben comenzar con 'use' + PascalCase"
    use_cases:
      pattern: "^[a-z_]+_use_case\\.py$"
      example: "create_user_use_case.py"
      error: "Use cases deben usar snake_case + '_use_case'"
    repositories:
      pattern: "^[a-z_]+_repository\\.py$"
      example: "user_repository.py"

  file_size_limits:
    components: 200  # líneas
    hooks: 150
    use_cases: 300
    repositories: 400
    enforcement: "CI warning + block en PR si > 150%"

# ═══════════════════════════════════════════════════════════════════════════
# GUARDRAILS DE CÓDIGO (Análisis estático)
# ═══════════════════════════════════════════════════════════════════════════

code_guardrails:

  prohibited_patterns:
    - name: "Direct DB access in use cases"
      pattern: "from sqlalchemy|import asyncpg"
      scope: "src/application/**"
      severity: error
      mensaje: "Use cases no deben acceder directamente a DB. Usa repositorios."

    - name: "Business logic in API layer"
      pattern: "if.*price|if.*discount|calculate"
      scope: "src/infrastructure/api/**"
      severity: warning
      mensaje: "Posible lógica de negocio en capa de API. Mover a use case."

    - name: "Hardcoded credentials"
      pattern: "password.*=.*['\"]|api_key.*=.*['\"]|secret.*=.*['\"]"
      scope: "**/*.py"
      severity: error
      mensaje: "Credenciales hardcodeadas detectadas. Usar variables de entorno."

    - name: "Console.log in production code"
      pattern: "console\\.(log|debug|info)"
      scope: "src/**/*.ts"
      exclude: "**/*.test.ts"
      severity: warning
      mensaje: "console.log detectado. Usar logger estructurado."

    - name: "Any type in TypeScript"
      pattern: ": any(?![a-zA-Z])"
      scope: "src/**/*.ts"
      severity: warning
      mensaje: "Tipo 'any' detectado. Definir tipo específico."

    - name: "SQL string concatenation"
      pattern: "f['\"].*SELECT|f['\"].*INSERT|f['\"].*UPDATE|\\+.*SELECT"
      scope: "**/*.py"
      severity: error
      mensaje: "Posible SQL injection. Usar parámetros de query."

  required_patterns:
    - name: "Error handling in API endpoints"
      pattern: "try:|except.*:|HTTPException"
      scope: "src/infrastructure/api/routes/*.py"
      severity: warning
      mensaje: "Endpoint sin manejo explícito de errores."

    - name: "Type hints in Python functions"
      pattern: "def.*\\(.*:.*\\).*->"
      scope: "src/**/*.py"
      severity: warning
      mensaje: "Función sin type hints completos."

# ═══════════════════════════════════════════════════════════════════════════
# GUARDRAILS DE DEPENDENCIAS
# ═══════════════════════════════════════════════════════════════════════════

dependency_guardrails:

  prohibited_dependencies:
    python:
      - package: "pickle"
        reason: "Riesgo de deserialización insegura"
      - package: "eval"
        reason: "Ejecución de código arbitrario"
      - package: "subprocess"
        scope_exception: "scripts/**"
        reason: "Ejecución de comandos del sistema"
    javascript:
      - package: "eval"
        reason: "Ejecución de código arbitrario"
      - package: "dangerouslySetInnerHTML"
        scope_exception: "src/components/RichText/**"
        reason: "XSS potencial"

  new_dependency_policy:
    requires_approval: true
    approvers: ["tech-lead", "security-lead"]
    required_checks:
      - "npm audit / pip-audit sin vulnerabilidades críticas"
      - "Licencia compatible (MIT, Apache, BSD)"
      - "Mantenimiento activo (commits en últimos 6 meses)"
      - "Documentación de justificación en PR"

  layer_dependencies:
    description: "Reglas de dependencia entre capas (Clean Architecture)"
    rules:
      - from: "domain"
        can_import: []
        cannot_import: ["application", "infrastructure"]
      - from: "application"
        can_import: ["domain"]
        cannot_import: ["infrastructure"]
      - from: "infrastructure"
        can_import: ["domain", "application"]
    enforcement: "import-linter + CI"
```

#### Paso 3: Configurar Pipeline de CI/CD con Gates de IA

```yaml
# .github/workflows/ia-native-ci.yml
# Pipeline de CI/CD diseñado para trabajo con IA
# Tech Lead: Carlos Rodríguez

name: IA-Native CI/CD Pipeline

on:
  pull_request:
    branches: [main, develop]
  push:
    branches: [main, develop]

env:
  PYTHON_VERSION: "3.11"
  NODE_VERSION: "20"

jobs:

  # ═══════════════════════════════════════════════════════════════════════
  # GATE 1: Validación de Estructura (Rápido, bloquea temprano)
  # ═══════════════════════════════════════════════════════════════════════

  structure-validation:
    name: "🔒 Gate 1: Structure Validation"
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
        with:
          fetch-depth: 0  # Necesario para comparar con base branch

      - name: Check protected paths
        run: |
          # Obtener archivos modificados
          CHANGED_FILES=$(git diff --name-only origin/${{ github.base_ref }}...HEAD)

          # Paths protegidos
          PROTECTED_PATHS=(
            "src/infrastructure/security/"
            "src/infrastructure/payments/"
            "alembic/versions/"
            ".env"
            "docker-compose.prod.yml"
          )

          for file in $CHANGED_FILES; do
            for protected in "${PROTECTED_PATHS[@]}"; do
              if [[ $file == $protected* ]]; then
                echo "❌ BLOCKED: $file is in protected path $protected"
                echo "This file requires manual modification by Tech Lead"
                exit 1
              fi
            done
          done
          echo "✅ No protected paths modified"

      - name: Validate naming conventions
        run: |
          # Verificar componentes React
          find src/components -name "*.tsx" | while read file; do
            basename=$(basename "$file")
            if [[ ! $basename =~ ^[A-Z][a-zA-Z]+\.tsx$ ]]; then
              echo "❌ Invalid component name: $file"
              echo "Expected: PascalCase.tsx"
              exit 1
            fi
          done
          echo "✅ Naming conventions valid"

      - name: Check file sizes
        run: |
          MAX_COMPONENT_LINES=200
          MAX_HOOK_LINES=150

          find src/components -name "*.tsx" | while read file; do
            lines=$(wc -l < "$file")
            if [ $lines -gt $MAX_COMPONENT_LINES ]; then
              echo "⚠️ WARNING: $file has $lines lines (max: $MAX_COMPONENT_LINES)"
            fi
          done

  # ═══════════════════════════════════════════════════════════════════════
  # GATE 2: Análisis Estático (Detecta problemas de código)
  # ═══════════════════════════════════════════════════════════════════════

  static-analysis:
    name: "🔍 Gate 2: Static Analysis"
    runs-on: ubuntu-latest
    needs: structure-validation
    steps:
      - uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Setup Node
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}

      - name: Install dependencies
        run: |
          pip install ruff mypy bandit
          npm ci

      - name: Python Linting (Ruff)
        run: ruff check src/ --output-format=github

      - name: Python Type Checking (MyPy)
        run: mypy src/ --ignore-missing-imports

      - name: Python Security Scan (Bandit)
        run: bandit -r src/ -ll -ii

      - name: TypeScript Linting (ESLint)
        run: npm run lint

      - name: TypeScript Type Checking
        run: npm run type-check

      - name: Check prohibited patterns
        run: |
          # SQL Injection patterns
          if grep -rn "f['\"].*SELECT\|f['\"].*INSERT" src/; then
            echo "❌ Possible SQL injection detected"
            exit 1
          fi

          # Hardcoded credentials
          if grep -rn "password.*=.*['\"]" src/ --include="*.py" | grep -v "test"; then
            echo "❌ Hardcoded credentials detected"
            exit 1
          fi

          echo "✅ No prohibited patterns found"

      - name: Check layer dependencies
        run: |
          # Domain no debe importar de application o infrastructure
          if grep -rn "from src.application\|from src.infrastructure" src/domain/; then
            echo "❌ Domain layer importing from outer layers"
            exit 1
          fi

          # Application no debe importar de infrastructure
          if grep -rn "from src.infrastructure" src/application/; then
            echo "❌ Application layer importing from infrastructure"
            exit 1
          fi

          echo "✅ Layer dependencies valid"

  # ═══════════════════════════════════════════════════════════════════════
  # GATE 3: Tests (Valida comportamiento)
  # ═══════════════════════════════════════════════════════════════════════

  tests:
    name: "🧪 Gate 3: Tests"
    runs-on: ubuntu-latest
    needs: static-analysis
    services:
      postgres:
        image: postgres:15
        env:
          POSTGRES_PASSWORD: test
          POSTGRES_DB: test
        ports:
          - 5432:5432
      redis:
        image: redis:7
        ports:
          - 6379:6379

    steps:
      - uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Setup Node
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}

      - name: Install dependencies
        run: |
          pip install -r requirements.txt -r requirements-dev.txt
          npm ci

      - name: Run Python Unit Tests
        run: pytest tests/unit/ -v --cov=src --cov-report=xml

      - name: Run Python Integration Tests
        run: pytest tests/integration/ -v
        env:
          DATABASE_URL: postgresql://postgres:test@localhost:5432/test
          REDIS_URL: redis://localhost:6379

      - name: Run Frontend Unit Tests
        run: npm run test:unit -- --coverage

      - name: Run Frontend Component Tests
        run: npm run test:components

      - name: Check coverage thresholds
        run: |
          # Extraer coverage de Python
          PYTHON_COV=$(python -c "import xml.etree.ElementTree as ET; print(float(ET.parse('coverage.xml').getroot().attrib['line-rate'])*100)")

          if (( $(echo "$PYTHON_COV < 80" | bc -l) )); then
            echo "❌ Python coverage $PYTHON_COV% < 80%"
            exit 1
          fi

          echo "✅ Coverage thresholds met"

  # ═══════════════════════════════════════════════════════════════════════
  # GATE 4: Security Scan (Vulnerabilidades)
  # ═══════════════════════════════════════════════════════════════════════

  security:
    name: "🛡️ Gate 4: Security"
    runs-on: ubuntu-latest
    needs: structure-validation
    steps:
      - uses: actions/checkout@v4

      - name: Python dependency audit
        run: |
          pip install pip-audit
          pip-audit -r requirements.txt

      - name: Node dependency audit
        run: npm audit --audit-level=high

      - name: SAST Scan (Semgrep)
        uses: returntocorp/semgrep-action@v1
        with:
          config: >-
            p/security-audit
            p/secrets
            p/owasp-top-ten

      - name: Check for secrets
        uses: trufflesecurity/trufflehog@main
        with:
          path: ./
          base: ${{ github.event.pull_request.base.sha }}
          head: ${{ github.event.pull_request.head.sha }}

  # ═══════════════════════════════════════════════════════════════════════
  # GATE 5: Performance (No degradación)
  # ═══════════════════════════════════════════════════════════════════════

  performance:
    name: "⚡ Gate 5: Performance"
    runs-on: ubuntu-latest
    needs: tests
    if: github.event_name == 'pull_request'
    steps:
      - uses: actions/checkout@v4

      - name: Check bundle size
        run: |
          npm ci
          npm run build

          # Verificar que el bundle no creció más del 5%
          BUNDLE_SIZE=$(du -sb dist/ | cut -f1)
          MAX_SIZE=5242880  # 5MB

          if [ $BUNDLE_SIZE -gt $MAX_SIZE ]; then
            echo "⚠️ Bundle size $BUNDLE_SIZE exceeds limit $MAX_SIZE"
          fi

      - name: Run Lighthouse
        uses: treosh/lighthouse-ci-action@v10
        with:
          urls: http://localhost:3000
          budgetPath: ./lighthouse-budget.json

  # ═══════════════════════════════════════════════════════════════════════
  # DECISIÓN FINAL: Auto-merge o Requiere Review
  # ═══════════════════════════════════════════════════════════════════════

  decision:
    name: "📋 Final Decision"
    runs-on: ubuntu-latest
    needs: [structure-validation, static-analysis, tests, security]
    if: github.event_name == 'pull_request'
    steps:
      - uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Determine review requirements
        id: review
        run: |
          CHANGED_FILES=$(git diff --name-only origin/${{ github.base_ref }}...HEAD)

          NEEDS_TECH_LEAD=false
          NEEDS_SECURITY=false
          AUTO_MERGE_ELIGIBLE=true

          for file in $CHANGED_FILES; do
            # Archivos que requieren Tech Lead
            if [[ $file == src/domain/* ]] || \
               [[ $file == src/infrastructure/database/* ]] || \
               [[ $file == src/application/use_cases/* ]]; then
              NEEDS_TECH_LEAD=true
              AUTO_MERGE_ELIGIBLE=false
            fi

            # Archivos que requieren Security Lead
            if [[ $file == *security* ]] || \
               [[ $file == *auth* ]] || \
               [[ $file == *payment* ]]; then
              NEEDS_SECURITY=true
              AUTO_MERGE_ELIGIBLE=false
            fi
          done

          echo "needs_tech_lead=$NEEDS_TECH_LEAD" >> $GITHUB_OUTPUT
          echo "needs_security=$NEEDS_SECURITY" >> $GITHUB_OUTPUT
          echo "auto_merge_eligible=$AUTO_MERGE_ELIGIBLE" >> $GITHUB_OUTPUT

      - name: Request reviewers if needed
        if: steps.review.outputs.needs_tech_lead == 'true' || steps.review.outputs.needs_security == 'true'
        uses: actions/github-script@v6
        with:
          script: |
            const reviewers = [];
            if ('${{ steps.review.outputs.needs_tech_lead }}' === 'true') {
              reviewers.push('tech-lead-username');
            }
            if ('${{ steps.review.outputs.needs_security }}' === 'true') {
              reviewers.push('security-lead-username');
            }

            await github.rest.pulls.requestReviewers({
              owner: context.repo.owner,
              repo: context.repo.repo,
              pull_number: context.issue.number,
              reviewers: reviewers
            });

      - name: Add labels
        uses: actions/github-script@v6
        with:
          script: |
            const labels = ['ia-generated'];

            if ('${{ steps.review.outputs.auto_merge_eligible }}' === 'true') {
              labels.push('auto-merge-eligible');
            }
            if ('${{ steps.review.outputs.needs_tech_lead }}' === 'true') {
              labels.push('needs-tech-lead-review');
            }
            if ('${{ steps.review.outputs.needs_security }}' === 'true') {
              labels.push('needs-security-review');
            }

            await github.rest.issues.addLabels({
              owner: context.repo.owner,
              repo: context.repo.repo,
              issue_number: context.issue.number,
              labels: labels
            });

      - name: Summary
        run: |
          echo "## 📋 PR Analysis Summary" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "| Check | Status |" >> $GITHUB_STEP_SUMMARY
          echo "|-------|--------|" >> $GITHUB_STEP_SUMMARY
          echo "| Auto-merge eligible | ${{ steps.review.outputs.auto_merge_eligible }} |" >> $GITHUB_STEP_SUMMARY
          echo "| Needs Tech Lead review | ${{ steps.review.outputs.needs_tech_lead }} |" >> $GITHUB_STEP_SUMMARY
          echo "| Needs Security review | ${{ steps.review.outputs.needs_security }} |" >> $GITHUB_STEP_SUMMARY
```

#### Paso 4: Crear Sistema de Contexto para IA

El Tech Lead debe mantener documentación que la IA consume para producir código de calidad.

```markdown
# AGENTS.md (Ejemplo de archivo mantenido por Tech Lead)
# Este archivo es leído por agentes de IA antes de modificar código

## Arquitectura del Sistema

Este sistema sigue Clean Architecture con las siguientes capas:

```
src/
├── domain/           # Entidades y reglas de negocio (NO tocar sin review)
├── application/      # Use cases y lógica de aplicación
├── infrastructure/   # Implementaciones concretas (DB, API, etc.)
└── presentation/     # UI components (React)
```

## Reglas Críticas

### SIEMPRE
- Usar inyección de dependencias en use cases
- Type hints completos en Python
- Interfaces TypeScript para props de componentes
- Tests para toda lógica nueva
- Imports relativos dentro de cada capa

### NUNCA
- Lógica de negocio en componentes UI
- Acceso directo a DB desde use cases
- Modificar archivos en `src/infrastructure/security/`
- Eliminar campos de modelos de DB (solo deprecar)
- Commits directos a main/develop

## Patrones de Código

### Use Case Pattern (Python)
```python
from abc import ABC, abstractmethod
from dataclasses import dataclass

@dataclass(frozen=True)
class CreateUserRequest:
    email: str
    name: str

@dataclass(frozen=True)
class CreateUserResponse:
    id: str
    email: str

class CreateUserUseCase:
    def __init__(self, user_repository: UserRepository):
        self._repository = user_repository

    async def execute(self, request: CreateUserRequest) -> CreateUserResponse:
        # Validaciones de dominio
        # Lógica de negocio
        # Persistencia via repositorio
        pass
```

### Component Pattern (React)
```typescript
interface UserCardProps {
  user: User;
  onEdit?: (id: string) => void;
  className?: string;
}

export function UserCard({ user, onEdit, className }: UserCardProps) {
  return (
    <div className={cn("rounded-lg p-4", className)}>
      {/* Contenido */}
    </div>
  );
}
```

## Convenciones de Nombrado

| Tipo | Patrón | Ejemplo |
|------|--------|---------|
| Componente React | PascalCase | `UserProfile.tsx` |
| Hook | usePascalCase | `useUserData.ts` |
| Use Case | snake_case | `create_user_use_case.py` |
| Repository | snake_case | `user_repository.py` |
| Test | {name}.test.{ext} | `UserCard.test.tsx` |

## Auto-invoke Skills

| Acción | Skill a cargar |
|--------|---------------|
| Crear componente React | `react` |
| Crear use case Python | `clean-architecture` |
| Modificar modelo de DB | ⚠️ REQUIERE REVIEW MANUAL |
| Tocar seguridad | ⚠️ PROHIBIDO PARA IA |
```

### Las Ceremonias Ágiles: Perspectiva del Tech Lead

#### Sprint Planning desde el Tech Lead IA-Native

```
┌─────────────────────────────────────────────────────────────────────────────┐
│            SPRINT PLANNING - ROL DEL TECH LEAD IA-NATIVE                    │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                             │
│  ANTES DEL PLANNING (Preparación)                                          │
│  ┌─────────────────────────────────────────────────────────────────────┐   │
│  │ El Tech Lead debe tener listo:                                      │   │
│  │ • Mapa de módulos actualizado con autonomías técnicas              │   │
│  │ • Guardrails configurados para los dominios del sprint             │   │
│  │ • Pipeline de CI/CD con gates necesarios                           │   │
│  │ • Documentación de contexto (AGENTS.md) actualizada                │   │
│  └─────────────────────────────────────────────────────────────────────┘   │
│                                                                             │
│  DURANTE EL PLANNING                                                        │
│  ┌─────────────────────────────────────────────────────────────────────┐   │
│  │ Para cada Policy Ticket, el Tech Lead responde:                    │   │
│  │                                                                     │   │
│  │ 1. "¿Qué módulos/archivos tocará este trabajo?"                    │   │
│  │ 2. "¿Cuál es la autonomía técnica de esos módulos?"                │   │
│  │ 3. "¿Qué guardrails específicos aplican?"                          │   │
│  │ 4. "¿Qué tests son obligatorios?"                                  │   │
│  │ 5. "¿Puede auto-merge si CI pasa, o requiere mi review?"           │   │
│  │ 6. "¿Hay deuda técnica que debamos considerar?"                    │   │
│  └─────────────────────────────────────────────────────────────────────┘   │
│                                                                             │
│  NEGOCIACIÓN TÉCNICA                                                        │
│  ┌─────────────────────────────────────────────────────────────────────┐   │
│  │ El Tech Lead puede:                                                 │   │
│  │ • Proponer dividir un Policy Ticket por nivel de autonomía         │   │
│  │   "Separemos UI (alta) de lógica de negocio (media)"              │   │
│  │ • Solicitar evidencias técnicas adicionales                        │   │
│  │   "Necesito benchmarks de performance para esta optimización"      │   │
│  │ • Advertir sobre deuda técnica                                     │   │
│  │   "Este módulo tiene tests frágiles, agregaría riesgo"            │   │
│  │ • Sugerir orden de ejecución                                       │   │
│  │   "Primero el refactor, luego la feature nueva"                   │   │
│  └─────────────────────────────────────────────────────────────────────┘   │
│                                                                             │
│  OUTPUT DEL TECH LEAD                                                       │
│  ┌─────────────────────────────────────────────────────────────────────┐   │
│  │ Para cada Policy Ticket en el sprint:                              │   │
│  │ • Lista de módulos afectados                                       │   │
│  │ • Autonomía técnica asignada                                       │   │
│  │ • Guardrails específicos activados                                 │   │
│  │ • Tests obligatorios definidos                                     │   │
│  │ • Criterio de auto-merge vs review manual                          │   │
│  └─────────────────────────────────────────────────────────────────────┘   │
│                                                                             │
└─────────────────────────────────────────────────────────────────────────────┘
```

#### Code Review IA-Native

El Tech Lead debe desarrollar nuevas habilidades para revisar código generado por IA.

```yaml
# GUÍA DE CODE REVIEW PARA CÓDIGO GENERADO POR IA
# Tech Lead: Carlos Rodríguez

principios:
  - "El código de IA es diferente al humano - ajusta tu revisión"
  - "Busca patrones, no solo errores puntuales"
  - "La IA puede ser consistentemente incorrecta"
  - "Verifica que el código resuelve el problema, no solo que funciona"

checklist_review_ia:

  coherencia_arquitectonica:
    descripcion: "¿El código sigue la arquitectura establecida?"
    items:
      - "¿Respeta las capas de Clean Architecture?"
      - "¿Las dependencias van en la dirección correcta?"
      - "¿Usa los patrones definidos (repository, use case, etc.)?"
      - "¿Está en el directorio correcto?"
    red_flags:
      - "Import de infrastructure en domain"
      - "Lógica de negocio en controllers/routes"
      - "Acceso directo a DB sin repository"

  consistencia_de_estilo:
    descripcion: "¿El código es consistente con el existente?"
    items:
      - "¿Sigue las convenciones de nombrado?"
      - "¿El formato es consistente?"
      - "¿Los comentarios son útiles y no redundantes?"
      - "¿El nivel de abstracción es apropiado?"
    red_flags:
      - "Mezcla de estilos (camelCase y snake_case)"
      - "Comentarios obvios (// increment counter)"
      - "Sobre-ingeniería para casos simples"

  calidad_tecnica:
    descripcion: "¿El código es técnicamente sólido?"
    items:
      - "¿Maneja errores correctamente?"
      - "¿Es eficiente (no N+1 queries, no loops innecesarios)?"
      - "¿Es seguro (no SQL injection, no XSS)?"
      - "¿Es testeable?"
    red_flags:
      - "Try/catch vacíos o que tragan excepciones"
      - "Queries dentro de loops"
      - "String concatenation para SQL"
      - "Estado global mutable"

  completitud:
    descripcion: "¿El código está completo?"
    items:
      - "¿Hay TODOs sin resolver?"
      - "¿Faltan edge cases?"
      - "¿La validación de input es completa?"
      - "¿Hay tests para los casos importantes?"
    red_flags:
      - "# TODO: implement this"
      - "Falta de validación de null/undefined"
      - "Tests que solo prueban el happy path"

  patrones_de_ia:
    descripcion: "Patrones comunes en código generado por IA"
    items:
      - "¿Hay código copiado con variaciones menores?"
      - "¿Hay sobre-documentación o comentarios excesivos?"
      - "¿El código resuelve el problema o solo parece hacerlo?"
      - "¿Hay imports no utilizados?"
    red_flags:
      - "Funciones casi idénticas que podrían unificarse"
      - "Docstrings que repiten el nombre de la función"
      - "Código que compila pero no hace lo esperado"
      - "Dependencias agregadas innecesariamente"

proceso_de_review:

  paso_1_contexto:
    descripcion: "Entender qué se pidió"
    acciones:
      - "Leer el Policy Ticket asociado"
      - "Entender la intención, no solo los archivos"
      - "Verificar que el scope es el autorizado"

  paso_2_diff_general:
    descripcion: "Vista general de los cambios"
    acciones:
      - "¿Cuántos archivos se modificaron?"
      - "¿Los archivos modificados tienen sentido para el ticket?"
      - "¿Hay archivos modificados que no deberían estarlo?"

  paso_3_arquitectura:
    descripcion: "Verificar coherencia arquitectónica"
    acciones:
      - "¿Se respetan las capas?"
      - "¿Las dependencias son correctas?"
      - "¿Se usan los patrones del proyecto?"

  paso_4_detalle:
    descripcion: "Revisar código específico"
    acciones:
      - "Lógica de negocio correcta"
      - "Manejo de errores"
      - "Seguridad"
      - "Performance"

  paso_5_tests:
    descripcion: "Verificar cobertura de tests"
    acciones:
      - "¿Hay tests para la nueva funcionalidad?"
      - "¿Los tests prueban comportamiento, no implementación?"
      - "¿Cubren edge cases?"

  paso_6_decision:
    descripcion: "Decidir acción"
    opciones:
      - approve: "Todo bien, puede mergearse"
      - request_changes: "Hay problemas que deben corregirse"
      - regenerate: "El enfoque es incorrecto, pedir a la IA que regenere"
      - escalate: "Necesita review de otro experto (security, DBA, etc.)"
```

### Ejemplos Prácticos Detallados

#### Ejemplo 1: El Tech Lead Configura Guardrails para Nuevo Módulo

**Situación:** Se va a desarrollar un nuevo módulo de "Reportes Financieros".

```yaml
# CONFIGURACIÓN DE NUEVO MÓDULO - Tech Lead
# Módulo: Reportes Financieros
# Fecha: 2026-01-24

analisis_inicial:
  funcionalidad: "Generar reportes de ventas, ingresos, y proyecciones"
  datos_involucrados:
    - "Transacciones de pago (sensible)"
    - "Datos de clientes (PII)"
    - "Métricas de negocio (confidencial)"

  clasificacion_inicial:
    dominio_riesgo: alto
    justificacion: "Acceso a datos financieros y PII"

decision_arquitectonica:
  estructura_propuesta: |
    src/
    └── modules/
        └── reporting/
            ├── domain/
            │   ├── entities/
            │   │   ├── report.py           # Entidad Report
            │   │   └── report_period.py    # Value Object
            │   └── interfaces/
            │       └── report_repository.py # Interface
            ├── application/
            │   └── use_cases/
            │       ├── generate_sales_report.py
            │       └── generate_revenue_report.py
            ├── infrastructure/
            │   ├── repositories/
            │   │   └── sql_report_repository.py
            │   └── exporters/
            │       ├── pdf_exporter.py
            │       └── excel_exporter.py
            └── presentation/
                └── api/
                    └── report_routes.py

guardrails_especificos:

  acceso_a_datos:
    regla: "Solo acceso de lectura a transacciones"
    implementacion: |
      # El repositorio solo expone métodos de lectura
      class ReportRepository(ABC):
          @abstractmethod
          async def get_transactions_for_period(
              self,
              start: datetime,
              end: datetime
          ) -> list[Transaction]:
              pass

          # NO hay métodos de escritura
    verificacion: "Grep en CI por INSERT|UPDATE|DELETE en módulo reporting"

  filtrado_de_pii:
    regla: "Reportes no deben incluir PII directamente"
    implementacion: |
      # Los datos se agregan antes de salir del dominio
      class SalesReport:
          # ✅ Correcto: datos agregados
          total_customers: int
          revenue_by_segment: dict[str, Decimal]

          # ❌ Prohibido: datos individuales
          # customer_emails: list[str]
          # individual_transactions: list[Transaction]
    verificacion: "Pattern match en CI buscando campos PII"

  rate_limiting:
    regla: "Reportes pesados requieren rate limiting"
    implementacion: |
      @router.get("/reports/sales")
      @rate_limit(max_calls=10, period=60)  # 10 por minuto
      async def generate_sales_report(...):
          pass
    verificacion: "Verificar decorador rate_limit en todos los endpoints"

  audit_logging:
    regla: "Todo acceso a reportes debe loguearse"
    implementacion: |
      # Middleware de auditoría obligatorio
      @audit_log(action="REPORT_GENERATED", include_params=["period"])
      async def execute(self, request: GenerateReportRequest):
          pass
    verificacion: "Verificar decorador audit_log en use cases"

autonomia_por_componente:

  domain_entities:
    autonomia: media
    razon: "Entidades son simples pero definen estructura de datos"
    guardrails:
      - "No imports externos"
      - "Solo dataclasses inmutables"
    requiere_review: "Senior dev o Tech Lead"

  use_cases:
    autonomia: media
    razon: "Contienen lógica de negocio de reportes"
    guardrails:
      - "Solo lectura de datos"
      - "Audit logging obligatorio"
    requiere_review: "Tech Lead"

  repositories:
    autonomia: baja
    razon: "Acceden a datos financieros reales"
    guardrails:
      - "Solo SELECT permitidos"
      - "Queries parametrizadas obligatorias"
      - "Timeout máximo de 30 segundos"
    requiere_review: "Tech Lead + DBA"

  exporters:
    autonomia: alta
    razon: "Solo formatean datos ya filtrados"
    guardrails:
      - "No acceso a datos crudos"
      - "Input validado por use case"
    auto_merge: "Si CI pasa"

  api_routes:
    autonomia: media
    razon: "Exponen funcionalidad, requieren auth y rate limit"
    guardrails:
      - "Autenticación obligatoria"
      - "Rate limiting obligatorio"
      - "Validación de permisos"
    requiere_review: "Tech Lead"

pipeline_ci_especifico:
  gates_adicionales:
    - name: "PII Check"
      run: "python scripts/check_pii_exposure.py src/modules/reporting/"
    - name: "SQL Injection Check"
      run: "bandit -r src/modules/reporting/ -ll"
    - name: "Read-only verification"
      run: "grep -r 'INSERT\\|UPDATE\\|DELETE' src/modules/reporting/ && exit 1 || exit 0"

documentacion_para_ia:
  actualizar_agents_md: true
  contenido_a_agregar: |
    ## Módulo de Reportes

    ### Restricciones Críticas
    - Solo acceso de lectura a base de datos
    - No incluir PII en outputs de reportes
    - Audit logging obligatorio en todos los use cases
    - Rate limiting en todos los endpoints

    ### Patrones Obligatorios
    - Repositorios solo exponen métodos de lectura
    - Datos agregados antes de salir del dominio
    - Exporters reciben datos ya procesados
```

#### Ejemplo 2: El Tech Lead Detecta y Corrige un Problema de IA

**Situación:** La IA generó código que pasa todos los tests pero viola la arquitectura.

```yaml
# ANÁLISIS DE INCIDENTE - Tech Lead
# Tipo: Violación de arquitectura no detectada por CI
# Fecha: 2026-01-24

descripcion_del_problema:
  pr_number: "#1234"
  titulo: "Add caching to product queries"
  generado_por: "IA (Claude)"
  ci_status: "✅ Todos los tests pasando"

  codigo_problematico: |
    # src/application/use_cases/get_products_use_case.py

    import redis  # ❌ Import directo de infraestructura

    class GetProductsUseCase:
        def __init__(self, repository: ProductRepository):
            self._repository = repository
            self._cache = redis.Redis()  # ❌ Instanciación directa

        async def execute(self, filters: ProductFilters) -> list[Product]:
            cache_key = f"products:{filters.hash()}"

            # Intentar obtener de cache
            cached = self._cache.get(cache_key)  # ❌ Uso directo
            if cached:
                return json.loads(cached)

            # Si no está en cache, obtener de DB
            products = await self._repository.get_products(filters)

            # Guardar en cache
            self._cache.setex(cache_key, 3600, json.dumps(products))

            return products

  porque_paso_ci: |
    - Tests mockeaban el redis client correctamente
    - El código funcionaba y producía resultados correctos
    - No había un guardrail específico para imports de infrastructure

analisis_del_tech_lead:
  violaciones_detectadas:
    - tipo: "Violación de Clean Architecture"
      descripcion: "Use case importa directamente de infrastructure (redis)"
      severidad: alta

    - tipo: "Acoplamiento rígido"
      descripcion: "Cache hardcodeado, no inyectado"
      severidad: media

    - tipo: "Responsabilidad mezclada"
      descripcion: "Use case maneja lógica de cache"
      severidad: media

  porque_importa: |
    - Dificulta testing unitario real (mock esconde el problema)
    - Impide cambiar implementación de cache
    - Use case ahora depende de detalles de infrastructure
    - Viola el principio de inversión de dependencias

solucion_correcta:
  paso_1_definir_interface: |
    # src/domain/interfaces/cache_service.py

    from abc import ABC, abstractmethod
    from typing import TypeVar, Generic, Optional

    T = TypeVar('T')

    class CacheService(ABC, Generic[T]):
        @abstractmethod
        async def get(self, key: str) -> Optional[T]:
            pass

        @abstractmethod
        async def set(self, key: str, value: T, ttl_seconds: int) -> None:
            pass

  paso_2_implementacion_infrastructure: |
    # src/infrastructure/cache/redis_cache_service.py

    import redis.asyncio as redis
    from src.domain.interfaces import CacheService

    class RedisCacheService(CacheService[T]):
        def __init__(self, redis_client: redis.Redis):
            self._client = redis_client

        async def get(self, key: str) -> Optional[T]:
            value = await self._client.get(key)
            return json.loads(value) if value else None

        async def set(self, key: str, value: T, ttl_seconds: int) -> None:
            await self._client.setex(key, ttl_seconds, json.dumps(value))

  paso_3_use_case_corregido: |
    # src/application/use_cases/get_products_use_case.py

    from src.domain.interfaces import ProductRepository, CacheService

    class GetProductsUseCase:
        def __init__(
            self,
            repository: ProductRepository,
            cache: CacheService[list[Product]]  # ✅ Inyectado
        ):
            self._repository = repository
            self._cache = cache

        async def execute(self, filters: ProductFilters) -> list[Product]:
            cache_key = f"products:{filters.hash()}"

            cached = await self._cache.get(cache_key)
            if cached:
                return cached

            products = await self._repository.get_products(filters)
            await self._cache.set(cache_key, products, 3600)

            return products

acciones_correctivas:

  inmediatas:
    - accion: "Rechazar PR con explicación detallada"
      ejecutado: true
    - accion: "Solicitar regeneración con contexto adicional"
      ejecutado: true
      prompt_usado: |
        El código viola Clean Architecture. Los use cases no deben
        importar de infrastructure. Usa inyección de dependencias:
        1. Define interface CacheService en domain
        2. Implementa RedisCacheService en infrastructure
        3. Inyecta el cache service en el use case

  preventivas:
    - accion: "Agregar guardrail de imports"
      tipo: "CI check"
      implementacion: |
        # .github/workflows/architecture-check.yml
        - name: Check layer imports
          run: |
            # Application no puede importar de infrastructure
            if grep -rn "from src.infrastructure\|import redis\|import sqlalchemy" \
               src/application/; then
              echo "❌ Application layer importing from infrastructure"
              exit 1
            fi

    - accion: "Actualizar AGENTS.md con ejemplo de cache"
      tipo: "Documentación"
      contenido: |
        ### Patrón de Cache (Correcto)

        ❌ INCORRECTO - Import directo:
        ```python
        import redis
        class MyUseCase:
            def __init__(self):
                self._cache = redis.Redis()
        ```

        ✅ CORRECTO - Inyección de dependencias:
        ```python
        class MyUseCase:
            def __init__(self, cache: CacheService):
                self._cache = cache
        ```

    - accion: "Crear test de arquitectura"
      tipo: "Test automatizado"
      implementacion: |
        # tests/architecture/test_layer_dependencies.py

        import ast
        import os

        def test_application_does_not_import_infrastructure():
            """Use cases should not import from infrastructure."""
            app_dir = "src/application"

            for root, dirs, files in os.walk(app_dir):
                for file in files:
                    if file.endswith('.py'):
                        path = os.path.join(root, file)
                        with open(path) as f:
                            tree = ast.parse(f.read())

                        for node in ast.walk(tree):
                            if isinstance(node, ast.Import):
                                for alias in node.names:
                                    assert 'infrastructure' not in alias.name
                            elif isinstance(node, ast.ImportFrom):
                                if node.module:
                                    assert 'infrastructure' not in node.module

metricas_del_incidente:
  tiempo_deteccion: "Durante code review manual"
  tiempo_resolucion: "2 horas"
  impacto: "Ninguno (detectado antes de merge)"
  costo_si_hubiera_pasado: |
    - Refactor posterior costoso
    - Tests menos confiables
    - Precedente para más violaciones

lecciones_aprendidas:
  - "Tests pasando no garantiza buena arquitectura"
  - "Guardrails de imports son críticos"
  - "Documentación de patrones previene errores"
  - "Review humano sigue siendo necesario para arquitectura"
```

#### Ejemplo 3: El Tech Lead Ajusta Pipeline Basándose en Datos

**Situación:** Después de un trimestre, el Tech Lead revisa métricas y ajusta el sistema.

```yaml
# REVISIÓN TRIMESTRAL DE PIPELINE - Tech Lead
# Período: Q4 2025
# Tech Lead: Carlos Rodríguez

metricas_del_trimestre:

  prs_totales: 342
  prs_generados_por_ia: 287  # 84%

  distribucion_por_autonomia:
    alta_autonomia: 124   # 43% - Auto-merge elegibles
    media_autonomia: 98   # 34% - Review de senior
    baja_autonomia: 52    # 18% - Review de Tech Lead
    prohibida: 13         # 5% - Modificación manual

  resultados_ci:
    passed_first_try: 198  # 69%
    failed_and_fixed: 76   # 26%
    rejected: 13           # 5%

  tiempos:
    tiempo_promedio_ci: "8.5 minutos"
    tiempo_promedio_review: "2.3 horas"
    tiempo_promedio_merge: "4.1 horas"

  incidentes:
    bugs_en_produccion: 3
    bugs_detectados_en_staging: 12
    violaciones_arquitectura: 7
    security_issues: 1

analisis_por_gate:

  gate_1_structure:
    efectividad: "Alta"
    prs_bloqueados: 23
    falsos_positivos: 2
    mejoras_identificadas: null

  gate_2_static_analysis:
    efectividad: "Media-Alta"
    prs_bloqueados: 45
    falsos_positivos: 12
    mejoras_identificadas:
      - "Regla de 'any' en TypeScript muy estricta"
      - "Falta regla para imports circulares"

  gate_3_tests:
    efectividad: "Alta"
    prs_bloqueados: 67
    falsos_positivos: 5
    mejoras_identificadas:
      - "Tests E2E demasiado lentos (5+ min)"
      - "Falta verificación de tests significativos"

  gate_4_security:
    efectividad: "Alta"
    prs_bloqueados: 8
    falsos_positivos: 1
    mejoras_identificadas:
      - "Agregar SAST para nuevas dependencias"

  gate_5_performance:
    efectividad: "Baja"
    prs_bloqueados: 3
    falsos_positivos: 8
    mejoras_identificadas:
      - "Lighthouse muy sensible a variaciones"
      - "Necesita baseline más estable"

analisis_de_incidentes:

  bug_1:
    descripcion: "Race condition en cache invalidation"
    origen: "Código de IA"
    detectado_en: "Producción"
    causa_raiz: "Test no cubría escenario concurrente"
    accion_correctiva: "Agregar tests de concurrencia obligatorios para cache"

  bug_2:
    descripcion: "Query N+1 en listado de productos"
    origen: "Código de IA"
    detectado_en: "Staging (performance test)"
    causa_raiz: "IA no consideró eager loading"
    accion_correctiva: "Agregar análisis de query count en tests"

  bug_3:
    descripcion: "Validación de email incorrecta"
    origen: "Código de IA"
    detectado_en: "Producción"
    causa_raiz: "Regex generado no manejaba casos edge"
    accion_correctiva: "Usar librerías estándar para validaciones comunes"

  violacion_seguridad_1:
    descripcion: "Endpoint sin rate limiting"
    origen: "Código de IA"
    detectado_en: "Security review manual"
    causa_raiz: "IA no aplicó decorador obligatorio"
    accion_correctiva: "Gate que verifica rate limiting en todos los endpoints"

ajustes_decididos:

  ajustes_de_guardrails:

    nuevo_guardrail_1:
      nombre: "Verificación de concurrencia"
      razon: "Bug 1 - race condition"
      implementacion: |
        # Requerir tests de concurrencia para código que use cache o locks
        - name: Check concurrency tests
          run: |
            CACHE_FILES=$(git diff --name-only | grep -E "cache|lock|mutex")
            for file in $CACHE_FILES; do
              test_file="${file%.py}_test.py"
              if ! grep -q "async.*gather\|concurrent\|asyncio.create_task" "$test_file"; then
                echo "⚠️ $file modifica cache/locks pero no tiene tests de concurrencia"
              fi
            done

    nuevo_guardrail_2:
      nombre: "Query count verification"
      razon: "Bug 2 - N+1 queries"
      implementacion: |
        # En tests de integración
        @pytest.fixture
        def query_counter(db_session):
            queries = []
            # Interceptar queries
            return queries

        def test_list_products_query_count(query_counter):
            products = await use_case.execute(filters)
            assert len(query_counter) <= 3, "Too many queries (N+1?)"

    nuevo_guardrail_3:
      nombre: "Standard validations"
      razon: "Bug 3 - validación incorrecta"
      implementacion: |
        # Prohibir regex custom para validaciones comunes
        - name: Check for custom validations
          run: |
            if grep -rn "re.match.*email\|re.compile.*email" src/; then
              echo "❌ Use pydantic EmailStr instead of custom regex"
              exit 1
            fi

  ajustes_de_pipeline:

    optimizacion_1:
      nombre: "Paralelizar tests E2E"
      razon: "Tests E2E muy lentos"
      cambio: "Ejecutar E2E en paralelo con sharding"
      impacto_esperado: "Reducir de 5 min a 2 min"

    optimizacion_2:
      nombre: "Cache de dependencias más agresivo"
      razon: "Install de dependencias toma 2 min"
      cambio: "Usar cache de npm/pip entre PRs"
      impacto_esperado: "Reducir CI de 8.5 min a 6 min"

    mejora_1:
      nombre: "Gate de imports circulares"
      razon: "Detectado en análisis"
      implementacion: |
        - name: Check circular imports
          run: |
            pip install pycycle
            pycycle --here src/

  ajustes_de_autonomia:

    modulo_notifications:
      autonomia_anterior: media
      autonomia_nueva: alta
      razon: "3 sprints sin incidentes, tests maduros"

    modulo_reporting:
      autonomia_anterior: media
      autonomia_nueva: baja
      razon: "2 bugs relacionados con queries, necesita más supervisión"

  actualizacion_documentacion:
    agents_md:
      - "Agregar ejemplo de eager loading obligatorio"
      - "Documentar uso de librer��as estándar para validación"
      - "Agregar sección de patrones de concurrencia"

    skills:
      - "Crear skill 'database-optimization' con patrones de query"
      - "Actualizar skill 'python' con ejemplos de pydantic"

proxima_revision: "2026-04-24"
```

### Anti-patrones del Tech Lead IA-Native

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                ANTI-PATRONES DEL TECH LEAD IA-NATIVE                        │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                             │
│  ❌ ANTI-PATRÓN 1: "El Tech Lead Complaciente"                             │
│  ──────────────────────────────────────────────────────────────────────────│
│  Descripción: Aprobar PRs de IA porque "los tests pasan"                   │
│  Síntoma: "Si CI está verde, está bien"                                    │
│  Problema: Tests no garantizan buena arquitectura ni seguridad             │
│  Solución: Mantener review humano para arquitectura y patrones             │
│                                                                             │
│  ❌ ANTI-PATRÓN 2: "El Tech Lead Bloqueador"                               │
│  ──────────────────────────────────────────────────────────────────────────│
│  Descripción: Revisar personalmente cada línea de código de IA             │
│  Síntoma: Cola de PRs esperando review del Tech Lead                       │
│  Problema: Se pierde el beneficio de velocidad de la IA                    │
│  Solución: Confiar en guardrails automatizados para bajo riesgo            │
│                                                                             │
│  ❌ ANTI-PATRÓN 3: "El Tech Lead Sin Guardrails"                           │
│  ──────────────────────────────────────────────────────────────────────────│
│  Descripción: No implementar verificaciones automatizadas                  │
│  Síntoma: "Revisamos todo manualmente, no necesitamos CI complejo"         │
│  Problema: Humanos fallan en detectar problemas repetitivos                │
│  Solución: Automatizar todo lo automatizable                               │
│                                                                             │
│  ❌ ANTI-PATRÓN 4: "El Tech Lead Desactualizado"                           │
│  ──────────────────────────────────────────────────────────────────────────│
│  Descripción: No mantener la documentación de contexto para IA             │
│  Síntoma: AGENTS.md desactualizado, skills obsoletos                       │
│  Problema: IA genera código con patrones viejos o incorrectos              │
│  Solución: Actualizar documentación como parte del Definition of Done      │
│                                                                             │
│  ❌ ANTI-PATRÓN 5: "El Tech Lead Aislado"                                  │
│  ──────────────────────────────────────────────────────────────────────────│
│  Descripción: No involucrar al equipo en decisiones de autonomía           │
│  Síntoma: Solo el Tech Lead sabe qué puede y qué no puede hacer la IA      │
│  Problema: Equipo no puede supervisar efectivamente                        │
│  Solución: Documentar y comunicar decisiones de autonomía                  │
│                                                                             │
│  ❌ ANTI-PATRÓN 6: "El Tech Lead Reactivo"                                 │
│  ──────────────────────────────────────────────────────────────────────────│
│  Descripción: Solo ajustar guardrails después de incidentes                │
│  Síntoma: "Agregamos esa regla después del bug de producción"              │
│  Problema: Cada error es un incidente en producción                        │
│  Solución: Anticipar problemas, revisar métricas proactivamente            │
│                                                                             │
│  ❌ ANTI-PATRÓN 7: "El Tech Lead Perfeccionista"                           │
│  ──────────────────────────────────────────────────────────────────────────│
│  Descripción: Rechazar código de IA por estilo, no por funcionalidad       │
│  Síntoma: "La IA usa 4 espacios, nosotros usamos tabs"                     │
│  Problema: Tiempo perdido en cosas que linters pueden arreglar             │
│  Solución: Automatizar formato, enfocarse en arquitectura y lógica         │
│                                                                             │
└─────────────────────────────────────────────────────────────────────────────┘
```

### Métricas del Tech Lead IA-Native

```yaml
# DASHBOARD DE MÉTRICAS DEL TECH LEAD IA-NATIVE

metricas_de_pipeline:

  efectividad_de_guardrails:
    definicion: "% de problemas detectados por CI vs detectados en review"
    objetivo: ">80% detectados por CI"
    interpretacion: |
      <80% = Guardrails insuficientes
      >95% = Review manual puede ser innecesario para algunos módulos

  tasa_de_falsos_positivos:
    definicion: "% de bloqueos de CI que fueron incorrectos"
    objetivo: "<5%"
    alerta: ">10% indica reglas demasiado estrictas"

  tiempo_de_ci:
    definicion: "Tiempo promedio de ejecución del pipeline"
    objetivo: "<10 minutos"
    alerta: ">15 minutos degrada productividad"

  cobertura_de_guardrails:
    definicion: "% de módulos con guardrails definidos"
    objetivo: "100%"
    importancia: "Módulos sin guardrails son puntos ciegos"

metricas_de_calidad:

  tasa_de_bugs_por_origen:
    definicion: "Bugs en producción segmentados por origen del código"
    segmentos:
      - "Código de IA con alta autonomía"
      - "Código de IA con review"
      - "Código humano"
    uso: "Identificar si algún segmento tiene más problemas"

  incidentes_arquitectonicos:
    definicion: "Violaciones de arquitectura detectadas"
    objetivo: "Tendencia decreciente"
    interpretacion: "Si crece, los guardrails no son suficientes"

  deuda_tecnica_introducida:
    definicion: "TODOs, workarounds, code smells introducidos"
    objetivo: "<5 por sprint"
    medicion: "Análisis estático + review manual"

metricas_de_productividad:

  throughput_de_prs:
    definicion: "PRs mergeados por semana"
    segmentacion: "Por nivel de autonomía"
    uso: "Medir si guardrails son apropiados"

  tiempo_en_review_queue:
    definicion: "Tiempo que PRs esperan review del Tech Lead"
    objetivo: "<4 horas para alta prioridad"
    alerta: ">8 horas indica cuello de botella"

  ratio_auto_merge:
    definicion: "% de PRs que se mergean sin review manual"
    objetivo: "30-50% para equipos maduros"
    interpretacion: |
      <20% = Posiblemente demasiado conservador
      >60% = Verificar que no se estén pasando problemas

metricas_de_contexto:

  actualizacion_de_documentacion:
    definicion: "Frecuencia de actualización de AGENTS.md y skills"
    objetivo: "Al menos 1 actualización por sprint"
    importancia: "Documentación desactualizada = código de IA de baja calidad"

  efectividad_de_contexto:
    definicion: "% de PRs de IA que siguen patrones documentados"
    objetivo: ">90%"
    medicion: "Muestreo en code reviews"
```

### Checklist de Adopción para el Tech Lead

```
┌─────────────────────────────────────────────────────────────────────────────┐
│            CHECKLIST DE ADOPCIÓN - TECH LEAD IA-NATIVE                      │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                             │
│  SEMANA 1: AUDITORÍA Y CLASIFICACIÓN                                       │
│  □ Mapear todos los módulos del sistema                                    │
│  □ Clasificar cada módulo por autonomía técnica (alta/media/baja/prohibida)│
│  □ Identificar archivos críticos que nunca debe tocar la IA               │
│  □ Documentar justificación de cada clasificación                          │
│  □ Compartir mapa con el equipo para validación                           │
│                                                                             │
│  SEMANA 2: GUARDRAILS BÁSICOS                                              │
│  □ Implementar protección de paths críticos en CI                          │
│  □ Configurar verificación de convenciones de nombrado                     │
│  □ Agregar análisis estático básico (linting, type checking)              │
│  □ Implementar verificación de dependencias de capas                       │
│  □ Configurar verificación de imports prohibidos                           │
│                                                                             │
│  SEMANA 3: PIPELINE COMPLETO                                               │
│  □ Configurar gates de CI por nivel de riesgo                             │
│  □ Implementar auto-asignación de reviewers basada en archivos            │
│  □ Configurar labels automáticos (ia-generated, needs-review, etc.)       │
│  □ Agregar security scanning (SAST, dependency audit)                      │
│  □ Configurar métricas de cobertura con thresholds                        │
│                                                                             │
│  SEMANA 4: DOCUMENTACIÓN DE CONTEXTO                                       │
│  □ Crear o actualizar AGENTS.md con arquitectura del sistema              │
│  □ Documentar patrones de código obligatorios                              │
│  □ Crear ejemplos de código correcto e incorrecto                         │
│  □ Definir skills para los patrones más comunes                           │
│  □ Entrenar al equipo en cómo mantener la documentación                   │
│                                                                             │
│  MES 2: REFINAMIENTO                                                        │
│  □ Analizar primeras métricas de CI (falsos positivos, tiempos)           │
│  □ Ajustar guardrails basándose en datos reales                           │
│  □ Implementar guardrails específicos por dominio                         │
│  □ Crear proceso de revisión trimestral de autonomías                     │
│  □ Documentar incidentes y lecciones aprendidas                           │
│                                                                             │
│  MES 3+: MADUREZ                                                            │
│  □ Realizar primera revisión trimestral de clasificaciones                │
│  □ Implementar métricas de efectividad de guardrails                      │
│  □ Optimizar tiempos de CI                                                 │
│  □ Expandir auto-merge a más módulos (si métricas lo justifican)         │
│  □ Mentorear a otros Tech Leads en la adopción                            │
│                                                                             │
└─────────────────────────────────────────────────────────────────────────────┘
```

### Conclusión: El Nuevo Contrato del Tech Lead

El Tech Lead IA-Native firma un nuevo contrato implícito con su organización:

> **"Acepto que mi rol ya no es solo revisar código, sino diseñar los sistemas de restricción que permiten a la IA operar de manera segura. Asumo la responsabilidad de definir guardrails, configurar pipelines, mantener documentación de contexto, y evolucionar estos sistemas basándome en datos reales. Entiendo que la efectividad de mi trabajo se mide no solo en la calidad del código que apruebo, sino en la calidad de los sistemas que previenen que código problemático llegue a mi review."**

Este nuevo contrato no disminuye la importancia del Tech Lead; por el contrario, lo eleva a una posición más estratégica donde sus decisiones de diseño de sistemas tienen impacto directo en la velocidad, calidad y seguridad de todo el desarrollo. El Tech Lead IA-Native es, en esencia, un **ingeniero de restricciones cognitivas**: alguien que diseña no solo sistemas técnicos, sino los límites dentro de los cuales la inteligencia artificial puede operar de manera productiva y segura.

---

## ★ El Scrum Master en el Marco IA-Native: De Facilitador de Ceremonias a Guardián del Sistema Socio-Técnico ★

> **"El Scrum Master del futuro no facilitará reuniones entre humanos, sino que orquestará la danza entre personas, procesos y agentes artificiales para mantener un equilibrio productivo y sostenible."**

El Scrum Master experimenta una transformación profunda en el marco IA-Native. Su rol, tradicionalmente centrado en facilitar ceremonias ágiles, remover impedimentos y proteger al equipo de interferencias externas, evoluciona hacia una función de **orquestación socio-técnica** donde debe equilibrar las dinámicas entre humanos y agentes de IA, asegurando que el sistema completo funcione de manera saludable.

Esta sección desarrolla en profundidad cómo el Scrum Master inicia, ejecuta y evoluciona su rol dentro del paradigma IA-Native, proporcionando guías prácticas, ejemplos detallados y herramientas concretas para la transición.

### La Naturaleza del Cambio: Por Qué el Scrum Master No Puede Seguir Igual

En el modelo tradicional, el Scrum Master opera como un facilitador y coach. Ayuda al equipo a seguir las prácticas ágiles, facilita las ceremonias, remueve impedimentos y protege al equipo de distracciones. Su éxito se mide en la madurez del equipo y su capacidad de auto-organizarse.

Cuando la inteligencia artificial entra como ejecutor, este modelo se transforma de formas fundamentales:

1. **Nuevos "miembros" del equipo**: Los agentes de IA son ahora participantes activos que producen trabajo. El Scrum Master debe considerar cómo integrarlos en la dinámica del equipo.

2. **Impedimentos de naturaleza diferente**: Ya no son solo bloqueos organizacionales o técnicos humanos. Ahora incluyen límites de tokens, alucinaciones de IA, falta de contexto, y conflictos entre lo que la IA produce y lo que el equipo espera.

3. **Ceremonias transformadas**: Las reuniones ya no son solo para sincronizar humanos. Deben incluir revisión del trabajo de IA, análisis de patrones de delegación, y ajustes de autonomía.

4. **Salud del equipo redefinida**: La fatiga no es solo humana. Existe "fatiga de supervisión" cuando los humanos deben revisar demasiado output de IA. El burnout puede venir de la sobrecarga cognitiva de validación.

5. **Métricas de velocidad obsoletas**: Story points y velocity tradicionales no capturan la nueva realidad donde la IA puede producir mucho pero requiere supervisión variable.

### Las Cinco Dimensiones del Scrum Master IA-Native

**Dimensión 1: Facilitador de Ceremonias Híbridas (tradicional transformada)**

El Scrum Master sigue facilitando ceremonias, pero ahora son **ceremonias híbridas** que incluyen revisión del trabajo de IA:

- Daily: Incluye estado de Policy Tickets, alertas de gobernanza, checkpoints pendientes
- Planning: Incluye clasificación de riesgo, asignación de autonomía, capacidad de supervisión
- Review: Incluye métricas de gobernanza, ratio de auto-merge, incidentes de IA
- Retrospectiva: Incluye patrones de la IA, efectividad de políticas, ajustes de autonomía

**Dimensión 2: Guardián del Balance Humano-IA (nueva)**

Esta dimensión es completamente nueva. El Scrum Master debe monitorear y proteger el equilibrio entre el trabajo humano y el de IA:

- ¿Los humanos están sobrecargados de revisión?
- ¿La IA está produciendo más de lo que el equipo puede validar?
- ¿Hay frustración por rechazos frecuentes de código de IA?
- ¿El equipo confía demasiado o demasiado poco en la IA?

**Dimensión 3: Coach de Supervisión Efectiva (nueva)**

El Scrum Master debe entrenar al equipo en cómo supervisar efectivamente el trabajo de IA:

- Técnicas de revisión de código generado por IA
- Cómo dar feedback a los prompts para mejorar resultados
- Cuándo aceptar, rechazar o pedir regeneración
- Cómo mantener el conocimiento del sistema cuando la IA escribe mucho código

**Dimensión 4: Removedor de Impedimentos Socio-Técnicos (tradicional ampliada)**

Los impedimentos ahora incluyen problemas en la interacción humano-IA:

- Contexto insuficiente para la IA (AGENTS.md desactualizado)
- Guardrails demasiado restrictivos que bloquean trabajo legítimo
- Falta de capacidad de supervisión para la carga de trabajo
- Conflictos entre diferentes agentes de IA
- Fatiga de validación en el equipo

**Dimensión 5: Protector de la Cultura de Aprendizaje (tradicional ampliada)**

El Scrum Master debe asegurar que el equipo siga aprendiendo y creciendo, no solo delegando:

- ¿Los juniors están aprendiendo o solo aprobando código de IA?
- ¿El equipo entiende el sistema o depende de la IA para explicarlo?
- ¿Se está perdiendo conocimiento tácito porque la IA hace todo?
- ¿Hay oportunidades de pair programming humano-humano?

### El Ciclo de Trabajo del Scrum Master IA-Native

```
┌─────────────────────────────────────────────────────────────────────────────┐
│           CICLO DE TRABAJO DEL SCRUM MASTER IA-NATIVE                       │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                             │
│  ┌──────────────┐                                                          │
│  │   OBSERVAR   │  Monitorear dinámicas del sistema humano-IA              │
│  │   SISTEMA    │  ¿Hay señales de desequilibrio?                          │
│  └──────┬───────┘                                                          │
│         │                                                                   │
│         ▼                                                                   │
│  ┌──────────────┐                                                          │
│  │   FACILITAR  │  Conducir ceremonias híbridas                            │
│  │  CEREMONIAS  │  Incluir perspectiva de gobernanza IA                    │
│  └──────┬───────┘                                                          │
│         │                                                                   │
│         ▼                                                                   │
│  ┌──────────────┐                                                          │
│  │   DETECTAR   │  Identificar impedimentos socio-técnicos                 │
│  │ IMPEDIMENTOS │  Bloqueos humanos + bloqueos de IA + fricción           │
│  └──────┬───────┘                                                          │
│         │                                                                   │
│         ▼                                                                   │
│  ┌──────────────┐                                                          │
│  │   INTERVENIR │  Remover impedimentos, mediar conflictos                 │
│  │   Y MEDIAR   │  Ajustar procesos, escalar si necesario                  │
│  └──────┬───────┘                                                          │
│         │                                                                   │
│         ▼                                                                   │
│  ┌──────────────┐                                                          │
│  │    COACH     │  Entrenar en supervisión efectiva de IA                  │
│  │   AL EQUIPO  │  Desarrollar habilidades de validación                   │
│  └──────┬───────┘                                                          │
│         │                                                                   │
│         ▼                                                                   │
│  ┌──────────────┐                                                          │
│  │   PROTEGER   │  Asegurar aprendizaje continuo del equipo                │
│  │ CRECIMIENTO  │  Evitar atrofia de habilidades                           │
│  └──────┬───────┘                                                          │
│         │                                                                   │
│         └──────────────────────────────────────────────────────────────────┘│
│                                                                             │
└─────────────────────────────────────────────────────────────────────────────┘
```

### Primer Día del Scrum Master IA-Native: Guía Paso a Paso

#### Paso 1: Diagnóstico del Sistema Socio-Técnico

El primer ejercicio del Scrum Master es evaluar el estado actual del sistema humano-IA.

```yaml
# DIAGNÓSTICO SOCIO-TÉCNICO - Scrum Master
# Equipo: Squad Restaurante Digital
# Fecha: 2026-01-24

composicion_del_equipo:
  humanos:
    - rol: "Product Owner"
      nombre: "María García"
      experiencia_ia: "media"
      carga_supervision: "alta"  # Revisa todos los PT de alto riesgo
    - rol: "Tech Lead"
      nombre: "Carlos Rodríguez"
      experiencia_ia: "alta"
      carga_supervision: "muy alta"  # Configura guardrails + review
    - rol: "Senior Developer"
      nombre: "Ana López"
      experiencia_ia: "alta"
      carga_supervision: "media"
    - rol: "Mid Developer"
      nombre: "Juan Martínez"
      experiencia_ia: "media"
      carga_supervision: "media"
    - rol: "Junior Developer"
      nombre: "Laura Torres"
      experiencia_ia: "baja"
      carga_supervision: "baja"  # ⚠️ Potencial problema
    - rol: "QA Lead"
      nombre: "Pedro Sánchez"
      experiencia_ia: "media"
      carga_supervision: "alta"

  agentes_ia:
    - nombre: "Claude Code"
      uso: "Desarrollo general, refactoring"
      autonomia_tipica: "media-alta"
    - nombre: "GitHub Copilot"
      uso: "Asistencia inline"
      autonomia_tipica: "baja (sugerencias)"

estado_actual:

  metricas_ultimo_sprint:
    policy_tickets_completados: 15
    ratio_ia_vs_humano: "70% IA / 30% humano"
    tiempo_promedio_review: "3.2 horas"
    rechazos_por_gobernanza: 4
    incidentes_produccion: 1

  señales_de_alerta:
    - tipo: "Sobrecarga de supervisión"
      descripcion: "Tech Lead tiene 12 PRs pendientes de review"
      severidad: alta

    - tipo: "Desconexión del junior"
      descripcion: "Laura aprueba PRs de IA sin entender el código"
      severidad: alta

    - tipo: "Frustración con rechazos"
      descripcion: "Juan expresó frustración porque la IA 'siempre hace algo mal'"
      severidad: media

    - tipo: "Dependencia excesiva"
      descripcion: "El equipo no recuerda cómo funciona el módulo de pagos"
      severidad: media

  salud_del_equipo:
    moral: "media-baja"
    fatiga_supervision: "alta"
    confianza_en_ia: "polarizada"  # Algunos confían mucho, otros poco
    aprendizaje: "estancado"

diagnostico_resumen: |
  El equipo está en una fase de transición difícil. La IA produce
  mucho output pero la capacidad de supervisión está saturada,
  especialmente en el Tech Lead. El junior está desconectado del
  proceso de aprendizaje real. Hay frustración acumulada por
  rechazos frecuentes. Se necesita rebalancear la carga y
  establecer mecanismos de aprendizaje protegido.

prioridades_inmediatas:
  1. "Redistribuir carga de review del Tech Lead"
  2. "Establecer programa de mentoría para Laura (junior)"
  3. "Sesión de equipo sobre cómo dar mejor contexto a la IA"
  4. "Revisar autonomías - posiblemente demasiado ambiciosas"
```

#### Paso 2: Rediseñar las Ceremonias para el Contexto IA-Native

```yaml
# CEREMONIAS REDISEÑADAS - Scrum Master IA-Native
# Equipo: Squad Restaurante Digital

daily_standup:
  duracion: "15 minutos estrictos"

  estructura_tradicional:
    - "¿Qué hice ayer?"
    - "¿Qué haré hoy?"
    - "¿Hay impedimentos?"

  estructura_ia_native:
    minutos_1_a_5:
      nombre: "Estado de Policy Tickets"
      preguntas:
        - "¿Qué Policy Tickets están activos?"
        - "¿Cuál es el estado de cada uno? (en progreso, en review, bloqueado)"
        - "¿Alguno cambió de nivel de autonomía?"
      facilitacion: |
        El Scrum Master tiene un dashboard visible con:
        - PT activos por persona
        - PT en cola de review
        - Alertas de gobernanza

    minutos_5_a_10:
      nombre: "Checkpoint de Supervisión"
      preguntas:
        - "¿Hay PRs de IA esperando review por más de 4 horas?"
        - "¿Alguien está sobrecargado de revisiones?"
        - "¿La IA produjo algo inesperado que necesitemos discutir?"
      facilitacion: |
        El Scrum Master identifica cuellos de botella de supervisión
        y propone redistribución si es necesario.

    minutos_10_a_15:
      nombre: "Impedimentos y Ajustes"
      preguntas:
        - "¿Hay bloqueos técnicos o de gobernanza?"
        - "¿Necesitamos ajustar alguna autonomía para hoy?"
        - "¿Alguien necesita pair programming (humano-humano)?"
      facilitacion: |
        El Scrum Master anota impedimentos y asigna seguimiento.
        Prioriza los que afectan supervisión.

  anti_patrones_a_evitar:
    - "Reportar líneas de código generadas por IA"
    - "Competir por quién delegó más a la IA"
    - "Ignorar señales de fatiga de supervisión"
    - "No mencionar código que no se entiende"

sprint_planning:
  duracion: "2-4 horas dependiendo del sprint"

  estructura_ia_native:
    fase_1_capacidad:
      nombre: "Capacidad de Supervisión"
      duracion: "30 minutos"
      actividades:
        - "Calcular horas disponibles para supervisión (no solo desarrollo)"
        - "Identificar quién puede revisar qué (por dominio de expertise)"
        - "Considerar ausencias, reuniones, otras responsabilidades"
      formula: |
        Capacidad_Supervisión = Σ(horas_disponibles × factor_expertise)

        Donde factor_expertise:
        - Senior en el dominio: 1.0
        - Mid con experiencia: 0.7
        - Junior o sin experiencia: 0.3

      output: "Máximo de horas de supervisión disponibles"

    fase_2_seleccion:
      nombre: "Selección de Policy Tickets"
      duracion: "60-90 minutos"
      actividades:
        - "PO presenta Policy Tickets priorizados"
        - "Tech Lead indica autonomía técnica de cada uno"
        - "Equipo estima carga de supervisión requerida"
        - "Scrum Master verifica que no exceda capacidad"

      validacion_scrum_master:
        - "¿La suma de supervisión requerida ≤ capacidad?"
        - "¿Hay diversidad de niveles de autonomía?"
        - "¿Los juniors tienen oportunidades de aprendizaje?"
        - "¿Alguien está asumiendo demasiada carga?"

    fase_3_asignacion:
      nombre: "Asignación y Responsabilidades"
      duracion: "30 minutos"
      actividades:
        - "Asignar responsables humanos a cada Policy Ticket"
        - "Definir reviewers para cada nivel de riesgo"
        - "Establecer checkpoints para PT de alta supervisión"
        - "Identificar oportunidades de pair programming"

      consideraciones_scrum_master:
        - "Balancear carga entre miembros del equipo"
        - "Asegurar que juniors trabajen con seniors"
        - "No sobrecargar al Tech Lead con todos los reviews"
        - "Dejar slack para impedimentos inesperados"

sprint_review:
  duracion: "1-2 horas"

  estructura_ia_native:
    parte_1_demo:
      nombre: "Demostración Funcional"
      duracion: "40% del tiempo"
      contenido: "Igual que siempre - mostrar funcionalidades"

    parte_2_gobernanza:
      nombre: "Reporte de Salud del Sistema"
      duracion: "30% del tiempo"
      metricas_a_presentar:
        - "Policy Tickets completados vs planificados"
        - "Ratio de trabajo IA vs humano"
        - "Tiempo promedio en cola de review"
        - "Rechazos por gobernanza y sus causas"
        - "Incidentes de IA (excedió alcance, alucinaciones, etc.)"

      preguntas_de_reflexion:
        - "¿Las autonomías asignadas fueron apropiadas?"
        - "¿Tuvimos suficiente capacidad de supervisión?"
        - "¿Qué aprendimos sobre trabajar con IA este sprint?"

    parte_3_stakeholders:
      nombre: "Feedback de Stakeholders"
      duracion: "30% del tiempo"
      contenido: |
        Además del feedback funcional tradicional, incluir:
        - Percepción de calidad del código generado por IA
        - Confianza en los mecanismos de gobernanza
        - Preguntas sobre el proceso

retrospectiva:
  duracion: "1-2 horas"

  estructura_ia_native:
    dimension_1_proceso:
      nombre: "El Proceso (tradicional)"
      preguntas:
        - "¿Qué funcionó bien en nuestras prácticas?"
        - "¿Qué podemos mejorar?"
        - "¿Qué experimentos queremos probar?"

    dimension_2_ia:
      nombre: "La Interacción con IA (nueva)"
      preguntas:
        - "¿Cómo fue trabajar con la IA este sprint?"
        - "¿Hubo frustraciones recurrentes?"
        - "¿Qué patrones de la IA nos sorprendieron (positiva o negativamente)?"
        - "¿Los niveles de autonomía fueron correctos?"
        - "¿Los guardrails fueron útiles o excesivos?"

    dimension_3_personas:
      nombre: "El Equipo (ampliada)"
      preguntas:
        - "¿Cómo está la moral del equipo?"
        - "¿Alguien sintió fatiga de supervisión?"
        - "¿Los juniors están aprendiendo o solo aprobando?"
        - "¿Estamos perdiendo conocimiento del sistema?"
        - "¿Hubo oportunidades de crecimiento profesional?"

    dimension_4_balance:
      nombre: "El Balance Humano-IA (nueva)"
      preguntas:
        - "¿El ratio de trabajo IA/humano fue saludable?"
        - "¿Hubo momentos donde deberíamos haber delegado más/menos?"
        - "¿La IA nos está ayudando a ser mejores o nos está reemplazando?"

    formato_sugerido: |
      Usar un tablero con 4 cuadrantes:
      ┌─────────────────┬─────────────────┐
      │    PROCESO      │    CON IA       │
      │  (tradicional)  │   (nuevo)       │
      ├─────────────────┼─────────────────┤
      │    EQUIPO       │    BALANCE      │
      │  (ampliado)     │   (nuevo)       │
      └─────────────────┴─────────────────┘
```

#### Paso 3: Establecer Mecanismos de Protección del Equipo

```yaml
# MECANISMOS DE PROTECCIÓN - Scrum Master IA-Native

proteccion_contra_sobrecarga:

  limites_de_supervision:
    descripcion: "Máximo de PRs de IA en review simultáneo por persona"
    reglas:
      - "Tech Lead: máximo 5 PRs pendientes"
      - "Senior: máximo 3 PRs pendientes"
      - "Mid: máximo 2 PRs pendientes"
      - "Junior: 0 PRs de review (solo pair review con senior)"

    acciones_cuando_se_excede:
      - "Pausar nuevos Policy Tickets hasta que baje la cola"
      - "Redistribuir reviews a otros miembros"
      - "Reducir autonomía de IA para generar menos volumen"
      - "Escalar a management si es estructural"

  dias_sin_ia:
    descripcion: "Días donde el equipo trabaja sin delegación a IA"
    frecuencia: "1 día cada 2 semanas"
    proposito:
      - "Mantener habilidades de desarrollo directo"
      - "Reducir fatiga de supervisión"
      - "Recordar cómo es el desarrollo tradicional"
      - "Identificar qué realmente necesita IA vs qué no"
    reglas:
      - "No se crean nuevos prompts a agentes de IA"
      - "Copilot inline permitido (es asistencia, no delegación)"
      - "Se pueden completar reviews de PRs existentes"

proteccion_del_aprendizaje:

  programa_junior_ia_native:
    descripcion: "Estructura para que juniors aprendan con IA"

    fase_1_observacion:
      duracion: "Primeras 2 semanas"
      actividades:
        - "Observar cómo seniors interactúan con IA"
        - "Leer código generado por IA con explicación de senior"
        - "Entender por qué se rechazaron ciertos PRs"
      prohibiciones:
        - "No aprobar PRs de IA solo"
        - "No crear Policy Tickets de alto riesgo"

    fase_2_pair_review:
      duracion: "Semanas 3-6"
      actividades:
        - "Hacer pair review con senior de PRs de IA"
        - "Escribir primeros prompts con supervisión"
        - "Comenzar a identificar patrones problemáticos"
      permisos:
        - "Puede aprobar PRs de bajo riesgo con sign-off de senior"

    fase_3_autonomia_supervisada:
      duracion: "Semanas 7-12"
      actividades:
        - "Crear Policy Tickets de bajo riesgo"
        - "Review de PRs de bajo riesgo solo"
        - "Proponer mejoras a prompts y contexto"
      validacion:
        - "Senior revisa 50% de sus aprobaciones (muestreo)"

    fase_4_autonomia:
      duracion: "Después de 3 meses"
      actividades:
        - "Autonomía completa en dominios de bajo riesgo"
        - "Puede escalar a medio riesgo con checkpoints"
      validacion:
        - "Revisión mensual de métricas personales"

  sesiones_de_codigo_sin_ia:
    descripcion: "Sesiones donde se escribe código manualmente"
    frecuencia: "2 horas por semana"
    formato: "Mob programming o pair programming"
    proposito:
      - "Mantener habilidades de desarrollo"
      - "Transferencia de conocimiento"
      - "Entender partes del sistema que la IA modificó"
    reglas:
      - "Sin Copilot ni agentes"
      - "Rotar quién escribe"
      - "Elegir partes del código que la IA ha tocado mucho"

proteccion_emocional:

  check_ins_regulares:
    frecuencia: "1:1 semanal con cada miembro"
    temas:
      - "¿Cómo te sientes trabajando con IA?"
      - "¿Hay algo que te frustre?"
      - "¿Sientes que estás aprendiendo?"
      - "¿La carga de supervisión es manejable?"
    señales_de_alerta:
      - "Siento que la IA hace todo y yo solo apruebo"
      - "No entiendo el código que estoy aprobando"
      - "Me frustra que la IA siempre haga algo mal"
      - "Ya no me acuerdo cómo funciona el sistema"

  canal_de_desahogo:
    descripcion: "Espacio seguro para expresar frustraciones con IA"
    formato: "Sesión opcional de 30 min post-retro"
    reglas:
      - "Todo queda en la sala"
      - "No se juzga"
      - "Se buscan soluciones, no culpables"
      - "La IA no es el enemigo, pero podemos estar frustrados"
```

### Ejemplos Prácticos del Scrum Master IA-Native

#### Ejemplo 1: Detectar y Resolver Sobrecarga de Supervisión

```yaml
# CASO: Sobrecarga de Supervisión
# Scrum Master: Elena Vargas
# Fecha: 2026-01-24

situacion:
  descripcion: |
    El Tech Lead Carlos tiene 8 PRs de IA pendientes de review.
    Llevan entre 1 y 3 días esperando. El equipo está frustrado
    porque no pueden mergear y avanzar. Carlos está estresado
    y trabajando horas extra.

  sintomas_observados:
    - "Carlos cancela 1:1s por falta de tiempo"
    - "PRs llevan más de 24 horas en review"
    - "Juan expresó frustración en la daily"
    - "Velocity del sprint está comprometida"

diagnostico_scrum_master:
  causa_raiz: |
    El sprint se planificó asumiendo que Carlos podía revisar
    todos los PRs de dominio medio y alto. La realidad es que
    la IA produjo más volumen del esperado y Carlos no da abasto.

  factores_contribuyentes:
    - "Autonomías quizás demasiado bajas (todo pasa por Carlos)"
    - "No hay backup de Carlos para ciertos dominios"
    - "El sprint tiene demasiados PT de riesgo medio/alto"

intervencion:

  paso_1_inmediato:
    accion: "Redistribuir reviews urgentes"
    como: |
      - Identificar 3 PRs que Ana (Senior) puede revisar
      - Ella conoce bien esos módulos aunque no es Tech Lead
      - Carlos solo revisa lo que requiere decisión arquitectónica

  paso_2_corto_plazo:
    accion: "Ajustar autonomías para este sprint"
    como: |
      - Subir autonomía del módulo de notificaciones a alta
      - Permitir auto-merge para PRs que pasen CI en ese módulo
      - Esto reduce 2-3 PRs de la cola de Carlos

  paso_3_para_siguiente_sprint:
    accion: "Capacitar backup del Tech Lead"
    como: |
      - Ana será backup para dominios de bajo-medio riesgo
      - Carlos la mentoreará en criterios de review arquitectónico
      - En el planning, calcular capacidad de ambos

  paso_4_estructural:
    accion: "Revisar distribución de autonomías con PO y Tech Lead"
    como: |
      - Sesión de 1 hora post-sprint
      - Analizar si las autonomías son muy conservadoras
      - Considerar subir autonomía de módulos maduros

seguimiento:
  dia_1:
    - "Ana revisó y aprobó 2 PRs"
    - "Carlos terminó otros 3"
    - "Cola bajó a 3 PRs"

  dia_3:
    - "Cola normalizada (1-2 PRs pendientes)"
    - "Carlos reporta menos estrés"
    - "Equipo volvió a mergear con normalidad"

  retrospectiva:
    - "Agregar al planning: revisar cola de review antes de comprometer más PT"
    - "Establecer límite de 5 PRs para Tech Lead como gate del sprint"

lecciones_aprendidas:
  - "La capacidad de supervisión debe calcularse explícitamente"
  - "Tener backup del Tech Lead es crítico"
  - "La IA puede producir más rápido de lo que humanos pueden revisar"
  - "El Scrum Master debe monitorear colas de review diariamente"
```

#### Ejemplo 2: Intervenir cuando un Junior está Desconectado

```yaml
# CASO: Junior Desconectado del Aprendizaje
# Scrum Master: Elena Vargas
# Fecha: 2026-01-24

situacion:
  descripcion: |
    Laura (Junior, 8 meses en el equipo) ha estado aprobando PRs
    de IA sin realmente entender el código. En una revisión de
    muestreo, Ana descubrió que Laura aprobó un PR con un bug
    obvio que no detectó. Cuando le preguntaron, Laura admitió
    que "confía en que la IA sabe lo que hace".

  sintomas_observados:
    - "Laura aprueba PRs muy rápido (< 10 minutos)"
    - "Nunca hace preguntas sobre el código"
    - "No puede explicar cambios que aprobó"
    - "En 1:1 admite que 'no entiende todo pero funciona'"

diagnostico_scrum_master:
  causa_raiz: |
    Laura se siente abrumada por el volumen de código generado
    por IA y no sabe cómo revisar efectivamente. Tiene miedo
    de preguntar porque piensa que debería entender. Ha entrado
    en un patrón de "aprobar y esperar que funcione".

  riesgos:
    - "Laura no está aprendiendo, está estancada"
    - "Puede aprobar bugs o vulnerabilidades serias"
    - "Su confianza y crecimiento profesional están comprometidos"
    - "El equipo puede perder confianza en las revisiones"

intervencion:

  paso_1_conversacion_privada:
    cuando: "Mismo día que se detectó"
    tono: "Empático, no punitivo"
    mensaje: |
      "Laura, noté que estás procesando reviews muy rápido y quería
      hablar contigo. No estás en problemas - esto es común cuando
      trabajamos con IA. ¿Cómo te sientes revisando código de IA?"

    escuchar:
      - "¿Se siente abrumada?"
      - "¿Entiende qué debe buscar?"
      - "¿Tiene miedo de preguntar?"
      - "¿Sabe que está bien no entender todo?"

  paso_2_plan_de_soporte:
    nombre: "Plan de Desarrollo para Laura"
    duracion: "6 semanas"

    semana_1_2:
      - "Laura no revisa sola - siempre pair review con Ana o Carlos"
      - "Ana explica en voz alta qué busca en cada review"
      - "Laura puede preguntar cualquier cosa sin juicio"

    semana_3_4:
      - "Laura lidera el pair review, Ana observa"
      - "Ana interviene si Laura aprueba algo problemático"
      - "Sesión semanal de 'code walkthrough' del código aprobado"

    semana_5_6:
      - "Laura revisa sola PRs de bajo riesgo"
      - "Ana hace muestreo del 30% de sus aprobaciones"
      - "1:1 semanal para discutir dudas acumuladas"

  paso_3_ajuste_del_sistema:
    acciones:
      - "Quitar a Laura de reviewers de riesgo medio/alto"
      - "Crear label 'junior-learning' para PRs donde Laura participa"
      - "Establecer tiempo mínimo de review (no < 15 min para PRs sustanciales)"

  paso_4_normalizacion_en_equipo:
    acciones:
      - "En retro, discutir que es normal no entender código de IA"
      - "Establecer cultura de 'preguntar es bueno'"
      - "Celebrar cuando alguien detecta algo que la IA hizo mal"

seguimiento:
  semana_2:
    - "Laura reporta que los pair reviews son muy útiles"
    - "Detectó su primer bug real (off-by-one en loop)"
    - "Ana nota mejora en las preguntas que hace"

  semana_4:
    - "Laura puede explicar la arquitectura del módulo que más revisó"
    - "Confianza visiblemente mejorada"
    - "Hace preguntas en las dailies sin vergüenza"

  semana_6:
    - "Revisión sola de PRs de bajo riesgo sin incidentes"
    - "Muestreo de Ana: 95% correctos"
    - "Laura propone mejora a un prompt de la IA"

metricas_de_exito:
  - "Tiempo de review aumentó de 8 min a 25 min (más profundo)"
  - "0 bugs aprobados en las últimas 4 semanas"
  - "Laura puede explicar código que aprobó"
  - "Participación activa en discusiones técnicas"

lecciones_aprendidas:
  - "Los juniors necesitan estructura explícita para aprender con IA"
  - "Aprobar rápido es una señal de alerta"
  - "La cultura de 'preguntar está bien' debe ser explícita"
  - "El pair review es la mejor herramienta de aprendizaje"
```

### Anti-patrones del Scrum Master IA-Native

```
┌─────────────────────────────────────────────────────────────────────────────┐
│              ANTI-PATRONES DEL SCRUM MASTER IA-NATIVE                       │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                             │
│  ❌ ANTI-PATRÓN 1: "El SM Ignorante de IA"                                 │
│  ──────────────────────────────────────────────────────────────────────────│
│  Descripción: Facilitar ceremonias sin considerar la dimensión IA          │
│  Síntoma: "Hagamos la daily como siempre, no hablemos de IA"               │
│  Problema: Se pierden señales críticas de desequilibrio                    │
│  Solución: Integrar perspectiva IA en todas las ceremonias                 │
│                                                                             │
│  ❌ ANTI-PATRÓN 2: "El SM Evangelista de IA"                               │
│  ──────────────────────────────────────────────────────────────────────────│
│  Descripción: Presionar para usar más IA sin considerar el equipo          │
│  Síntoma: "Deberíamos delegar más, la IA puede hacerlo"                    │
│  Problema: Ignora fatiga, aprendizaje y preferencias del equipo            │
│  Solución: Balancear beneficios de IA con bienestar del equipo             │
│                                                                             │
│  ❌ ANTI-PATRÓN 3: "El SM Ausente del Proceso"                             │
│  ──────────────────────────────────────────────────────────────────────────│
│  Descripción: No monitorear métricas de supervisión y gobernanza           │
│  Síntoma: "Eso es responsabilidad del Tech Lead, no mía"                   │
│  Problema: Impedimentos socio-técnicos no se detectan a tiempo             │
│  Solución: Incluir métricas de IA en el radar del SM                       │
│                                                                             │
│  ❌ ANTI-PATRÓN 4: "El SM que Ignora el Burnout de Supervisión"            │
│  ──────────────────────────────────────────────────────────────────────────│
│  Descripción: No reconocer que revisar código de IA es agotador            │
│  Síntoma: "Pero si la IA hace el trabajo duro, solo hay que revisar"       │
│  Problema: El equipo se quema revisando volúmenes masivos                  │
│  Solución: Calcular y proteger capacidad de supervisión                    │
│                                                                             │
│  ❌ ANTI-PATRÓN 5: "El SM que Abandona a los Juniors"                      │
│  ──────────────────────────────────────────────────────────────────────────│
│  Descripción: Asumir que juniors aprenden solos con IA                     │
│  Síntoma: "La IA les enseña, ven el código que genera"                     │
│  Problema: Juniors aprueban sin entender, no desarrollan habilidades       │
│  Solución: Estructura explícita de aprendizaje con mentoría humana         │
│                                                                             │
│  ❌ ANTI-PATRÓN 6: "El SM Obsesionado con Velocity"                        │
│  ──────────────────────────────────────────────────────────────────────────│
│  Descripción: Medir éxito solo por output, no por salud del sistema        │
│  Síntoma: "Este sprint entregamos 3x más gracias a la IA"                  │
│  Problema: Alta velocidad puede esconder deuda técnica y burnout           │
│  Solución: Balancear métricas de output con métricas de salud              │
│                                                                             │
│  ❌ ANTI-PATRÓN 7: "El SM que No Facilita Desahogo"                        │
│  ──────────────────────────────────────────────────────────────────────────│
│  Descripción: No crear espacio para frustraciones con IA                   │
│  Síntoma: "No quiero quejas sobre la IA, es lo que hay"                    │
│  Problema: Frustración se acumula y explota                                │
│  Solución: Espacios seguros para expresar y procesar emociones             │
│                                                                             │
└─────────────────────────────────────────────────────────────────────────────┘
```

### Métricas del Scrum Master IA-Native

```yaml
# DASHBOARD DE MÉTRICAS DEL SCRUM MASTER IA-NATIVE

metricas_de_flujo:

  tiempo_en_cola_review:
    definicion: "Tiempo promedio que PRs esperan review"
    objetivo: "<4 horas para alto riesgo, <8 horas para medio"
    alerta: ">24 horas indica cuello de botella"
    accion: "Redistribuir carga o ajustar autonomías"

  ratio_rechazo_vs_aprobacion:
    definicion: "% de PRs de IA rechazados"
    objetivo: "5-15%"
    alerta_bajo: "<5% puede indicar revisiones superficiales"
    alerta_alto: ">20% puede indicar contexto insuficiente para IA"

  throughput_por_tipo:
    definicion: "Policy Tickets completados segmentados por autonomía"
    uso: "Entender dónde fluye bien vs dónde hay fricción"

metricas_de_salud_del_equipo:

  carga_de_supervision_por_persona:
    definicion: "Horas de review por persona por semana"
    objetivo: "<30% del tiempo disponible"
    alerta: ">40% indica sobrecarga"

  indice_de_fatiga:
    definicion: "Encuesta anónima de fatiga de supervisión (1-5)"
    frecuencia: "Cada sprint"
    objetivo: "<3 promedio"
    alerta: ">3.5 requiere intervención"

  satisfaccion_con_ia:
    definicion: "Encuesta de satisfacción trabajando con IA (1-5)"
    frecuencia: "Cada sprint"
    objetivo: ">3.5 promedio"
    uso: "Detectar frustración antes de que escale"

metricas_de_aprendizaje:

  participacion_junior_en_reviews:
    definicion: "% de reviews donde participan juniors (pair o solo)"
    objetivo: ">50% de PRs de bajo riesgo"
    alerta: "<30% indica exclusión del aprendizaje"

  preguntas_en_code_review:
    definicion: "Número de comentarios/preguntas por PR"
    objetivo: ">3 para PRs sustanciales"
    alerta: "<1 puede indicar revisiones superficiales"

  sesiones_pair_programming:
    definicion: "Horas de pair programming humano-humano por sprint"
    objetivo: ">4 horas por persona"
    importancia: "Mantiene habilidades y transfiere conocimiento"

metricas_de_balance:

  ratio_trabajo_ia_vs_humano:
    definicion: "% de código generado por IA vs escrito por humanos"
    objetivo: "60-80% IA para equipos maduros"
    alerta_alto: ">90% puede indicar pérdida de habilidades"
    alerta_bajo: "<40% puede indicar subutilización"

  conocimiento_del_sistema:
    definicion: "Evaluación de cuánto entiende el equipo el sistema"
    frecuencia: "Trimestral"
    metodo: "Preguntas aleatorias sobre módulos"
    alerta: "Si nadie puede explicar un módulo, hay problema"
```

### Checklist de Adopción para el Scrum Master

```
┌─────────────────────────────────────────────────────────────────────────────┐
│           CHECKLIST DE ADOPCIÓN - SCRUM MASTER IA-NATIVE                    │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                             │
│  SEMANA 1: DIAGNÓSTICO                                                      │
│  □ Realizar diagnóstico socio-técnico del equipo                           │
│  □ Identificar niveles de experiencia con IA de cada miembro               │
│  □ Mapear carga actual de supervisión                                      │
│  □ Detectar señales de alerta existentes                                   │
│  □ Entender los mecanismos de gobernanza actuales (con PO y Tech Lead)     │
│                                                                             │
│  SEMANA 2: REDISEÑO DE CEREMONIAS                                          │
│  □ Adaptar formato de daily para incluir dimensión IA                      │
│  □ Modificar planning para calcular capacidad de supervisión               │
│  □ Preparar formato de review con métricas de gobernanza                   │
│  □ Diseñar retrospectiva con cuadrantes humano-IA                          │
│  □ Comunicar cambios al equipo                                             │
│                                                                             │
│  SEMANA 3: MECANISMOS DE PROTECCIÓN                                        │
│  □ Establecer límites de PRs en review por persona                         │
│  □ Crear programa de desarrollo para juniors                               │
│  □ Implementar check-ins de salud del equipo                               │
│  □ Establecer canal de desahogo seguro                                     │
│  □ Definir métricas a monitorear                                           │
│                                                                             │
│  SEMANA 4: PRIMER SPRINT COMPLETO                                          │
│  □ Facilitar planning con cálculo de capacidad de supervisión              │
│  □ Monitorear diariamente cola de reviews                                  │
│  □ Intervenir si hay sobrecarga                                            │
│  □ Observar participación de juniors                                       │
│  □ Recoger feedback informal                                               │
│                                                                             │
│  MES 2: REFINAMIENTO                                                        │
│  □ Analizar métricas del primer mes                                        │
│  □ Ajustar ceremonias basándose en feedback                                │
│  □ Refinar mecanismos de protección                                        │
│  □ Hacer primera encuesta de satisfacción con IA                           │
│  □ Identificar patrones de impedimentos recurrentes                        │
│                                                                             │
│  MES 3+: MADUREZ                                                            │
│  □ El equipo auto-gestiona balance humano-IA                               │
│  □ Métricas se revisan regularmente                                        │
│  □ Ceremonias fluyen naturalmente con dimensión IA                         │
│  □ Juniors progresan en el programa de desarrollo                          │
│  □ Cultura de cuidado del equipo está establecida                          │
│                                                                             │
└─────────────────────────────────────────────────────────────────────────────┘
```

### Conclusión: El Nuevo Contrato del Scrum Master

El Scrum Master IA-Native firma un nuevo contrato implícito con su organización:

> **"Acepto que mi rol ya no es solo facilitar ceremonias entre humanos, sino orquestar un sistema socio-técnico donde personas y agentes artificiales colaboran. Asumo la responsabilidad de proteger al equipo de la sobrecarga de supervisión, asegurar que todos sigan aprendiendo y creciendo, y mantener un balance saludable entre la velocidad que ofrece la IA y el bienestar de las personas. Entiendo que el éxito se mide no solo en velocity, sino en la sostenibilidad y salud del equipo a largo plazo."**

Este nuevo contrato no disminuye la importancia del Scrum Master; por el contrario, lo eleva a una posición más crítica donde sus habilidades de facilitación, coaching y protección del equipo son más necesarias que nunca. El Scrum Master IA-Native es, en esencia, un **guardián del equilibrio**: alguien que asegura que la adopción de IA beneficie a todos sin sacrificar el crecimiento, bienestar y humanidad del equipo.

---

## ★ El QA Lead en el Marco IA-Native: De Detector de Bugs a Diseñador de Sistemas de Verificación ★

> **"El QA del futuro no encontrará bugs después de que el código está escrito; diseñará sistemas que impidan que los bugs lleguen a existir."**

El QA Lead experimenta una de las transformaciones más radicales del marco IA-Native. Su rol, tradicionalmente centrado en testing manual y automatizado, detección de defectos y validación de requerimientos, evoluciona hacia una función de **ingeniería de verificación** donde debe diseñar sistemas automatizados que validen continuamente el trabajo de la IA, establecer gates de calidad que operen en tiempo real, y definir estrategias de testing adaptadas a la naturaleza del código generado por inteligencia artificial.

Esta sección desarrolla en profundidad cómo el QA Lead inicia, ejecuta y evoluciona su rol dentro del paradigma IA-Native.

### La Naturaleza del Cambio: Por Qué el QA No Puede Seguir Igual

En el modelo tradicional, el QA opera al final del proceso. Recibe código escrito por humanos, lo prueba, encuentra bugs, los reporta, y el ciclo se repite. Su efectividad se mide en bugs encontrados, cobertura de tests, y calidad del release.

Cuando la inteligencia artificial entra como generador de código, este modelo se vuelve insostenible:

1. **Volumen masivo**: La IA puede generar en horas lo que antes tomaba semanas. El QA no puede testear todo manualmente al mismo ritmo.

2. **Patrones de error diferentes**: El código de IA tiene tipos de errores distintos al humano. Puede ser sintácticamente perfecto pero semánticamente incorrecto. Puede seguir patrones obsoletos. Puede "alucinar" funcionalidades que no existen.

3. **Velocidad de iteración**: Los ciclos de feedback se aceleran. No hay tiempo para testing extensivo al final de cada sprint cuando la IA produce cambios continuamente.

4. **Testing generado por IA**: La propia IA puede generar tests, pero estos tests pueden ser tan defectuosos como el código que prueban. "Tests que pasan" ya no es garantía de calidad.

5. **Responsabilidad de verificación**: Antes el developer era responsable de su código. Ahora el código viene de IA, pero alguien debe validarlo. El QA se convierte en la última línea de defensa.

### Las Seis Dimensiones del QA Lead IA-Native

**Dimensión 1: Arquitecto de Estrategia de Testing (tradicional ampliada)**

El QA Lead sigue definiendo la estrategia de testing, pero ahora debe considerar múltiples fuentes de código:

- Testing de código humano (tradicional)
- Testing de código generado por IA
- Testing de tests generados por IA
- Testing de interacciones humano-IA

**Dimensión 2: Diseñador de Gates Automatizados (nueva)**

Esta dimensión es completamente nueva. El QA debe diseñar "gates" que bloqueen automáticamente código problemático:

- Gates de cobertura mínima
- Gates de análisis estático
- Gates de seguridad
- Gates de performance
- Gates de consistencia arquitectónica

**Dimensión 3: Ingeniero de Verificación Continua (nueva)**

El QA diseña sistemas que verifican continuamente, no solo al final:

- Verificación en tiempo real durante la generación de IA
- Validación de evidencias producidas por Policy Tickets
- Monitoreo de métricas de calidad en producción
- Alertas tempranas de degradación

**Dimensión 4: Curador de Tests de Calidad (tradicional transformada)**

El QA debe asegurar que los tests sean confiables, especialmente cuando la IA los genera:

- Revisar tests generados por IA
- Detectar tests vacíos o triviales
- Identificar tests que prueban implementación en vez de comportamiento
- Mantener suite de tests significativa

**Dimensión 5: Definidor de Criterios de Aceptación Técnica (nueva)**

Además de criterios de aceptación funcional, el QA define criterios técnicos:

- Umbrales de cobertura por nivel de riesgo
- Métricas de complejidad aceptables
- Estándares de performance
- Requisitos de seguridad verificables

**Dimensión 6: Analista de Patrones de Defectos de IA (nueva)**

El QA debe estudiar y anticipar los tipos de errores que la IA comete:

- Catalogar errores recurrentes de la IA
- Diseñar tests específicos para patrones problemáticos
- Feedback a Tech Lead para mejorar contexto de IA
- Evolucionar estrategia basándose en datos

### El Ciclo de Trabajo del QA Lead IA-Native

```
┌─────────────────────────────────────────────────────────────────────────────┐
│               CICLO DE TRABAJO DEL QA LEAD IA-NATIVE                        │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                             │
│  ┌──────────────┐                                                          │
│  │   DISEÑAR    │  Definir estrategia de testing para código de IA         │
│  │  ESTRATEGIA  │  Considerar volumen, patrones de error, riesgo           │
│  └──────┬───────┘                                                          │
│         │                                                                   │
│         ▼                                                                   │
│  ┌──────────────┐                                                          │
│  │  CONFIGURAR  │  Implementar gates automatizados en CI/CD                │
│  │    GATES     │  Cobertura, seguridad, performance, arquitectura         │
│  └──────┬───────┘                                                          │
│         │                                                                   │
│         ▼                                                                   │
│  ┌──────────────┐                                                          │
│  │   VALIDAR    │  Revisar tests generados por IA                          │
│  │    TESTS     │  Detectar tests vacíos, triviales, incorrectos           │
│  └──────┬───────┘                                                          │
│         │                                                                   │
│         ▼                                                                   │
│  ┌──────────────┐                                                          │
│  │  MONITOREAR  │  Observar métricas de calidad en tiempo real             │
│  │   CALIDAD    │  Alertas, tendencias, anomalías                          │
│  └──────┬───────┘                                                          │
│         │                                                                   │
│         ▼                                                                   │
│  ┌──────────────┐                                                          │
│  │   ANALIZAR   │  Estudiar patrones de defectos de IA                     │
│  │   PATRONES   │  Catalogar, clasificar, anticipar                        │
│  └──────┬───────┘                                                          │
│         │                                                                   │
│         ▼                                                                   │
│  ┌──────────────┐                                                          │
│  │   MEJORAR    │  Feedback a equipo para mejorar calidad de IA            │
│  │   SISTEMA    │  Actualizar contexto, refinar gates                      │
│  └──────┬───────┘                                                          │
│         │                                                                   │
│         └──────────────────────────────────────────────────────────────────┘│
│                                                                             │
└─────────────────────────────────────────────────────────────────────────────┘
```

### Primer Día del QA Lead IA-Native: Guía Paso a Paso

#### Paso 1: Auditoría del Estado de Testing Actual

```yaml
# AUDITORÍA DE TESTING - QA Lead IA-Native
# QA Lead: Pedro Sánchez
# Fecha: 2026-01-24

estado_actual_testing:

  cobertura:
    total: 72%
    por_modulo:
      frontend_components: 85%
      frontend_hooks: 68%
      backend_use_cases: 78%
      backend_repositories: 65%
      infrastructure_api: 70%
      domain: 90%

  tipos_de_tests:
    unit: 450
    integration: 120
    e2e: 35
    performance: 8
    security: 12

  tests_generados_por_ia:
    total: 180  # 40% de unit tests
    calidad_estimada: "variable"
    problemas_detectados:
      - "15% son tests triviales (solo verifican que función existe)"
      - "8% tienen assertions incorrectas"
      - "5% prueban implementación, no comportamiento"

  tiempo_ci:
    total: "12 minutos"
    unit: "3 min"
    integration: "5 min"
    e2e: "4 min"

analisis_por_origen_de_codigo:

  codigo_humano:
    bugs_ultimo_trimestre: 12
    bugs_detectados_en_ci: 9  # 75%
    bugs_en_produccion: 3

  codigo_ia_con_alta_autonomia:
    bugs_ultimo_trimestre: 8
    bugs_detectados_en_ci: 5  # 62.5%
    bugs_en_produccion: 3

  codigo_ia_con_supervision:
    bugs_ultimo_trimestre: 5
    bugs_detectados_en_ci: 4  # 80%
    bugs_en_produccion: 1

patrones_de_defectos_ia:

  patron_1:
    nombre: "Off-by-one en iteraciones"
    frecuencia: "Alta"
    ejemplo: "for i in range(len(items)) en vez de for item in items"
    tests_existentes: "Insuficientes"

  patron_2:
    nombre: "Null/undefined no manejado"
    frecuencia: "Alta"
    ejemplo: "Acceso a propiedades sin verificar existencia"
    tests_existentes: "Parciales"

  patron_3:
    nombre: "Race conditions en async"
    frecuencia: "Media"
    ejemplo: "Await faltante, estado mutado sin sincronización"
    tests_existentes: "Muy pocos"

  patron_4:
    nombre: "Validación de entrada incompleta"
    frecuencia: "Media"
    ejemplo: "Valida tipo pero no rango o formato"
    tests_existentes: "Parciales"

  patron_5:
    nombre: "Queries N+1"
    frecuencia: "Alta"
    ejemplo: "Loop con query individual en vez de batch"
    tests_existentes: "Ninguno específico"

brechas_identificadas:
  - "No hay gate de query count para detectar N+1"
  - "Tests de concurrencia casi inexistentes"
  - "No se validan tests generados por IA"
  - "Cobertura de edge cases es baja"
  - "No hay tests de mutación para verificar calidad de tests"

plan_de_mejora:
  prioridad_1: "Agregar gate de validación de tests de IA"
  prioridad_2: "Implementar tests para patrones de defectos conocidos"
  prioridad_3: "Agregar monitoreo de query count"
  prioridad_4: "Tests de concurrencia para código async"
```

#### Paso 2: Diseñar Sistema de Gates de Calidad

```yaml
# SISTEMA DE GATES DE CALIDAD - QA Lead IA-Native

gates_por_nivel_de_riesgo:

  # ═══════════════════════════════════════════════════════════════════════
  # GATES PARA CÓDIGO DE BAJO RIESGO
  # ═══════════════════════════════════════════════════════════════════════

  bajo_riesgo:
    descripcion: "UI components, documentación, tests"
    gates:
      - name: "Lint & Format"
        tipo: "blocking"
        comando: "npm run lint && npm run format:check"
        tiempo_maximo: "1 min"

      - name: "Type Check"
        tipo: "blocking"
        comando: "npm run type-check"
        tiempo_maximo: "2 min"

      - name: "Unit Tests"
        tipo: "blocking"
        comando: "npm run test:unit"
        tiempo_maximo: "3 min"
        umbral_cobertura: 70%

      - name: "Snapshot Tests"
        tipo: "blocking"
        comando: "npm run test:snapshot"
        tiempo_maximo: "1 min"

    auto_merge_si: "Todos los gates pasan"

  # ═══════════════════════════════════════════════════════════════════════
  # GATES PARA CÓDIGO DE MEDIO RIESGO
  # ═══════════════════════════════════════════════════════════════════════

  medio_riesgo:
    descripcion: "Lógica de negocio, hooks, use cases"
    gates:
      - name: "Todos los gates de bajo riesgo"
        tipo: "blocking"

      - name: "Integration Tests"
        tipo: "blocking"
        comando: "npm run test:integration"
        tiempo_maximo: "5 min"
        umbral_cobertura: 80%

      - name: "Test Quality Check"
        tipo: "blocking"
        comando: "npm run test:quality"
        validaciones:
          - "No tests vacíos (expect() sin argumentos)"
          - "No tests sin assertions"
          - "Al menos 2 assertions por test de lógica"

      - name: "Complexity Check"
        tipo: "warning"
        comando: "npm run complexity:check"
        umbral:
          cyclomatic: 10
          cognitive: 15

      - name: "Security Scan"
        tipo: "blocking"
        comando: "npm run security:scan"

    requiere_review: true
    auto_merge_si: false

  # ═══════════════════════════════════════════════════════════════════════
  # GATES PARA CÓDIGO DE ALTO RIESGO
  # ═══════════════════════════════════════════════════════════════════════

  alto_riesgo:
    descripcion: "Datos de usuario, integraciones, APIs"
    gates:
      - name: "Todos los gates de medio riesgo"
        tipo: "blocking"

      - name: "E2E Tests"
        tipo: "blocking"
        comando: "npm run test:e2e -- --tag=high-risk"
        tiempo_maximo: "10 min"

      - name: "Query Count Verification"
        tipo: "blocking"
        comando: "npm run test:query-count"
        umbral: "Max 5 queries por endpoint"

      - name: "Data Validation Tests"
        tipo: "blocking"
        comando: "npm run test:data-validation"
        cobertura_edge_cases: 90%

      - name: "Concurrency Tests"
        tipo: "blocking"
        comando: "npm run test:concurrency"
        escenarios:
          - "Acceso simultáneo"
          - "Race conditions"
          - "Deadlocks"

      - name: "SAST (Security)"
        tipo: "blocking"
        comando: "semgrep --config=p/security-audit"

    requiere_review: "Tech Lead + QA Lead"
    validacion_manual_obligatoria:
      - "Revisión de edge cases cubiertos"
      - "Verificación de rollback"

  # ═══════════════════════════════════════════════════════════════════════
  # GATES PARA CÓDIGO CRÍTICO
  # ═══════════════════════════════════════════════════════════════════════

  critico:
    descripcion: "Pagos, seguridad, datos sensibles"
    gates:
      - name: "Todos los gates de alto riesgo"
        tipo: "blocking"

      - name: "Penetration Test Suite"
        tipo: "blocking"
        comando: "npm run test:pentest"

      - name: "Compliance Check"
        tipo: "blocking"
        comando: "npm run test:compliance"
        frameworks:
          - "PCI-DSS (si es pagos)"
          - "GDPR (si es datos personales)"
          - "HIPAA (si es salud)"

      - name: "Chaos Engineering"
        tipo: "blocking"
        comando: "npm run test:chaos"
        escenarios:
          - "Falla de DB"
          - "Timeout de API externa"
          - "Alta carga"

      - name: "Audit Trail Verification"
        tipo: "blocking"
        comando: "npm run test:audit-trail"

    requiere_review: "Tech Lead + QA Lead + Security Lead"
    testing_manual_obligatorio:
      - "Test exploratorio de seguridad"
      - "Revisión de logs de auditoría"
      - "Simulación de escenarios de fraude"

gate_de_validacion_de_tests_ia:
  descripcion: "Gate especial para validar tests generados por IA"
  aplica_a: "Todo PR que incluya tests nuevos generados por IA"

  validaciones:
    - name: "No Empty Tests"
      regla: "Detectar tests sin assertions o con expect().toBeTruthy() vacío"
      accion: "blocking"

    - name: "Meaningful Assertions"
      regla: "Al menos una assertion que verifique comportamiento, no existencia"
      ejemplo_malo: "expect(fn).toBeDefined()"
      ejemplo_bueno: "expect(fn(input)).toEqual(expectedOutput)"
      accion: "blocking"

    - name: "Test Description Matches"
      regla: "El nombre del test debe coincidir con lo que prueba"
      accion: "warning"

    - name: "No Implementation Testing"
      regla: "Detectar tests que verifican implementación interna"
      ejemplo_malo: "expect(obj._privateMethod).toHaveBeenCalled()"
      accion: "warning"

    - name: "Edge Cases Included"
      regla: "Tests de funciones deben incluir al menos 1 edge case"
      edge_cases_comunes:
        - "null/undefined"
        - "empty array/string"
        - "boundary values"
      accion: "warning"

    - name: "Mutation Score"
      regla: "Tests deben detectar al menos 70% de mutaciones"
      herramienta: "stryker-mutator"
      accion: "warning (info en PR)"
```

#### Paso 3: Implementar Catálogo de Tests para Patrones de IA

```python
# tests/ia_patterns/test_common_ia_mistakes.py
"""
Tests específicos para detectar errores comunes de IA.
QA Lead: Pedro Sánchez

Estos tests están diseñados para capturar patrones de error
que la IA comete frecuentemente y que los tests estándar
pueden no detectar.
"""

import pytest
from typing import Any
import asyncio

# ═══════════════════════════════════════════════════════════════════════════
# PATRÓN 1: Off-by-one errors
# La IA frecuentemente comete errores de índice
# ═══════════════════════════════════════════════════════════════════════════

class TestOffByOneErrors:
    """Tests para detectar errores de índice comunes en código de IA."""

    def test_iteration_includes_last_element(self):
        """La IA a veces usa range(len-1) o < en vez de <=."""
        items = [1, 2, 3, 4, 5]
        result = process_items(items)
        assert len(result) == 5, "Debe procesar TODOS los elementos"

    def test_slice_boundaries(self):
        """La IA confunde slices inclusivos vs exclusivos."""
        text = "Hello World"
        result = get_substring(text, 0, 5)
        assert result == "Hello", "Slice debe incluir índice final esperado"

    def test_pagination_last_page(self):
        """La IA calcula mal la última página."""
        total_items = 25
        page_size = 10
        pages = calculate_pages(total_items, page_size)
        assert pages == 3, "25 items / 10 per page = 3 páginas"


# ═══════════════════════════════════════════════════════════════════════════
# PATRÓN 2: Null/Undefined handling
# La IA olvida verificar existencia antes de acceder
# ═══════════════════════════════════════════════════════════════════════════

class TestNullHandling:
    """Tests para verificar manejo correcto de valores nulos."""

    def test_handles_none_input(self):
        """Función debe manejar None sin explotar."""
        result = process_user(None)
        assert result is None or isinstance(result, dict)

    def test_handles_missing_nested_property(self):
        """Acceso a propiedades anidadas con datos faltantes."""
        user = {"name": "John"}  # Sin 'address'
        result = get_user_city(user)
        assert result is None, "Debe retornar None si falta propiedad"

    def test_handles_empty_list(self):
        """Operaciones en lista vacía."""
        result = get_first_item([])
        assert result is None, "Lista vacía debe retornar None, no explotar"

    def test_handles_empty_string(self):
        """String vacío no debe tratarse como valor válido."""
        result = validate_email("")
        assert result is False, "String vacío no es email válido"


# ═══════════════════════════════════════════════════════════════════════════
# PATRÓN 3: Async/Concurrency issues
# La IA olvida awaits o crea race conditions
# ═══════════════════════════════════════════════════════════════════════════

class TestAsyncPatterns:
    """Tests para verificar correcta implementación async."""

    @pytest.mark.asyncio
    async def test_all_async_calls_awaited(self):
        """Verificar que no hay promesas sin await."""
        result = await fetch_user_with_posts(user_id=1)
        # Si hay un await faltante, esto será una coroutine, no data
        assert not asyncio.iscoroutine(result)
        assert isinstance(result, dict)

    @pytest.mark.asyncio
    async def test_concurrent_updates_safe(self):
        """Verificar que updates concurrentes no causan race condition."""
        counter = Counter(initial=0)

        # Simular 100 incrementos concurrentes
        await asyncio.gather(*[
            counter.increment() for _ in range(100)
        ])

        assert counter.value == 100, "Race condition detectada"

    @pytest.mark.asyncio
    async def test_resource_cleanup_on_error(self):
        """Recursos deben liberarse incluso si hay error."""
        connection = await create_connection()

        try:
            await failing_operation(connection)
        except Exception:
            pass

        assert connection.is_closed, "Conexión debe cerrarse en finally"


# ═══════════════════════════════════════════════════════════════════════════
# PATRÓN 4: Validation gaps
# La IA valida tipo pero no rango, formato o reglas de negocio
# ═══════════════════════════════════════════════════════════════════════════

class TestValidationCompleteness:
    """Tests para verificar validación completa de inputs."""

    def test_validates_string_length(self):
        """No solo tipo string, también longitud."""
        # String de 1000 caracteres (excede límite razonable de nombre)
        long_name = "a" * 1000
        with pytest.raises(ValidationError):
            create_user(name=long_name)

    def test_validates_number_range(self):
        """No solo tipo number, también rango."""
        with pytest.raises(ValidationError):
            create_product(price=-10)  # Precio negativo

        with pytest.raises(ValidationError):
            create_product(price=999999999)  # Precio irreal

    def test_validates_email_format(self):
        """No solo string, también formato de email."""
        invalid_emails = [
            "not-an-email",
            "@nodomain.com",
            "spaces in@email.com",
            "missing@domain",
        ]
        for email in invalid_emails:
            with pytest.raises(ValidationError):
                create_user(email=email)

    def test_validates_business_rules(self):
        """Validación de reglas de negocio, no solo formato."""
        # Fecha de nacimiento en el futuro
        with pytest.raises(ValidationError):
            create_user(birth_date="2030-01-01")


# ═══════════════════════════════════════════════════════════════════════════
# PATRÓN 5: N+1 Query detection
# La IA frecuentemente crea loops con queries individuales
# ═══════════════════════════════════════════════════════════════════════════

class TestQueryEfficiency:
    """Tests para detectar problemas de N+1 queries."""

    def test_list_users_query_count(self, query_counter):
        """Listar usuarios no debe hacer query por usuario."""
        users = list_users(limit=50)

        # Máximo: 1 query para usuarios + 1 para roles (eager load)
        assert query_counter.count <= 3, \
            f"N+1 detectado: {query_counter.count} queries para 50 usuarios"

    def test_user_with_orders_query_count(self, query_counter):
        """Obtener usuario con órdenes debe usar join/eager."""
        user = get_user_with_orders(user_id=1)

        # Máximo: 1 query con join
        assert query_counter.count <= 2, \
            f"N+1 detectado: {query_counter.count} queries"

    def test_batch_operation_uses_bulk(self, query_counter):
        """Operaciones batch deben usar bulk insert/update."""
        items = [{"name": f"Item {i}"} for i in range(100)]
        create_items_batch(items)

        # Debe ser 1 bulk insert, no 100 inserts individuales
        assert query_counter.count <= 2, \
            f"No usa bulk: {query_counter.count} queries para 100 items"
```

### Ejemplos Prácticos del QA Lead IA-Native

#### Ejemplo 1: Detectar Tests de IA de Baja Calidad

```yaml
# CASO: Tests generados por IA que son inútiles
# QA Lead: Pedro Sánchez
# Fecha: 2026-01-24

situacion:
  descripcion: |
    Un PR generado por IA incluye 15 tests nuevos para un módulo
    de cálculo de precios. CI reporta 95% de cobertura.
    Sin embargo, al revisar manualmente, se detecta que los
    tests son de muy baja calidad.

  pr_number: "#456"
  archivos_de_test: "src/services/pricing.test.ts"
  cobertura_reportada: 95%

analisis:

  tests_problematicos:

    test_1:
      codigo: |
        test('calculatePrice should work', () => {
          const result = calculatePrice(100, 0.1);
          expect(result).toBeDefined();
        });
      problema: "Solo verifica que retorna algo, no el valor correcto"
      tipo: "Test trivial"

    test_2:
      codigo: |
        test('calculatePrice handles discount', () => {
          const result = calculatePrice(100, 0.2);
          expect(typeof result).toBe('number');
        });
      problema: "Solo verifica tipo, no valor"
      tipo: "Test trivial"

    test_3:
      codigo: |
        test('applyTax works correctly', () => {
          const price = 100;
          const tax = 0.21;
          const result = applyTax(price, tax);
          expect(result).toBe(price * (1 + tax));
        });
      problema: "Replica la implementación en el test"
      tipo: "Test tautológico"

    test_4:
      codigo: |
        test('handles negative price', () => {
          // TODO: implement
          expect(true).toBe(true);
        });
      problema: "Test vacío que siempre pasa"
      tipo: "Test vacío"

    test_5:
      codigo: |
        test('complex calculation', () => {
          const result = calculateTotal(items, discount, tax);
          expect(result).toMatchSnapshot();
        });
      problema: "Snapshot sin verificación de lógica"
      tipo: "Snapshot inapropiado"

  resumen:
    tests_totales: 15
    tests_triviales: 6
    tests_vacios: 2
    tests_tautologicos: 3
    tests_utiles: 4
    cobertura_real_util: "~30%"

accion_del_qa:

  paso_1_rechazar_pr:
    mensaje: |
      ## ❌ Tests de baja calidad detectados

      Este PR tiene 95% de cobertura reportada, pero el análisis
      muestra que la mayoría de los tests son triviales o inútiles.

      ### Problemas encontrados:
      - 6 tests solo verifican `toBeDefined()` o `typeof`
      - 2 tests vacíos (`expect(true).toBe(true)`)
      - 3 tests replican la implementación (tautológicos)

      ### Tests que necesitan reescritura:
      [Lista detallada]

      ### Guía para tests útiles:
      ❌ `expect(result).toBeDefined()` - No prueba nada
      ✅ `expect(result).toBe(121)` - Prueba valor específico

      ❌ `expect(result).toBe(a * b)` - Replica implementación
      ✅ `expect(calculatePrice(100, 0.21)).toBe(121)` - Valor hardcodeado

  paso_2_actualizar_gate:
    nombre: "Test Quality Gate"
    reglas_nuevas:
      - "Prohibir tests con solo `toBeDefined()` en funciones que retornan valores"
      - "Prohibir tests con `expect(true).toBe(true)`"
      - "Requerir al menos un assertion con valor específico por test"
      - "Detectar y alertar sobre tests que replican implementación"

  paso_3_actualizar_contexto_ia:
    archivo: "AGENTS.md"
    seccion: "Testing Patterns"
    contenido_a_agregar: |
      ## Tests de Calidad

      ### ❌ NUNCA generar:
      ```typescript
      // Test trivial - no prueba nada útil
      expect(result).toBeDefined();
      expect(typeof result).toBe('number');

      // Test vacío
      expect(true).toBe(true);

      // Test tautológico - replica implementación
      expect(result).toBe(input * 1.21);
      ```

      ### ✅ SIEMPRE generar:
      ```typescript
      // Test con valor específico
      expect(calculatePrice(100, 0.21)).toBe(121);

      // Test de edge case
      expect(calculatePrice(0, 0.21)).toBe(0);
      expect(() => calculatePrice(-1, 0.21)).toThrow();

      // Test de comportamiento
      expect(applyDiscount(100, 50)).toBe(50);
      expect(applyDiscount(100, 100)).toBe(0);
      expect(applyDiscount(100, 150)).toBe(0); // No negativo
      ```

resultado:
  - "PR rechazado con feedback detallado"
  - "Gate de calidad de tests actualizado"
  - "Contexto de IA mejorado para futuros tests"
  - "IA regeneró tests con 85% de tests útiles en segundo intento"
```

### Anti-patrones del QA Lead IA-Native

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                 ANTI-PATRONES DEL QA LEAD IA-NATIVE                         │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                             │
│  ❌ ANTI-PATRÓN 1: "El QA que Confía en Cobertura"                         │
│  ──────────────────────────────────────────────────────────────────────────│
│  Descripción: Asumir que alta cobertura = alta calidad                     │
│  Síntoma: "95% de cobertura, los tests están bien"                         │
│  Problema: Tests de IA pueden ser triviales y dar cobertura falsa          │
│  Solución: Validar calidad de tests, no solo cantidad                      │
│                                                                             │
│  ❌ ANTI-PATRÓN 2: "El QA Manual en Mundo de IA"                           │
│  ──────────────────────────────────────────────────────────────────────────│
│  Descripción: Seguir testeando manualmente todo el código                  │
│  Síntoma: "Yo pruebo cada PR a mano antes de aprobar"                      │
│  Problema: No escala con el volumen de código de IA                        │
│  Solución: Diseñar automatización, reservar manual para alto riesgo        │
│                                                                             │
│  ❌ ANTI-PATRÓN 3: "El QA al Final del Proceso"                            │
│  ──────────────────────────────────────────────────────────────────────────│
│  Descripción: Solo intervenir cuando el código está "listo"                │
│  Síntoma: "Avísenme cuando terminen para testear"                          │
│  Problema: Bugs de IA se propagan si no se detectan temprano               │
│  Solución: Gates en CI que bloquean antes de merge                         │
│                                                                             │
│  ❌ ANTI-PATRÓN 4: "El QA que Ignora Patrones de IA"                       │
│  ──────────────────────────────────────────────────────────────────────────│
│  Descripción: No estudiar los errores específicos de la IA                 │
│  Síntoma: "Un bug es un bug, no importa quién lo escribió"                 │
│  Problema: Se pierden oportunidades de prevención sistemática              │
│  Solución: Catalogar y crear tests específicos para patrones de IA         │
│                                                                             │
│  ❌ ANTI-PATRÓN 5: "El QA Pasivo"                                          │
│  ──────────────────────────────────────────────────────────────────────────│
│  Descripción: Solo reportar bugs, no mejorar el sistema                    │
│  Síntoma: "Encontré 10 bugs este sprint, hice mi trabajo"                  │
│  Problema: Los mismos bugs se repiten porque no se mejora contexto         │
│  Solución: Feedback activo para mejorar prompts y guardrails               │
│                                                                             │
└─────────────────────────────────────────────────────────────────────────────┘
```

### Métricas del QA Lead IA-Native

```yaml
metricas_de_efectividad:

  tasa_deteccion_pre_merge:
    definicion: "% de bugs detectados antes de merge"
    objetivo: ">90%"
    segmentacion: "Por origen (humano vs IA)"

  calidad_de_tests:
    definicion: "% de tests que detectan mutaciones"
    herramienta: "Mutation testing (Stryker)"
    objetivo: ">70%"

  falsos_positivos_gates:
    definicion: "% de bloqueos de CI que fueron incorrectos"
    objetivo: "<5%"

metricas_de_prevencion:

  bugs_por_patron_ia:
    definicion: "Bugs categorizados por patrón conocido de IA"
    uso: "Identificar qué patrones necesitan más tests"
    objetivo: "Tendencia decreciente por patrón"

  efectividad_de_contexto:
    definicion: "Reducción de bugs después de actualizar contexto de IA"
    medicion: "Comparar bugs antes/después de mejora de AGENTS.md"
```

### Conclusión: El Nuevo Contrato del QA Lead

El QA Lead IA-Native firma un nuevo contrato implícito con su organización:

> **"Acepto que mi rol ya no es encontrar bugs al final del proceso, sino diseñar sistemas que impidan que los bugs existan. Asumo la responsabilidad de crear gates automatizados, validar la calidad de tests generados por IA, catalogar patrones de error, y mejorar continuamente el contexto que la IA utiliza. Entiendo que la calidad se construye en cada paso del proceso, no se inspecciona al final."**

El QA Lead IA-Native es, en esencia, un **ingeniero de confiabilidad**: alguien que diseña sistemas de verificación que operan continuamente para asegurar que tanto humanos como agentes de IA produzcan código de calidad.

---

## ★ El Programador en 2026: De Escritor de Código a Arquitecto de Decisiones ★

> **"El programador del futuro no será quien mejor escriba código, sino quien mejor sepa gobernar a quienes lo escriben por él."**

La transformación más profunda y personal que introduce el paradigma IA-Native ocurre en la identidad misma del programador. Durante décadas, ser desarrollador de software significaba dominar lenguajes, frameworks y patrones; significaba traducir problemas del mundo real en instrucciones ejecutables por una máquina. El valor del programador residía en su capacidad de producción: líneas de código, funcionalidades implementadas, bugs resueltos.

En el año 2026, esta definición se vuelve obsoleta.

Esta sección desarrolla en profundidad cómo el programador individual experimenta, adapta y prospera en el paradigma IA-Native, proporcionando guías prácticas, ejemplos del día a día y herramientas concretas para la transición personal.

### La Naturaleza del Cambio: La Gran Inversión del Rol

El programador deja de ser un **productor** y se convierte en un **supervisor cognitivo**. Este cambio no es gradual ni opcional; es estructural. Cuando un agente de inteligencia artificial puede generar en minutos lo que antes tomaba días, el cuello de botella del desarrollo se desplaza radicalmente. Ya no está en la escritura del código, sino en su comprensión, validación y gobierno.

### Las Seis Dimensiones del Programador IA-Native

El rol del programador IA-Native se articula en seis dimensiones que definen su trabajo diario:

**Dimensión 1: Definidor de Intenciones**

El programador ya no escribe código; escribe *políticas*. Su trabajo comienza mucho antes de la primera línea: debe articular con precisión qué quiere lograr, estableciendo objetivos claros que un agente pueda interpretar y ejecutar.

```yaml
# Ejemplo: Intención bien definida vs mal definida

mal_definida:
  prompt: "Agrega validación al formulario"
  problema: "¿Qué campos? ¿Qué reglas? ¿Qué mensajes de error?"
  resultado_tipico: "IA adivina, probablemente mal"

bien_definida:
  prompt: |
    Agregar validación al formulario de registro de usuario:
    - Campo email: formato válido, dominio no desechable
    - Campo password: mínimo 8 caracteres, 1 mayúscula, 1 número
    - Campo edad: entre 18 y 120 años
    - Mostrar errores inline debajo de cada campo
    - Deshabilitar submit hasta que todo sea válido
    - Usar los componentes de error existentes en @/components/forms
  resultado_tipico: "IA produce exactamente lo necesario"
```

**Dimensión 2: Establecedor de Límites**

Tan importante como definir qué debe hacerse es definir qué *no* debe hacerse. El programador diseña guardrails explícitos:

```yaml
# Ejemplo: Límites explícitos en un prompt

contexto: "Refactorizar el módulo de carrito de compras"

limites_explicitos:
  puede_hacer:
    - "Extraer funciones duplicadas"
    - "Mejorar nombres de variables"
    - "Agregar type hints faltantes"
    - "Reorganizar imports"

  no_puede_hacer:
    - "Cambiar la lógica de cálculo de precios"
    - "Modificar la estructura de datos del carrito"
    - "Agregar nuevas dependencias"
    - "Tocar los tests existentes (solo agregar nuevos)"

  razon_de_limites: |
    La lógica de precios está auditada y certificada.
    Cualquier cambio requiere re-certificación.
```

**Dimensión 3: Evaluador de Resultados**

El programador debe desarrollar la habilidad de evaluar código que no escribió:

```
┌─────────────────────────────────────────────────────────────────────────────┐
│              PROCESO DE EVALUACIÓN DE CÓDIGO DE IA                          │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                             │
│  PASO 1: VERIFICAR ALCANCE (2 min)                                         │
│  ┌─────────────────────────────────────────────────────────────────────┐   │
│  │ • ¿Los archivos modificados son los esperados?                      │   │
│  │ • ¿Hay archivos que no deberían estar modificados?                  │   │
│  │ • ¿El volumen de cambios es razonable para la tarea?               │   │
│  └─────────────────────────────────────────────────────────────────────┘   │
│                                                                             │
│  PASO 2: REVISAR ESTRUCTURA (5 min)                                        │
│  ┌─────────────────────────────────────────────────────────────────────┐   │
│  │ • ¿Sigue los patrones del proyecto?                                 │   │
│  │ • ¿Las dependencias van en la dirección correcta?                   │   │
│  │ • ¿Los nombres son consistentes con el resto del código?           │   │
│  └─────────────────────────────────────────────────────────────────────┘   │
│                                                                             │
│  PASO 3: ANALIZAR LÓGICA (10-30 min)                                       │
│  ┌─────────────────────────────────────────────────────────────────────┐   │
│  │ • ¿El código hace lo que se pidió?                                  │   │
│  │ • ¿Maneja los edge cases?                                          │   │
│  │ • ¿Hay lógica que no entiendo? (Si sí, investigar antes de aprobar)│   │
│  └─────────────────────────────────────────────────────────────────────┘   │
│                                                                             │
│  PASO 4: VERIFICAR TESTS (5 min)                                           │
│  ┌─────────────────────────────────────────────────────────────────────┐   │
│  │ • ¿Hay tests para la nueva funcionalidad?                          │   │
│  │ • ¿Los tests son significativos (no triviales)?                    │   │
│  │ • ¿Cubren los casos importantes?                                   │   │
│  └─────────────────────────────────────────────────────────────────────┘   │
│                                                                             │
│  PASO 5: DECISIÓN (1 min)                                                  │
│  ┌─────────────────────────────────────────────────────────────────────┐   │
│  │ □ APROBAR - Todo bien                                              │   │
│  │ □ PEDIR CAMBIOS - Problemas específicos identificados              │   │
│  │ □ RECHAZAR Y REGENERAR - Enfoque incorrecto                        │   │
│  │ □ ESCALAR - Necesito ayuda de otro (Tech Lead, Security)          │   │
│  └─────────────────────────────────────────────────────────────────────┘   │
│                                                                             │
└─────────────────────────────────────────────────────────────────────────────┘
```

**Dimensión 4: Tomador de Decisiones de Integración**

El momento del merge es una *decisión de responsabilidad*:

```yaml
# Checklist mental antes de aprobar un PR de IA

preguntas_obligatorias:
  - pregunta: "¿Entiendo qué hace este código?"
    si_no: "No aprobar hasta entender"

  - pregunta: "¿Puedo explicar este cambio a otro dev?"
    si_no: "Necesito más tiempo de análisis"

  - pregunta: "¿Si esto falla en producción, podré debuggearlo?"
    si_no: "Pedir documentación o comentarios"

  - pregunta: "¿Asumo responsabilidad por este código?"
    si_no: "No aprobar - escalar o rechazar"

recordatorio: |
  Al hacer clic en "Approve", estás firmando:
  "Revisé este código, lo entiendo, y asumo
  responsabilidad por sus consecuencias."
```

**Dimensión 5: Mantenedor de Conocimiento**

El programador debe activamente combatir la pérdida de comprensión global:

```yaml
# Prácticas para mantener conocimiento del sistema

practicas_diarias:
  - nombre: "Lectura de diffs de otros"
    frecuencia: "30 min/día"
    beneficio: "Entender cambios que no escribí"

  - nombre: "Documentar decisiones no obvias"
    frecuencia: "Cada vez que apruebo algo no trivial"
    formato: "Comentario en PR o ADR"

  - nombre: "Code walkthrough semanal"
    frecuencia: "1 hora/semana"
    formato: "Un dev explica un módulo al equipo"

practicas_periodicas:
  - nombre: "Día sin IA"
    frecuencia: "1 día cada 2 semanas"
    beneficio: "Mantener habilidades de desarrollo directo"

  - nombre: "Auditoría de conocimiento"
    frecuencia: "Mensual"
    formato: "¿Puedo explicar cada módulo principal?"
```

**Dimensión 6: Comunicador con IA**

El programador desarrolla habilidades de comunicación con agentes:

```yaml
# Habilidades de comunicación con IA

habilidad_1_contexto:
  descripcion: "Dar contexto suficiente y preciso"
  ejemplo_malo: "Arregla el bug"
  ejemplo_bueno: |
    Bug: El carrito muestra precio incorrecto cuando hay descuento > 50%
    Ubicación: src/services/cart/priceCalculator.ts línea 45
    Comportamiento actual: Precio negativo cuando descuento > precio
    Comportamiento esperado: Precio mínimo debe ser 0
    Tests relacionados: tests/cart/priceCalculator.test.ts

habilidad_2_iteracion:
  descripcion: "Iterar efectivamente sobre resultados"
  proceso:
    - "Revisar primera generación"
    - "Identificar problemas específicos"
    - "Dar feedback preciso, no vago"
    - "Pedir regeneración parcial si es posible"
  ejemplo_feedback_malo: "Esto no está bien, hazlo de nuevo"
  ejemplo_feedback_bueno: |
    El cálculo del descuento es correcto, pero:
    1. Falta manejar el caso donde items es array vacío
    2. El nombre 'calc' debe ser 'calculateFinalPrice'
    3. Agregar type hint para el retorno

habilidad_3_desambiguacion:
  descripcion: "Anticipar y resolver ambigüedades"
  estrategia: "Antes de dar el prompt, preguntarte qué podría malinterpretarse"
  ejemplo: |
    "Ordena los usuarios" → ¿Alfabéticamente? ¿Por fecha? ¿Ascendente?
    Mejor: "Ordena los usuarios por fecha de creación, más recientes primero"
```

### El Ciclo de Trabajo del Programador IA-Native

```
┌─────────────────────────────────────────────────────────────────────────────┐
│             CICLO DE TRABAJO DEL PROGRAMADOR IA-NATIVE                      │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                             │
│  ┌──────────────┐                                                          │
│  │   ENTENDER   │  Leer el Policy Ticket, entender la intención            │
│  │    TAREA     │  ¿Qué quiero lograr? ¿Qué está prohibido?                │
│  └──────┬───────┘                                                          │
│         │                                                                   │
│         ▼                                                                   │
│  ┌──────────────┐                                                          │
│  │   PREPARAR   │  Reunir contexto para la IA                              │
│  │   CONTEXTO   │  Archivos relevantes, patrones, restricciones            │
│  └──────┬───────┘                                                          │
│         │                                                                   │
│         ▼                                                                   │
│  ┌──────────────┐                                                          │
│  │   REDACTAR   │  Escribir prompt claro y completo                        │
│  │    PROMPT    │  Incluir límites y expectativas                          │
│  └──────┬───────┘                                                          │
│         │                                                                   │
│         ▼                                                                   │
│  ┌──────────────┐                                                          │
│  │   EJECUTAR   │  Dejar que la IA genere                                  │
│  │      IA      │  (Puede ser instantáneo o tomar minutos)                 │
│  └──────┬───────┘                                                          │
│         │                                                                   │
│         ▼                                                                   │
│  ┌──────────────┐                                                          │
│  │   EVALUAR    │  Revisar el resultado críticamente                       │
│  │  RESULTADO   │  ¿Cumple? ¿Es seguro? ¿Lo entiendo?                      │
│  └──────┬───────┘                                                          │
│         │                                                                   │
│         ├─── Si no cumple ──→ ITERAR (dar feedback, regenerar)             │
│         │                                                                   │
│         ▼                                                                   │
│  ┌──────────────┐                                                          │
│  │   INTEGRAR   │  Aprobar y mergear asumiendo responsabilidad             │
│  │   O ESCALAR  │  O escalar si hay dudas                                  │
│  └──────┬───────┘                                                          │
│         │                                                                   │
│         ▼                                                                   │
│  ┌──────────────┐                                                          │
│  │  DOCUMENTAR  │  Registrar decisiones no obvias                          │
│  │  APRENDIZAJE │  Actualizar contexto si es necesario                     │
│  └──────────────┘                                                          │
│                                                                             │
└─────────────────────────────────────────────────────────────────────────────┘
```

### Un Día en la Vida del Programador IA-Native

```yaml
# UN DÍA TÍPICO - Programador IA-Native
# Desarrollador: Juan Martínez (Mid-level, 3 años de experiencia)
# Fecha: Martes típico

8:30_inicio:
  actividad: "Revisar dashboard de PRs pendientes"
  duracion: "15 min"
  detalle: |
    - 2 PRs míos de ayer esperando CI
    - 1 PR de IA que debo revisar (asignado por Tech Lead)
    - 3 notificaciones de cambios en módulos que mantengo

8:45_daily:
  actividad: "Daily standup IA-Native"
  duracion: "15 min"
  mi_reporte: |
    - Ayer: Completé PT-089 (refactor de carrito)
    - Hoy: PT-092 (nueva validación de checkout)
    - Bloqueos: Necesito clarificación del PO sobre regla de negocio

9:00_trabajo_principal:
  actividad: "Iniciar Policy Ticket PT-092"
  duracion: "2 horas"
  pasos:
    - paso_1:
        accion: "Leer PT completo"
        duracion: "10 min"
        notas: "Agregar validación de CVV, entiendo la intención"

    - paso_2:
        accion: "Revisar código existente de checkout"
        duracion: "20 min"
        notas: "Identifico dónde agregar validación, entiendo el flujo"

    - paso_3:
        accion: "Preparar prompt para IA"
        duracion: "15 min"
        prompt_creado: |
          Agregar validación de CVV al formulario de checkout.

          Contexto:
          - Archivo: src/components/checkout/PaymentForm.tsx
          - Componente de input existente: @/components/forms/Input
          - Validación existente usa react-hook-form + zod

          Requisitos:
          - CVV debe ser 3 o 4 dígitos (Visa/MC = 3, Amex = 4)
          - Detectar tipo de tarjeta del campo cardNumber
          - Error inline: "CVV debe ser {3|4} dígitos para {tipo}"
          - Input type="password" para ocultar

          NO hacer:
          - No modificar lógica de envío del formulario
          - No agregar dependencias nuevas
          - No tocar otros campos del formulario

    - paso_4:
        accion: "Ejecutar prompt en Claude Code"
        duracion: "2 min"
        resultado: "Genera 3 archivos: componente, schema, test"

    - paso_5:
        accion: "Revisar resultado"
        duracion: "30 min"
        revision:
          - "✅ Estructura correcta"
          - "✅ Usa componentes existentes"
          - "⚠️ Test no cubre caso de Amex"
          - "❌ Detecta tipo de tarjeta con regex incorrecto"

    - paso_6:
        accion: "Iterar con feedback"
        duracion: "15 min"
        feedback: |
          Dos correcciones:
          1. El regex para Amex debe ser ^3[47] no ^3[4]
          2. Agregar test para tarjeta Amex (4 dígitos CVV)

    - paso_7:
        accion: "Revisar segunda iteración"
        duracion: "10 min"
        resultado: "✅ Todo correcto, listo para PR"

11:00_break:
  actividad: "Café + revisar Slack"
  duracion: "15 min"

11:15_review_de_otros:
  actividad: "Revisar PR de IA asignado"
  duracion: "45 min"
  pr: "#1234 - Refactor de notificaciones"
  proceso:
    - "Verifico alcance: 4 archivos, esperado"
    - "Reviso estructura: sigue patrones"
    - "Analizo lógica: entiendo el cambio"
    - "Tests: presentes y significativos"
    - "Decisión: APROBAR con comentario menor"

12:00_almuerzo:
  actividad: "Almuerzo"
  duracion: "1 hora"

13:00_continuacion:
  actividad: "Finalizar PT-092"
  duracion: "1.5 horas"
  pasos:
    - "Crear PR con cambios"
    - "Escribir descripción detallada"
    - "Vincular al Policy Ticket"
    - "Asignar reviewer (Ana)"
    - "Esperar CI"

14:30_aprendizaje:
  actividad: "Code walkthrough semanal del equipo"
  duracion: "1 hora"
  tema: "El módulo de pagos - cómo funciona"
  valor: "Entiendo mejor código que no escribí"

15:30_segundo_ticket:
  actividad: "Iniciar PT-093 (mejora de performance)"
  duracion: "1.5 horas"
  detalle: "Ciclo similar al de la mañana"

17:00_cierre:
  actividad: "Cierre del día"
  duracion: "30 min"
  tareas:
    - "Revisar estado de mis PRs"
    - "Responder comentarios en code review"
    - "Actualizar PT-092 si Ana dejó feedback"
    - "Planificar mañana"

resumen_del_dia:
  codigo_escrito_por_mi: "~20 líneas (prompts + pequeños ajustes)"
  codigo_generado_por_ia: "~200 líneas"
  codigo_revisado: "~150 líneas (PR de otro)"
  tiempo_en_prompts: "30 min"
  tiempo_en_revision: "1.5 horas"
  tiempo_en_aprendizaje: "1 hora"
  policy_tickets_avanzados: 2
```

### Ejemplos Prácticos del Programador IA-Native

#### Ejemplo 1: Manejar un Resultado de IA que No Entiendo

```yaml
# SITUACIÓN: La IA generó código que funciona pero no entiendo
# Programador: Juan Martínez

contexto:
  tarea: "Optimizar query de productos con filtros"
  resultado_ia: |
    La IA generó una query con CTEs, window functions,
    y un patrón de indexing que nunca había visto.
    Los tests pasan. Performance mejoró 10x.
    Pero no entiendo cómo funciona.

decision_incorrecta:
  accion: "Aprobar porque funciona y los tests pasan"
  problema: |
    Si falla en producción, no podré debuggearlo.
    Si alguien pregunta, no podré explicarlo.
    Estoy aprobando código que no entiendo.

decision_correcta:
  paso_1:
    accion: "Pedir a la IA que explique"
    prompt: |
      Explica paso a paso qué hace esta query:
      [código]

      Específicamente:
      1. ¿Qué hace cada CTE?
      2. ¿Por qué usas window function aquí?
      3. ¿Cuál es el beneficio de performance?

  paso_2:
    accion: "Verificar la explicación"
    metodo: |
      - Ejecutar cada CTE por separado
      - Ver los resultados intermedios
      - Confirmar que la explicación coincide

  paso_3:
    accion: "Documentar para el futuro"
    donde: "Comentario en el código o PR description"
    contenido: |
      // Optimización de query con CTEs:
      // 1. CTE products_filtered: Pre-filtra por categoría
      // 2. CTE with_ranks: Agrega ranking por precio
      // 3. Final: Pagina sobre el ranking
      // Ver PR #1234 para explicación detallada

  paso_4:
    accion: "Ahora sí aprobar"
    condicion: "Solo después de entender"

resultado:
  - "Entendí una técnica nueva"
  - "Puedo debuggear si falla"
  - "El equipo puede entender el código"
  - "Mi conocimiento creció"

leccion: |
  "No aprobar código que no entiendes"
  no significa "rechazar todo lo complejo".
  Significa "invertir tiempo en entender antes de aprobar".
```

#### Ejemplo 2: La IA Insiste en un Enfoque Incorrecto

```yaml
# SITUACIÓN: La IA genera código con un patrón que sé que está mal
# Programador: Juan Martínez

contexto:
  tarea: "Agregar cache a las queries de productos"
  resultado_ia: |
    La IA implementó cache directamente en el use case,
    importando Redis desde infrastructure.
    Sé que esto viola Clean Architecture.

primer_intento:
  mi_feedback: "Esto viola Clean Architecture, hazlo bien"
  resultado_ia: "Genera lo mismo con nombres diferentes"
  problema: "La IA no entiende qué está mal"

segundo_intento:
  mi_feedback: |
    El problema específico es:
    - Los use cases no deben importar de infrastructure
    - La implementación de cache debe ser inyectada
    - Necesitas crear una interface en domain

    Patrón correcto:
    1. Interface CacheService en domain/interfaces/
    2. Implementación RedisCacheService en infrastructure/
    3. Use case recibe cache por constructor

  resultado_ia: "Genera código correcto con el patrón indicado"

tercer_intento_si_sigue_mal:
  estrategia: "Escribir la estructura yo y pedir a la IA que complete"
  mi_codigo: |
    # domain/interfaces/cache_service.py
    class CacheService(ABC):
        @abstractmethod
        async def get(self, key: str) -> Optional[T]: ...

        @abstractmethod
        async def set(self, key: str, value: T, ttl: int) -> None: ...

  prompt: |
    Dado esta interface, implementa:
    1. RedisCacheService en infrastructure/cache/
    2. Actualiza el use case para usar inyección de dependencias

  resultado: "La IA completa correctamente con la guía"

leccion: |
  A veces la IA necesita que le muestres el patrón correcto,
  no solo que le digas que está mal. Si insiste en un
  enfoque incorrecto, dar la estructura y pedir que complete.
```

### Anti-patrones del Programador IA-Native

```
┌─────────────────────────────────────────────────────────────────────────────┐
│               ANTI-PATRONES DEL PROGRAMADOR IA-NATIVE                       │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                             │
│  ❌ ANTI-PATRÓN 1: "El Aprobador Ciego"                                    │
│  ──────────────────────────────────────────────────────────────────────────│
│  Descripción: Aprobar código de IA sin entenderlo                          │
│  Síntoma: "Los tests pasan, debe estar bien"                               │
│  Problema: No podrás debuggear cuando falle                                │
│  Solución: No aprobar hasta entender                                       │
│                                                                             │
│  ❌ ANTI-PATRÓN 2: "El Prompt Vago"                                        │
│  ──────────────────────────────────────────────────────────────────────────│
│  Descripción: Dar instrucciones ambiguas a la IA                           │
│  Síntoma: "Arregla esto", "Mejora el código"                               │
│  Problema: La IA adivina, probablemente mal                                │
│  Solución: Prompts específicos con contexto y límites                      │
│                                                                             │
│  ❌ ANTI-PATRÓN 3: "El Dependiente Total"                                  │
│  ──────────────────────────────────────────────────────────────────────────│
│  Descripción: No poder programar sin IA                                    │
│  Síntoma: "Espera, deja que la IA lo haga" para todo                       │
│  Problema: Atrofia de habilidades fundamentales                            │
│  Solución: Práctica regular sin IA, code walkthroughs                      │
│                                                                             │
│  ❌ ANTI-PATRÓN 4: "El Iterador Infinito"                                  │
│  ──────────────────────────────────────────────────────────────────────────│
│  Descripción: Regenerar infinitamente esperando perfección                 │
│  Síntoma: "No, tampoco. Otra vez. No. Otra vez."                           │
│  Problema: A veces es más rápido escribir o editar manualmente             │
│  Solución: Después de 3 iteraciones, considerar hacer el cambio tú         │
│                                                                             │
│  ❌ ANTI-PATRÓN 5: "El Ignorante del Sistema"                              │
│  ──────────────────────────────────────────────────────────────────────────│
│  Descripción: No conocer el código que la IA está modificando              │
│  Síntoma: "No sé cómo funciona ese módulo, pero la IA lo arregló"          │
│  Problema: No puedes validar cambios en código que no conoces              │
│  Solución: Estudiar el código antes de delegar cambios                     │
│                                                                             │
│  ❌ ANTI-PATRÓN 6: "El Competidor de la IA"                                │
│  ──────────────────────────────────────────────────────────────────────────│
│  Descripción: Rechazar IA por orgullo de "hacerlo yo"                      │
│  Síntoma: "Prefiero escribirlo yo, así sé que está bien"                   │
│  Problema: Desperdicia la ventaja de velocidad de la IA                    │
│  Solución: Delegar lo delegable, enfocarse en lo que agrega valor          │
│                                                                             │
└─────────────────────────────────────────────────────────────────────────────┘
```

### Métricas del Programador IA-Native

```yaml
metricas_de_efectividad:

  ratio_aprobacion_vs_rechazo:
    definicion: "% de código de IA que apruebo vs rechazo"
    rango_saludable: "70-90% aprobación"
    si_muy_alto: "Posiblemente aprobando sin revisar bien"
    si_muy_bajo: "Posiblemente prompts de baja calidad"

  iteraciones_por_tarea:
    definicion: "Promedio de regeneraciones hasta aprobar"
    objetivo: "1-3 iteraciones"
    si_muy_alto: "Mejorar calidad de prompts"

  tiempo_de_revision:
    definicion: "Tiempo dedicado a revisar código de IA"
    rango_saludable: "20-40% del tiempo de desarrollo"
    si_muy_bajo: "Posiblemente revisiones superficiales"
    si_muy_alto: "Posiblemente la IA no está ayudando"

metricas_de_conocimiento:

  modulos_que_puedo_explicar:
    definicion: "% de módulos principales que puedo explicar"
    objetivo: ">80%"
    medicion: "Auto-evaluación mensual"

  bugs_en_codigo_que_aprobe:
    definicion: "Bugs en producción en código que yo aprobé"
    objetivo: "Tendencia decreciente"
    uso: "Mejorar proceso de revisión"

metricas_de_crecimiento:

  tecnicas_nuevas_aprendidas:
    definicion: "Técnicas o patrones nuevos aprendidos de código de IA"
    objetivo: "Al menos 2 por mes"
    importancia: "La IA puede ser fuente de aprendizaje"

  habilidades_mantenidas:
    definicion: "Capacidad de programar sin IA"
    medicion: "Días sin IA completados exitosamente"
    objetivo: "2 días/mes mínimo"
```

### Checklist de Adopción para el Programador

```
┌─────────────────────────────────────────────────────────────────────────────┐
│           CHECKLIST DE ADOPCIÓN - PROGRAMADOR IA-NATIVE                     │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                             │
│  SEMANA 1: FUNDAMENTOS                                                      │
│  □ Entender el concepto de Policy Ticket y gobernanza                      │
│  □ Aprender a leer un Policy Ticket asignado                               │
│  □ Entender niveles de autonomía de IA en el proyecto                      │
│  □ Conocer los guardrails y gates de CI configurados                       │
│  □ Practicar escribir prompts básicos                                      │
│                                                                             │
│  SEMANA 2: PRÁCTICA GUIADA                                                  │
│  □ Completar primer Policy Ticket de bajo riesgo con IA                    │
│  □ Pair programming con senior en review de código de IA                   │
│  □ Aprender a dar feedback efectivo para iteraciones                       │
│  □ Practicar el proceso de evaluación de resultados                        │
│  □ Documentar primeras lecciones aprendidas                                │
│                                                                             │
│  SEMANA 3-4: AUTONOMÍA PROGRESIVA                                          │
│  □ Completar Policy Tickets de bajo riesgo solo                            │
│  □ Comenzar a revisar PRs de IA (con supervisión)                          │
│  □ Identificar patrones de errores comunes de la IA                        │
│  □ Desarrollar biblioteca personal de prompts efectivos                    │
│  □ Participar en code walkthroughs                                         │
│                                                                             │
│  MES 2: CONSOLIDACIÓN                                                       │
│  □ Manejar Policy Tickets de riesgo medio                                  │
│  □ Revisar PRs de IA de forma autónoma                                     │
│  □ Contribuir a mejorar documentación de contexto                          │
│  □ Completar primer "día sin IA" exitosamente                              │
│  □ Poder explicar módulos que mantengo                                     │
│                                                                             │
│  MES 3+: MADUREZ                                                            │
│  □ Autonomía completa en dominios asignados                                │
│  □ Mentorear a otros en prácticas de supervisión                           │
│  □ Proponer mejoras a guardrails y contexto                                │
│  □ Balance saludable entre IA y habilidades propias                        │
│  □ Identidad profesional adaptada al nuevo paradigma                       │
│                                                                             │
└─────────────────────────────────────────────────────────────────────────────┘
```

### La Responsabilidad Inalienable

Este es quizás el principio más importante que el programador de 2026 debe internalizar:

> **Delegar ejecución no es delegar responsabilidad.**

Cuando un agente introduce un bug en producción, cuando un cambio automatizado genera una vulnerabilidad de seguridad, cuando una refactorización rompe funcionalidad crítica, la responsabilidad no recae en la IA. Recae en el humano que autorizó la acción, que revisó el resultado, que aprobó la integración.

Esta realidad no es un castigo; es la base misma de la profesión. El programador sigue siendo el eslabón responsable porque es el único que puede serlo. La inteligencia artificial no tiene ética, no tiene consecuencias personales, no responde ante clientes ni reguladores. El programador sí.

### Conclusión: El Nuevo Contrato del Programador

El Programador IA-Native firma un nuevo contrato implícito con su profesión:

> **"Acepto que mi valor ya no está en las líneas de código que escribo, sino en las decisiones que tomo, el código que valido, y la responsabilidad que asumo. Me comprometo a entender antes de aprobar, a mantener mis habilidades fundamentales, y a usar la IA como herramienta sin convertirme en dependiente de ella. Entiendo que cada merge es una firma profesional, y que delegar ejecución no me libera de responsabilidad."**

El programador de 2026 es, en esencia, un **arquitecto de decisiones**: alguien que diseña no solo sistemas técnicos, sino los marcos dentro de los cuales otros —humanos y máquinas— operan de manera segura y efectiva.

---

## ★ El DBA en el Marco IA-Native: De Administrador de Bases de Datos a Arquitecto de Integridad de Información ★

### El Cambio Fundamental: De "Guardián de la Base de Datos" a "Diseñador de Sistemas de Confiabilidad de Datos"

El DBA tradicional se definía por su dominio técnico sobre el motor de base de datos. Era el experto que sabía optimizar queries, diseñar índices, configurar replicación, ejecutar backups, y responder a emergencias de rendimiento. Su valor residía en conocimientos especializados que pocos otros en la organización poseían: la capacidad de "hablar" el lenguaje del motor de base de datos, de entender sus internals, de detectar cuellos de botella invisibles para otros.

En 2026, la IA puede generar queries optimizados en segundos, proponer índices basándose en análisis de patrones de acceso, escribir scripts de migración complejos, y hasta sugerir esquemas de particionamiento. Los LLMs han absorbido décadas de mejores prácticas de bases de datos y pueden aplicarlas instantáneamente. Un desarrollador junior con acceso a una IA puede producir código SQL que antes requería años de experiencia.

¿Significa esto que el DBA desaparece? Todo lo contrario. Significa que emerge un rol más crítico aún: el **Arquitecto de Integridad de Información**.

Los datos son el activo más valioso y más peligroso de una organización. Una migración mal ejecutada puede destruir información irrecuperable. Un índice incorrecto puede degradar el rendimiento de producción. Una constraint mal diseñada puede bloquear operaciones críticas de negocio. Un esquema que viola normalización puede crear inconsistencias que propaguen errores durante años.

La IA puede escribir código de base de datos rápidamente. Pero la IA no entiende:

- El **contexto histórico** de por qué ciertos datos están estructurados como están
- Las **implicaciones de negocio** de modificar un esquema en producción
- Los **patrones de uso reales** que no están documentados pero son críticos
- Las **dependencias ocultas** entre sistemas que comparten datos
- La **sensibilidad regulatoria** de ciertos campos y tablas

El DBA IA-Native no deja de ser técnico; se convierte en el experto que **diseña los sistemas de verificación que aseguran que las intervenciones de IA sobre datos sean seguras, reversibles y auditables**.

### Las Seis Dimensiones del DBA IA-Native

El DBA IA-Native opera en seis dimensiones interdependientes:

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                     EL DBA IA-NATIVE: SEIS DIMENSIONES                      │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                             │
│   ┌─────────────────┐     ┌─────────────────┐     ┌─────────────────┐      │
│   │   ARQUITECTO    │     │   GUARDIÁN DE   │     │   DISEÑADOR     │      │
│   │   DE ESQUEMAS   │────▶│   INTEGRIDAD    │────▶│   DE GATES      │      │
│   │   EVOLUTIVOS    │     │   DE DATOS      │     │   DE DATOS      │      │
│   └────────┬────────┘     └────────┬────────┘     └────────┬────────┘      │
│            │                       │                       │                │
│            │   ┌───────────────────┴───────────────────┐   │                │
│            │   │                                       │   │                │
│            ▼   ▼                                       ▼   ▼                │
│   ┌─────────────────┐                         ┌─────────────────┐          │
│   │    REVISOR DE   │                         │   ESTRATEGA DE  │          │
│   │   MIGRACIONES   │◀───────────────────────▶│   RENDIMIENTO   │          │
│   │   GENERADAS     │                         │   PREDICTIVO    │          │
│   └────────┬────────┘                         └────────┬────────┘          │
│            │                                           │                    │
│            │         ┌─────────────────┐               │                    │
│            └────────▶│   PROTECTOR DE  │◀──────────────┘                    │
│                      │   CONTINUIDAD   │                                    │
│                      │   OPERATIVA     │                                    │
│                      └─────────────────┘                                    │
│                                                                             │
│  ═══════════════════════════════════════════════════════════════════════   │
│                                                                             │
│   Ciclo de Trabajo:                                                         │
│   ANÁLISIS ──▶ DISEÑO ──▶ VALIDACIÓN ──▶ EJECUCIÓN ──▶ VERIFICACIÓN        │
│       │                                                      │              │
│       └──────────────── APRENDIZAJE ◀────────────────────────┘              │
│                                                                             │
└─────────────────────────────────────────────────────────────────────────────┘
```

#### Dimensión 1: Arquitecto de Esquemas Evolutivos

El DBA ya no diseña esquemas estáticos. Diseña **sistemas de evolución controlada** que permiten que los datos crezcan y cambien sin romper integridad ni compatibilidad.

**Antes (2020):**
```
DBA recibe requerimiento → Diseña esquema → Escribe migración → Ejecuta en producción
```

**Ahora (2026):**
```
DBA define políticas de evolución → IA propone cambios de esquema →
DBA valida compatibilidad y reversibilidad → Gates automáticos verifican →
DBA supervisa ejecución → Sistemas de rollback listos
```

El cambio clave: el DBA no diseña cada cambio individual, sino las **reglas bajo las cuales los cambios son válidos**.

**Ejemplo de Policy Ticket para evolución de esquemas:**

```yaml
policy_ticket:
  id: PT-DBA-2026-042
  tipo: evolucion_esquema
  dominio: base_datos_clientes
  clasificacion_riesgo: alto

  contexto:
    tabla_afectada: customers
    filas_estimadas: 2.3M
    dependencias:
      - orders (FK customer_id)
      - payments (FK customer_id)
      - audit_log (customer_id reference)
    ventana_mantenimiento: "Sábado 02:00-06:00 UTC"

  intencion: |
    Agregar soporte para múltiples direcciones por cliente.
    Actualmente la dirección está embebida en la tabla customers.
    Se necesita una tabla separada customer_addresses con relación 1:N.

  autonomia_ia:
    analisis_impacto: permitida
    generacion_migracion: con_revision
    generacion_rollback: con_revision
    ejecucion_staging: permitida
    ejecucion_produccion: prohibida

  requisitos_obligatorios:
    - Migración debe ser reversible sin pérdida de datos
    - Downtime máximo: 15 minutos
    - Queries existentes deben seguir funcionando (vista de compatibilidad)
    - Backup point-in-time antes de ejecución
    - Prueba completa en staging con datos anonimizados de producción

  validaciones_requeridas:
    - [ ] Script de migración revisado por DBA senior
    - [ ] Script de rollback probado en staging
    - [ ] Análisis de locks y bloqueos durante migración
    - [ ] Impacto en índices documentado
    - [ ] Queries críticos testeados con nuevo esquema
    - [ ] Comunicación a equipos dependientes completada

  responsable_aprobacion: dba-senior@empresa.com
  fecha_limite: 2026-01-28
```

#### Dimensión 2: Guardián de Integridad de Datos

La integridad de datos no es solo constraints técnicas; es la **promesa fundamental** de que los datos significan lo que dicen significar.

La IA puede generar código que inserta datos, actualiza registros, elimina filas. Pero no entiende las **invariantes de negocio** que esos datos representan:

- Un pedido no puede tener fecha_envio anterior a fecha_creacion
- Un cliente no puede tener saldo negativo si su tipo es "prepago"
- Un producto no puede tener precio_venta < precio_costo sin aprobación especial
- Un usuario no puede pertenecer a dos organizaciones competidoras simultáneamente

El DBA IA-Native diseña y mantiene el sistema de **constraints de negocio** que va más allá de lo que el motor de base de datos puede expresar nativamente:

```sql
-- Constraint técnica estándar
ALTER TABLE orders
ADD CONSTRAINT chk_order_dates
CHECK (ship_date IS NULL OR ship_date >= created_at);

-- Pero esto NO captura la regla de negocio completa.
-- El DBA IA-Native define triggers y validaciones adicionales:

CREATE OR REPLACE FUNCTION validate_order_business_rules()
RETURNS TRIGGER AS $$
BEGIN
    -- Regla 1: No enviar a clientes suspendidos
    IF NEW.status = 'shipped' THEN
        IF EXISTS (SELECT 1 FROM customers WHERE id = NEW.customer_id AND status = 'suspended') THEN
            RAISE EXCEPTION 'Cannot ship to suspended customer: %', NEW.customer_id;
        END IF;
    END IF;

    -- Regla 2: Pedidos > $10,000 requieren aprobación manual
    IF NEW.total_amount > 10000 AND NEW.approval_status != 'approved' THEN
        RAISE EXCEPTION 'Orders over $10,000 require manual approval';
    END IF;

    -- Regla 3: No modificar pedidos ya facturados
    IF TG_OP = 'UPDATE' AND OLD.invoice_id IS NOT NULL AND OLD.status != NEW.status THEN
        RAISE EXCEPTION 'Cannot modify status of invoiced order: %', OLD.id;
    END IF;

    RETURN NEW;
END;
$$ LANGUAGE plpgsql;

-- Este trigger captura conocimiento de negocio que la IA NO puede inferir
-- y que DEBE ser preservado incluso cuando la IA genera código de inserción/actualización
```

El trabajo del DBA es asegurar que estas reglas estén:
1. **Documentadas** en un lugar accesible para la IA
2. **Implementadas** a nivel de base de datos (no solo aplicación)
3. **Testeadas** con casos que la IA podría violar
4. **Monitoreadas** para detectar violaciones o intentos de bypass

#### Dimensión 3: Diseñador de Gates de Datos

El DBA IA-Native diseña los **gates automatizados** que validan cualquier cambio a datos o esquemas antes de que llegue a producción.

**Arquitectura de Gates de Base de Datos:**

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                    PIPELINE DE GATES PARA CAMBIOS DE BD                     │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                             │
│   CAMBIO PROPUESTO (Migración, Script, Schema Change)                       │
│         │                                                                   │
│         ▼                                                                   │
│   ┌───────────────────────────────────────────────────────────────────┐    │
│   │ GATE 1: ANÁLISIS ESTÁTICO                                         │    │
│   │   • Sintaxis SQL válida                                           │    │
│   │   • No DROP sin confirmación explícita                            │    │
│   │   • No DELETE/UPDATE sin WHERE                                    │    │
│   │   • No TRUNCATE en tablas protegidas                              │    │
│   │   • Convenciones de naming respetadas                             │    │
│   └─────────────────────────────────┬─────────────────────────────────┘    │
│                                     │ PASS                                  │
│                                     ▼                                       │
│   ┌───────────────────────────────────────────────────────────────────┐    │
│   │ GATE 2: ANÁLISIS DE IMPACTO                                       │    │
│   │   • Tablas afectadas identificadas                                │    │
│   │   • Filas estimadas a modificar                                   │    │
│   │   • Tiempo estimado de lock                                       │    │
│   │   • Dependencias de FK mapeadas                                   │    │
│   │   • Índices afectados identificados                               │    │
│   └─────────────────────────────────┬─────────────────────────────────┘    │
│                                     │ PASS                                  │
│                                     ▼                                       │
│   ┌───────────────────────────────────────────────────────────────────┐    │
│   │ GATE 3: EJECUCIÓN EN STAGING                                      │    │
│   │   • Migración ejecutada en copia de producción                    │    │
│   │   • Tiempo real de ejecución medido                               │    │
│   │   • Queries críticos ejecutados post-migración                    │    │
│   │   • Integridad referencial verificada                             │    │
│   │   • Rollback ejecutado y verificado                               │    │
│   └─────────────────────────────────┬─────────────────────────────────┘    │
│                                     │ PASS                                  │
│                                     ▼                                       │
│   ┌───────────────────────────────────────────────────────────────────┐    │
│   │ GATE 4: REVISIÓN HUMANA (según nivel de riesgo)                   │    │
│   │   • Bajo riesgo: Aprobación automática con notificación           │    │
│   │   • Medio riesgo: Aprobación de DBA                               │    │
│   │   • Alto riesgo: Aprobación de DBA senior + Tech Lead             │    │
│   │   • Crítico: Comité de cambios + ventana de mantenimiento         │    │
│   └─────────────────────────────────┬─────────────────────────────────┘    │
│                                     │ APPROVED                              │
│                                     ▼                                       │
│   ┌───────────────────────────────────────────────────────────────────┐    │
│   │ GATE 5: EJECUCIÓN CONTROLADA                                      │    │
│   │   • Backup automático pre-ejecución                               │    │
│   │   • Ejecución con monitoreo en tiempo real                        │    │
│   │   • Alertas configuradas para anomalías                           │    │
│   │   • Rollback automático si métricas exceden umbrales              │    │
│   └─────────────────────────────────┬─────────────────────────────────┘    │
│                                     │ SUCCESS                               │
│                                     ▼                                       │
│                            CAMBIO EN PRODUCCIÓN                             │
│                                                                             │
└─────────────────────────────────────────────────────────────────────────────┘
```

**Configuración de gates por clasificación de riesgo:**

```yaml
database_gates:
  clasificacion:
    bajo:
      descripcion: "Cambios que no afectan datos existentes"
      ejemplos:
        - Agregar columnas nullable
        - Crear índices en tablas pequeñas (<100K filas)
        - Agregar constraints CHECK nuevas
      gates_requeridos:
        - analisis_estatico: obligatorio
        - staging: obligatorio
        - revision_humana: notificacion_post_merge
      autonomia_ia: alta

    medio:
      descripcion: "Cambios que modifican estructura pero son reversibles"
      ejemplos:
        - Modificar tipos de datos (compatible)
        - Crear índices en tablas medianas (100K-10M filas)
        - Agregar foreign keys
        - Renombrar columnas
      gates_requeridos:
        - analisis_estatico: obligatorio
        - analisis_impacto: obligatorio
        - staging: obligatorio
        - revision_humana: aprobacion_dba
      autonomia_ia: media

    alto:
      descripcion: "Cambios que requieren migración de datos"
      ejemplos:
        - Modificar tipos de datos (incompatible)
        - Normalizar/desnormalizar tablas
        - Crear índices en tablas grandes (>10M filas)
        - Eliminar columnas con datos
      gates_requeridos:
        - analisis_estatico: obligatorio
        - analisis_impacto: obligatorio
        - staging: obligatorio_con_datos_produccion
        - revision_humana: aprobacion_dba_senior_y_tech_lead
        - ventana_mantenimiento: requerida
      autonomia_ia: baja

    critico:
      descripcion: "Cambios que afectan datos sensibles o son difícilmente reversibles"
      ejemplos:
        - DROP TABLE/DATABASE
        - DELETE/UPDATE masivo
        - Cambios en tablas de auditoría
        - Modificaciones a datos PII/PCI
      gates_requeridos:
        - analisis_estatico: obligatorio
        - analisis_impacto: obligatorio
        - staging: obligatorio_con_datos_produccion
        - revision_humana: comite_de_cambios
        - ventana_mantenimiento: requerida
        - comunicacion_stakeholders: obligatoria
        - plan_rollback: documentado_y_probado
      autonomia_ia: prohibida
```

#### Dimensión 4: Revisor de Migraciones Generadas

Cuando la IA genera scripts de migración, el DBA debe evaluarlos con un ojo crítico que va más allá de "¿funciona?".

**Checklist de Revisión de Migraciones IA-Generated:**

```markdown
## Checklist de Revisión de Migración

### 1. Correctitud Técnica
- [ ] Sintaxis válida para el motor de BD específico
- [ ] Tipos de datos apropiados y consistentes
- [ ] Constraints expresadas correctamente
- [ ] Índices necesarios incluidos
- [ ] No hay SQL injection posible en nombres dinámicos

### 2. Compatibilidad
- [ ] Compatible con versión actual del motor
- [ ] No usa features deprecadas
- [ ] Funciona con la configuración actual (collation, timezone, etc.)
- [ ] Queries existentes siguen funcionando

### 3. Rendimiento
- [ ] Operación no causa locks prolongados
- [ ] Índices no son redundantes
- [ ] No crea tabla temporal excesivamente grande
- [ ] Batch size apropiado para operaciones masivas

### 4. Reversibilidad
- [ ] Script de rollback incluido
- [ ] Rollback no pierde datos
- [ ] Rollback fue probado en staging
- [ ] Tiempo de rollback es aceptable

### 5. Integridad
- [ ] Constraints de FK no crean huérfanos
- [ ] Constraints CHECK no invalidan datos existentes
- [ ] NOT NULL tiene valores por defecto apropiados
- [ ] UNIQUE no viola datos existentes

### 6. Seguridad
- [ ] No expone datos sensibles en logs
- [ ] Permisos apropiados verificados
- [ ] No crea usuarios o grants innecesarios
- [ ] Audit trail preservado

### 7. Operacional
- [ ] Estimación de tiempo documentada
- [ ] Impacto en espacio de disco calculado
- [ ] Dependencias con otras migraciones claras
- [ ] Plan de comunicación definido
```

**Patrones comunes de error en migraciones IA-generated:**

```sql
-- ERROR 1: La IA olvida el contexto de datos existentes
-- IA genera:
ALTER TABLE users ADD COLUMN email VARCHAR(255) NOT NULL;
-- PROBLEMA: Falla si la tabla ya tiene filas

-- CORRECTO:
ALTER TABLE users ADD COLUMN email VARCHAR(255);
UPDATE users SET email = 'unknown@legacy.com' WHERE email IS NULL;
ALTER TABLE users ALTER COLUMN email SET NOT NULL;

---

-- ERROR 2: La IA no considera locks en tablas grandes
-- IA genera:
CREATE INDEX idx_orders_customer ON orders(customer_id);
-- PROBLEMA: En tabla de 50M filas, esto puede lockear por horas

-- CORRECTO (PostgreSQL):
CREATE INDEX CONCURRENTLY idx_orders_customer ON orders(customer_id);
-- O en MySQL, usar pt-online-schema-change

---

-- ERROR 3: La IA no preserva constraints existentes
-- IA genera:
ALTER TABLE products DROP COLUMN category_id;
ALTER TABLE products ADD COLUMN category_id INTEGER REFERENCES categories(id);
-- PROBLEMA: Pierde todos los valores existentes

-- CORRECTO:
ALTER TABLE products
  DROP CONSTRAINT fk_products_category,
  ALTER COLUMN category_id SET DATA TYPE INTEGER,
  ADD CONSTRAINT fk_products_category
    FOREIGN KEY (category_id) REFERENCES categories(id);

---

-- ERROR 4: La IA genera rollback que pierde datos
-- Migración:
ALTER TABLE orders ADD COLUMN shipping_notes TEXT;
-- Rollback generado por IA:
ALTER TABLE orders DROP COLUMN shipping_notes;
-- PROBLEMA: Si ya se guardaron datos, se pierden

-- CORRECTO: El rollback debe considerar datos existentes
-- Paso 1: Crear backup de datos
CREATE TABLE _backup_orders_shipping_notes AS
  SELECT id, shipping_notes FROM orders WHERE shipping_notes IS NOT NULL;
-- Paso 2: Entonces sí eliminar
ALTER TABLE orders DROP COLUMN shipping_notes;
-- Documentar: restauración requiere merge manual con backup
```

#### Dimensión 5: Estratega de Rendimiento Predictivo

El DBA IA-Native no solo reacciona a problemas de rendimiento; diseña sistemas de **monitoreo predictivo** que detectan degradación antes de que se vuelva crítica.

**Framework de Monitoreo Predictivo:**

```yaml
performance_monitoring:
  metricas_base:
    - nombre: query_response_time_p95
      umbral_alerta: >200ms
      umbral_critico: >500ms
      ventana: 5min

    - nombre: slow_queries_per_minute
      umbral_alerta: >10
      umbral_critico: >50
      ventana: 1min

    - nombre: index_hit_ratio
      umbral_alerta: <95%
      umbral_critico: <90%
      ventana: 15min

    - nombre: connection_pool_usage
      umbral_alerta: >80%
      umbral_critico: >95%
      ventana: 1min

    - nombre: replication_lag
      umbral_alerta: >1s
      umbral_critico: >5s
      ventana: instant

  analisis_tendencias:
    - metrica: table_size_growth
      frecuencia: diaria
      alerta_si: growth_rate > 10% semanal sin explicación

    - metrica: index_bloat
      frecuencia: semanal
      alerta_si: bloat > 20%

    - metrica: query_plan_changes
      frecuencia: continua
      alerta_si: plan_change AND regression > 2x

  queries_criticos:
    - identificador: checkout_flow
      query_patterns:
        - SELECT * FROM cart WHERE user_id = ?
        - SELECT * FROM products WHERE id IN (?)
        - INSERT INTO orders (...)
      sla: p99 < 100ms
      impacto_negocio: "Cada 100ms adicional = 1% menos conversión"

    - identificador: report_generation
      query_patterns:
        - SELECT ... FROM orders JOIN products ... GROUP BY ...
      sla: p95 < 30s
      ventana_ejecucion: "Solo entre 00:00-06:00 UTC"
```

**Respuesta a degradación detectada:**

```
┌─────────────────────────────────────────────────────────────────────────────┐
│              FLUJO DE RESPUESTA A DEGRADACIÓN DE RENDIMIENTO                │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                             │
│   ALERTA DETECTADA                                                          │
│         │                                                                   │
│         ▼                                                                   │
│   ┌─────────────────────────────────────────────────────────────────┐      │
│   │ 1. DIAGNÓSTICO AUTOMÁTICO                                       │      │
│   │    • Identificar queries más lentos                             │      │
│   │    • Analizar planes de ejecución                               │      │
│   │    • Verificar estadísticas de tablas                           │      │
│   │    • Revisar cambios recientes en esquema                       │      │
│   └─────────────────────────────────┬───────────────────────────────┘      │
│                                     │                                       │
│                                     ▼                                       │
│   ┌─────────────────────────────────────────────────────────────────┐      │
│   │ 2. IA PROPONE SOLUCIONES                                        │      │
│   │    • Índices sugeridos                                          │      │
│   │    • Reescritura de queries                                     │      │
│   │    • Ajustes de configuración                                   │      │
│   │    • Particionamiento recomendado                               │      │
│   └─────────────────────────────────┬───────────────────────────────┘      │
│                                     │                                       │
│                                     ▼                                       │
│   ┌─────────────────────────────────────────────────────────────────┐      │
│   │ 3. DBA EVALÚA Y DECIDE                                          │      │
│   │    • ¿La solución aborda la causa raíz?                         │      │
│   │    • ¿Hay efectos secundarios no considerados?                  │      │
│   │    • ¿Es la solución apropiada para producción?                 │      │
│   │    • ¿Requiere ventana de mantenimiento?                        │      │
│   └─────────────────────────────────┬───────────────────────────────┘      │
│                                     │                                       │
│                   ┌─────────────────┴─────────────────┐                     │
│                   ▼                                   ▼                     │
│   ┌───────────────────────────┐       ┌───────────────────────────┐        │
│   │ IMPLEMENTACIÓN INMEDIATA  │       │ PLANIFICACIÓN ESTRUCTURADA│        │
│   │ (si riesgo bajo)          │       │ (si riesgo alto)          │        │
│   │ • Agregar índice          │       │ • Policy Ticket           │        │
│   │ • Ajustar parámetro       │       │ • Testing en staging      │        │
│   │ • Terminar query abusivo  │       │ • Ventana mantenimiento   │        │
│   └───────────────────────────┘       └───────────────────────────┘        │
│                                                                             │
└─────────────────────────────────────────────────────────────────────────────┘
```

#### Dimensión 6: Protector de Continuidad Operativa

El DBA es el último responsable de que los datos estén disponibles, seguros y recuperables. Esta responsabilidad no se delega a la IA.

**Framework de Continuidad de Datos:**

```yaml
continuidad_operativa:
  backup_strategy:
    full_backup:
      frecuencia: diario
      retencion: 30 días
      verificacion: restauración mensual a ambiente de prueba
      ubicacion:
        - primario: mismo datacenter
        - secundario: datacenter remoto
        - terciario: cloud (encriptado)

    incremental_backup:
      frecuencia: cada 15 minutos
      retencion: 7 días
      verificacion: restauración semanal aleatoria

    point_in_time_recovery:
      habilitado: true
      retencion_logs: 7 días
      rpo_objetivo: 15 minutos

  disaster_recovery:
    rto_objetivo: 4 horas
    rpo_objetivo: 15 minutos

    escenarios_probados:
      - nombre: "Falla de servidor primario"
        frecuencia_prueba: mensual
        procedimiento: failover_automatico_replica.md

      - nombre: "Corrupción de datos por bug de aplicación"
        frecuencia_prueba: trimestral
        procedimiento: point_in_time_recovery.md

      - nombre: "Pérdida de datacenter completo"
        frecuencia_prueba: anual
        procedimiento: dr_datacenter_remoto.md

      - nombre: "Ransomware / ataque malicioso"
        frecuencia_prueba: semestral
        procedimiento: restauracion_desde_backup_air_gapped.md

  alta_disponibilidad:
    replicacion:
      tipo: sincrona
      replicas: 2
      failover: automatico_con_verificacion

    conexiones:
      pool_primario: 100
      pool_replica: 50
      timeout: 30s
      retry_policy: exponential_backoff

  monitoreo_continuidad:
    alertas_criticas:
      - backup_fallido
      - replicacion_detenida
      - lag_replica > 5min
      - espacio_disco < 20%
      - conexiones_agotadas

    reportes:
      - backup_status: diario
      - dr_readiness: semanal
      - capacity_planning: mensual
```

### El Primer Día como DBA IA-Native: Una Guía Práctica

Si eres un DBA que está adoptando el enfoque IA-Native por primera vez, aquí está una guía estructurada para tu primer día:

```yaml
dia_uno_dba_ia_native:

  hora_1_preparacion:
    objetivo: "Entender el estado actual de gobernanza de datos"
    acciones:
      - Revisar documentación de esquemas existente
      - Identificar qué cambios de BD fueron hechos con IA recientemente
      - Listar las tablas más críticas para el negocio
      - Mapear dependencias entre sistemas que comparten datos
    entregable: "Mapa de criticidad de datos"

  hora_2_inventario_riesgos:
    objetivo: "Clasificar tablas y operaciones por nivel de riesgo"
    acciones:
      - categorizar_tablas:
          critico: "datos financieros, PII, auditoría"
          alto: "datos de negocio core, configuración"
          medio: "datos operativos, logs estructurados"
          bajo: "cache, datos temporales, staging"
      - identificar_operaciones_peligrosas:
          - "Migraciones sin rollback probado"
          - "Scripts de corrección de datos ad-hoc"
          - "Cambios de permisos no documentados"
    entregable: "Matriz de clasificación de riesgo de datos"

  hora_3_establecer_gates_minimos:
    objetivo: "Implementar controles básicos inmediatos"
    acciones:
      - Crear pre-commit hook para validar SQL básico
      - Configurar alerta para cualquier DROP/TRUNCATE
      - Establecer que toda migración requiera script de rollback
      - Documentar proceso de aprobación por nivel de riesgo
    entregable: "Pipeline básico de validación de cambios de BD"

  hora_4_primer_policy_ticket:
    objetivo: "Crear template y primer Policy Ticket real"
    acciones:
      - Adaptar template de Policy Ticket para contexto de datos
      - Identificar un cambio de BD pendiente
      - Documentarlo como Policy Ticket completo
      - Ejecutar el ciclo completo con el equipo
    entregable: "Policy Ticket de ejemplo funcionando"
```

### Ciclo de Trabajo del DBA IA-Native

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                     CICLO DIARIO DEL DBA IA-NATIVE                          │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                             │
│   MAÑANA: MONITOREO Y PLANIFICACIÓN                                         │
│   ┌─────────────────────────────────────────────────────────────────┐      │
│   │ • Revisar alertas nocturnas                                     │      │
│   │ • Verificar estado de backups                                   │      │
│   │ • Analizar métricas de rendimiento                              │      │
│   │ • Revisar cola de migraciones pendientes                        │      │
│   │ • Priorizar Policy Tickets de cambios de BD                     │      │
│   └─────────────────────────────────────────────────────────────────┘      │
│                              │                                              │
│                              ▼                                              │
│   MEDIA MAÑANA: REVISIÓN DE CAMBIOS                                         │
│   ┌─────────────────────────────────────────────────────────────────┐      │
│   │ • Revisar migraciones generadas por IA                          │      │
│   │ • Evaluar propuestas de índices                                 │      │
│   │ • Validar scripts de corrección de datos                        │      │
│   │ • Aprobar/rechazar cambios según clasificación                  │      │
│   └─────────────────────────────────────────────────────────────────┘      │
│                              │                                              │
│                              ▼                                              │
│   TARDE: EJECUCIÓN Y SOPORTE                                                │
│   ┌─────────────────────────────────────────────────────────────────┐      │
│   │ • Supervisar ejecución de migraciones aprobadas                 │      │
│   │ • Atender consultas de equipos de desarrollo                    │      │
│   │ • Investigar problemas de rendimiento reportados                │      │
│   │ • Documentar decisiones y lecciones aprendidas                  │      │
│   └─────────────────────────────────────────────────────────────────┘      │
│                              │                                              │
│                              ▼                                              │
│   FIN DE DÍA: PREPARACIÓN Y MEJORA                                          │
│   ┌─────────────────────────────────────────────────────────────────┐      │
│   │ • Programar migraciones para ventanas de mantenimiento          │      │
│   │ • Actualizar documentación de esquemas                          │      │
│   │ • Refinar reglas de gates basándose en experiencia del día      │      │
│   │ • Preparar comunicaciones para cambios próximos                 │      │
│   └─────────────────────────────────────────────────────────────────┘      │
│                                                                             │
└─────────────────────────────────────────────────────────────────────────────┘
```

### Ejemplo Práctico 1: Revisión de Migración IA-Generated

**Escenario**: Un desarrollador usó IA para generar una migración que agrega soft-delete a la tabla de productos.

**Migración generada por IA:**

```sql
-- Migration: Add soft delete to products
-- Generated by: AI Assistant

ALTER TABLE products ADD COLUMN deleted_at TIMESTAMP NULL;
ALTER TABLE products ADD COLUMN deleted_by INTEGER NULL;

CREATE INDEX idx_products_deleted ON products(deleted_at);

-- Update all queries to filter by deleted_at IS NULL
```

**Análisis del DBA:**

```yaml
revision_migracion:
  id: MIG-2026-089
  generada_por: ia_asistente
  fecha_revision: 2026-01-24
  revisor: dba-senior

  evaluacion_tecnica:
    sintaxis: ✓ Correcta
    compatibilidad: ✓ Compatible con PostgreSQL 15
    rendimiento:
      estado: ⚠️ ADVERTENCIA
      detalle: |
        El índice en deleted_at es parcialmente útil.
        La mayoría de queries filtrarán deleted_at IS NULL,
        pero un índice regular no es óptimo para esto.

  problemas_detectados:
    - id: 1
      severidad: media
      descripcion: "Índice subóptimo para el caso de uso"
      solucion: |
        Usar índice parcial:
        CREATE INDEX idx_products_active ON products(id) WHERE deleted_at IS NULL;
        Esto es más eficiente para el 99% de queries.

    - id: 2
      severidad: alta
      descripcion: "No hay FK para deleted_by"
      solucion: |
        ALTER TABLE products ADD CONSTRAINT fk_products_deleted_by
        FOREIGN KEY (deleted_by) REFERENCES users(id);

    - id: 3
      severidad: alta
      descripcion: "No considera cascada en tablas relacionadas"
      detalle: |
        Las tablas order_items, cart_items, product_reviews
        referencian products. ¿Qué pasa cuando un producto
        está "soft-deleted" pero aparece en pedidos históricos?
      solucion: |
        Documentar política: productos soft-deleted siguen
        visibles en contexto histórico (pedidos pasados).
        Agregar vista products_active para nuevas operaciones.

    - id: 4
      severidad: media
      descripcion: "Migración incompleta - falta rollback"
      solucion: |
        Agregar script de rollback:
        ALTER TABLE products DROP COLUMN deleted_at;
        ALTER TABLE products DROP COLUMN deleted_by;
        DROP INDEX idx_products_active;

    - id: 5
      severidad: alta
      descripcion: "El comentario 'Update all queries' no es ejecutable"
      detalle: |
        La IA sugiere actualizar queries pero no lo hace.
        Esto dejará queries que retornan productos eliminados.
      solucion: |
        Crear vista de compatibilidad:
        CREATE VIEW products_v AS
        SELECT * FROM products WHERE deleted_at IS NULL;

        Migrar gradualmente los queries al nuevo esquema.

  decision: RECHAZADA_CON_MODIFICACIONES

  migracion_corregida: |
    -- Migration: Add soft delete to products
    -- Original by: AI Assistant
    -- Reviewed by: DBA Senior
    -- PT: PT-DBA-2026-089

    -- 1. Agregar columnas
    ALTER TABLE products ADD COLUMN deleted_at TIMESTAMP NULL;
    ALTER TABLE products ADD COLUMN deleted_by INTEGER REFERENCES users(id);

    -- 2. Índice parcial optimizado para el caso de uso común
    CREATE INDEX idx_products_active ON products(id) WHERE deleted_at IS NULL;

    -- 3. Índice para queries de auditoría (productos eliminados)
    CREATE INDEX idx_products_deleted ON products(deleted_at)
      WHERE deleted_at IS NOT NULL;

    -- 4. Vista de compatibilidad para transición gradual
    CREATE VIEW products_active AS
    SELECT * FROM products WHERE deleted_at IS NULL;

    -- 5. Documentar en tabla de metadatos
    INSERT INTO schema_documentation (table_name, column_name, description, added_date)
    VALUES
      ('products', 'deleted_at', 'Soft delete timestamp. NULL = active product.', NOW()),
      ('products', 'deleted_by', 'User who deleted. FK to users.id.', NOW());

    -- ROLLBACK SCRIPT (guardar separado):
    -- DROP VIEW products_active;
    -- DROP INDEX idx_products_deleted;
    -- DROP INDEX idx_products_active;
    -- ALTER TABLE products DROP COLUMN deleted_by;
    -- ALTER TABLE products DROP COLUMN deleted_at;
    -- DELETE FROM schema_documentation WHERE table_name = 'products'
    --   AND column_name IN ('deleted_at', 'deleted_by');

  comunicacion_equipo: |
    La migración de soft-delete para productos está lista.
    Cambios vs. propuesta original:
    - Índices optimizados para el patrón de uso real
    - FK agregada para auditoría de quién eliminó
    - Vista products_active para transición gradual

    ACCIÓN REQUERIDA:
    - Nuevos queries deben usar products_active
    - Queries existentes se migrarán gradualmente
    - Reportes históricos usan products directamente
```

### Ejemplo Práctico 2: Investigación de Degradación de Rendimiento

**Escenario**: El sistema de monitoreo detecta que los queries del checkout han aumentado su p95 de 80ms a 350ms.

```yaml
investigacion_rendimiento:
  id: PERF-2026-017
  fecha: 2026-01-24
  reportado_por: sistema_monitoreo

  sintomas:
    - metrica: checkout_queries_p95
      antes: 80ms
      ahora: 350ms
      inicio_degradacion: "2026-01-23 14:30 UTC"

  investigacion_inicial:
    queries_afectados:
      - query: "SELECT * FROM cart_items ci JOIN products p ON ci.product_id = p.id WHERE ci.cart_id = ?"
        plan_antes: "Index Scan using idx_cart_items_cart_id"
        plan_ahora: "Seq Scan on cart_items"
        tiempo_antes: 2ms
        tiempo_ahora: 180ms

    correlacion_temporal:
      - "14:25 - Despliegue de migración MIG-2026-088"
      - "14:30 - Inicio de degradación"

    migracion_sospechosa:
      id: MIG-2026-088
      contenido: |
        ALTER TABLE cart_items ADD COLUMN discount_code VARCHAR(50);
        ALTER TABLE cart_items ADD COLUMN discount_amount DECIMAL(10,2);
        REINDEX TABLE cart_items;

  diagnostico:
    causa_raiz: |
      El REINDEX TABLE cart_items invalidó las estadísticas del
      optimizador. Aunque el índice existe, el planner está eligiendo
      Seq Scan porque las estadísticas están desactualizadas.

    contribucion_ia: |
      La migración fue generada por IA. El REINDEX fue agregado
      "por precaución" sin entender que esto requiere un ANALYZE
      posterior para actualizar estadísticas.

  solucion_inmediata:
    accion: "ANALYZE cart_items;"
    tiempo_ejecucion: "3 segundos"
    resultado: "p95 retornó a 85ms"

  acciones_correctivas:
    - tipo: gate_nuevo
      descripcion: |
        Agregar validación al pipeline: cualquier REINDEX debe ir
        seguido de ANALYZE en la misma migración.

    - tipo: documentacion
      descripcion: |
        Agregar al checklist de revisión de migraciones:
        "Si hay REINDEX, verificar que incluye ANALYZE"

    - tipo: capacitacion
      descripcion: |
        Compartir este caso en la retrospectiva como ejemplo de
        por qué las migraciones IA-generated requieren revisión experta.

  leccion_aprendida: |
    La IA tiene conocimiento de sintaxis SQL y mejores prácticas
    generales, pero no entiende las implicaciones operativas de
    ciertos comandos en contexto de producción. REINDEX es técnicamente
    correcto pero operativamente peligroso sin ANALYZE posterior.

    El DBA debe verificar no solo "¿funciona?" sino "¿qué efectos
    secundarios tiene en el comportamiento del optimizador?"
```

### Anti-patrones del DBA en el Contexto IA-Native

#### Anti-patrón 1: "La IA Sabe de Bases de Datos"

```
SÍNTOMA:
DBA confía ciegamente en migraciones generadas por IA porque
"la sintaxis es correcta y está basada en mejores prácticas".

REALIDAD:
La IA no conoce:
- El volumen real de datos en las tablas
- Los patrones de acceso específicos de tu aplicación
- Las ventanas de mantenimiento disponibles
- Las dependencias con otros sistemas
- El contexto histórico de decisiones de esquema

SOLUCIÓN:
Toda migración IA-generated debe pasar por:
1. Análisis de impacto con datos reales de producción
2. Prueba en staging con volumen similar a producción
3. Revisión de un DBA que conozca el contexto
```

#### Anti-patrón 2: "Automatización Total de Migraciones"

```
SÍNTOMA:
Configurar pipeline para que migraciones pasen automáticamente
si los tests de integración pasan.

REALIDAD:
Los tests de integración corren contra bases de datos de prueba
con datos mínimos. No detectan:
- Locks que tardan minutos en tablas grandes
- Degradación gradual de rendimiento
- Conflictos con queries de reporting nocturno
- Problemas de espacio en disco

SOLUCIÓN:
Mantener aprobación humana obligatoria para migraciones
que afectan tablas de más de N filas o que modifican
columnas existentes. Automatizar solo lo de bajo riesgo.
```

#### Anti-patrón 3: "Rollback Automático Resuelve Todo"

```
SÍNTOMA:
Confiar en que el rollback automático de la migración
revertirá cualquier problema.

REALIDAD:
Muchas migraciones no son reversibles sin pérdida de datos:
- DROP COLUMN no se puede revertir
- Cambios de tipo de datos pueden perder precisión
- Datos insertados entre migración y rollback se pierden
- Algunas operaciones no tienen rollback (TRUNCATE con FK)

SOLUCIÓN:
Clasificar migraciones por reversibilidad:
- Reversible sin pérdida: índices, columnas nuevas nullable
- Reversible con cuidado: constraints, FK
- Irreversible: DROP, TRUNCATE, cambios de tipo
Para irreversibles, backup obligatorio + ventana extendida.
```

#### Anti-patrón 4: "El DBA Solo Interviene en Emergencias"

```
SÍNTOMA:
DBA deja que equipos manejen sus migraciones autónomamente
y solo aparece cuando hay incidentes de producción.

REALIDAD:
Los problemas de datos son como deuda técnica: se acumulan
silenciosamente. Cuando el DBA interviene en emergencia,
ya es tarde. Los esquemas están inconsistentes, hay datos
huérfanos, índices redundantes, y queries subóptimos
que nadie sabe por qué existen.

SOLUCIÓN:
Establecer revisión proactiva:
- Review semanal de migraciones ejecutadas
- Análisis mensual de rendimiento de queries
- Auditoría trimestral de integridad de datos
- Participación en planning para anticipar cambios de esquema
```

#### Anti-patrón 5: "Staging es Suficiente"

```
SÍNTOMA:
"Funcionó en staging, debe funcionar en producción".

REALIDAD:
Staging típicamente tiene:
- 1% del volumen de datos de producción
- Datos sintéticos que no representan casos edge
- Ninguna carga concurrente real
- Configuración de recursos diferente

SOLUCIÓN:
Para migraciones de riesgo medio/alto:
- Usar réplica de producción para testing
- Anonimizar datos pero mantener volumen
- Simular carga concurrente durante migración
- Ejecutar en horario de bajo tráfico primero
```

### Métricas del DBA IA-Native

```yaml
metricas_dba:
  eficiencia:
    - nombre: "Migraciones revisadas / día"
      objetivo: "Mantener ritmo con generación IA"
      alerta_si: backlog > 5 migraciones

    - nombre: "Tiempo promedio de revisión"
      objetivo: "< 30 min para bajo riesgo, < 2h para alto riesgo"
      contexto: "No sacrificar calidad por velocidad"

  calidad:
    - nombre: "Migraciones rechazadas / total"
      objetivo: "10-20%"
      interpretacion: |
        < 10%: ¿Revisión demasiado laxa?
        > 30%: ¿IA mal configurada o equipos no siguen guidelines?

    - nombre: "Rollbacks de emergencia / mes"
      objetivo: "0"
      max_aceptable: 1
      accion_si_excede: "Revisar gates y criterios de aprobación"

    - nombre: "Incidentes de integridad de datos / trimestre"
      objetivo: "0"
      severidad: critica

  rendimiento:
    - nombre: "Queries en SLA"
      objetivo: "> 99%"
      desglose_por: "criticidad de query"

    - nombre: "Alertas de rendimiento / semana"
      objetivo: "< 5"
      contexto: "Alertas reales, no ruido"

  continuidad:
    - nombre: "Backups exitosos"
      objetivo: "100%"
      alerta: cualquier_fallo

    - nombre: "RPO real vs. objetivo"
      objetivo: "< 15 min"
      medicion: "Prueba mensual de restauración"

    - nombre: "DR drills completados / año"
      objetivo: "4"
      tipo: "1 por escenario definido"

  gobernanza:
    - nombre: "Cobertura de Policy Tickets"
      objetivo: "100% de cambios alto/crítico riesgo"
      definicion: "Cambios con PT asociado / total cambios alto riesgo"

    - nombre: "Tiempo de resolución de Policy Ticket"
      objetivo: "< 48h para bajo riesgo, < 1 semana para alto"
```

### Checklist de Adopción para DBAs

```yaml
checklist_adopcion_dba:
  semana_1:
    - [ ] Mapear todas las bases de datos bajo mi responsabilidad
    - [ ] Clasificar tablas por nivel de riesgo (crítico/alto/medio/bajo)
    - [ ] Identificar operaciones que la IA está realizando actualmente
    - [ ] Documentar gaps entre prácticas actuales y gobernanza deseada
    - [ ] Establecer canal de comunicación con equipos de desarrollo

  semana_2:
    - [ ] Crear template de Policy Ticket para cambios de BD
    - [ ] Configurar gate básico (validación de sintaxis, detección de DROP)
    - [ ] Establecer proceso de revisión para migraciones
    - [ ] Documentar checklist de revisión de migraciones
    - [ ] Primer Policy Ticket completado end-to-end

  mes_1:
    - [ ] Gates para los 4 niveles de riesgo configurados
    - [ ] Staging con datos similares a producción disponible
    - [ ] Proceso de revisión funcionando para todas las migraciones
    - [ ] Métricas básicas de rendimiento monitoreadas
    - [ ] Backup strategy documentada y verificada

  trimestre_1:
    - [ ] >80% de cambios alto/crítico riesgo tienen Policy Ticket
    - [ ] Cero incidentes de integridad de datos
    - [ ] DR drill completado exitosamente
    - [ ] Equipo entiende y sigue el proceso de gobernanza
    - [ ] Refinamiento de gates basado en experiencia real

  estado_objetivo:
    - [ ] 100% de cambios de esquema gobernados
    - [ ] Migraciones IA-generated son la norma pero siempre revisadas
    - [ ] Incidentes de datos son excepcionales, no rutinarios
    - [ ] Equipo ve al DBA como enabler, no como obstáculo
    - [ ] Proceso de datos es auditable y trazable
```

### La Paradoja del DBA IA-Native

El DBA enfrenta una paradoja similar a la de otros roles: la IA puede hacer mucho del trabajo técnico que antes lo definía, pero esto no lo vuelve prescindible sino más crítico.

**La paradoja se resuelve así:**

Antes: El DBA era valioso porque sabía cosas que otros no sabían (sintaxis SQL avanzada, internals del motor, trucos de optimización).

Ahora: El DBA es valioso porque entiende cosas que la IA no puede entender:
- El contexto de negocio de los datos
- El historial de decisiones de diseño
- Las implicaciones de largo plazo de cambios de esquema
- La diferencia entre "funciona" y "es seguro en producción"
- Las dependencias no documentadas entre sistemas

### Conclusión: El Nuevo Contrato del DBA

El DBA IA-Native firma un nuevo contrato con su profesión:

> **"Acepto que mi valor ya no está en escribir SQL más rápido que otros, sino en ser el guardián de la integridad, disponibilidad y seguridad de los datos de la organización. Me comprometo a diseñar sistemas de verificación que aseguren que cualquier cambio —sea generado por humano o por IA— sea seguro, reversible y auditable. Entiendo que los datos son el activo más crítico y que cada migración aprobada lleva mi firma profesional. Uso la IA como herramienta pero nunca delego la responsabilidad de proteger los datos."**

El DBA de 2026 es, en esencia, un **arquitecto de confiabilidad de datos**: alguien que diseña no solo esquemas técnicos, sino los sistemas de control que aseguran que los datos mantengan su integridad, disponibilidad y valor a través del tiempo, sin importar quién o qué los modifique.

---

# PARTE V: ADOPCIÓN Y VALIDACIÓN

## Caso de Uso: Adopción en una Software Factory Real

Para comprender cómo este marco de trabajo se traduce en la práctica, resulta útil situarlo en un contexto organizacional concreto. No en un laboratorio ideal ni en un equipo experimental, sino en una software factory real, con múltiples proyectos, presión comercial, plazos ajustados y una base de código heterogénea.

Imaginemos una software factory de tamaño mediano, dedicada al desarrollo de productos digitales para terceros. Cuenta con varios equipos que trabajan en paralelo, utilizan metodologías ágiles tradicionales y han comenzado a incorporar herramientas de inteligencia artificial para acelerar el desarrollo. Al inicio, el uso de la IA se da de manera informal: algunos desarrolladores la utilizan para generar código, otros para refactorizar módulos, otros para escribir tests o documentación.

En una primera etapa, los resultados parecen positivos. La velocidad de entrega aumenta y ciertas tareas repetitivas se resuelven con mayor rapidez. Sin embargo, al cabo de algunas iteraciones comienzan a aparecer problemas. El código se vuelve más difícil de entender, los refactors no siempre respetan las decisiones arquitectónicas previas y los equipos pierden claridad sobre por qué se hicieron ciertos cambios. Cuando surge un defecto importante, rastrear su origen se vuelve complejo, ya que muchas decisiones no quedaron registradas explícitamente.

Frente a este escenario, la organización decide introducir el marco IA-Native de manera gradual. El primer paso no consiste en prohibir el uso de la IA, sino en hacerlo explícito y gobernable. Se identifica un conjunto de dominios de bajo riesgo, por ejemplo, refactors internos, generación de tests o mejoras de legibilidad, y se establece que cualquier intervención de la IA en esos dominios debe estar respaldada por un Policy Ticket.

En esta fase inicial, los Policy Tickets son simples. Definen claramente la intención de la intervención, delimitan el alcance permitido y exigen evidencia mínima, como diffs de código y resultados de tests. La autonomía de la IA es limitada: genera propuestas que deben ser revisadas antes de integrarse. El objetivo no es maximizar la velocidad, sino aprender a gobernar el proceso.

Con el tiempo, la organización gana confianza en el marco. Se refinan las políticas, se ajustan los niveles de autonomía y se incorporan mecanismos automáticos de verificación. En algunos dominios, como la generación de tests de regresión o la actualización de documentación, la IA comienza a operar con mayor independencia, siempre bajo controles claros. En otros, como la lógica de negocio o la seguridad, la intervención sigue estando fuertemente restringida.

Un aspecto clave de esta adopción es la visibilidad. Cada intervención de la IA queda asociada a un Policy Ticket, a un responsable humano y a un conjunto de evidencias. Esto permite reconstruir decisiones, analizar patrones de uso y detectar áreas de riesgo antes de que se conviertan en problemas críticos. La organización deja de depender de la memoria informal de los equipos y pasa a contar con un registro explícito de la delegación cognitiva.

Con el paso de los sprints, se observa un cambio cultural. Los equipos dejan de ver a la IA como una "caja negra" que produce resultados rápidos y comienzan a tratarla como un actor que debe ser gobernado. Las discusiones técnicas ya no giran solo en torno a si un cambio funciona, sino a si fue correctamente autorizado, si respeta las políticas y si produce la evidencia esperada.

Este caso ilustra un punto central del marco: la adopción exitosa de la IA no depende únicamente de la tecnología utilizada, sino de la capacidad organizacional para establecer límites, asumir responsabilidades y aprender del uso real de los agentes. El marco IA-Native proporciona una estructura para ese aprendizaje, permitiendo a las software factories escalar el uso de la inteligencia artificial sin perder control ni calidad.

---

## Validación y Evaluación del Marco en la Práctica

Un marco de trabajo orientado a la adopción de inteligencia artificial en el desarrollo de software no puede sostenerse únicamente sobre argumentos conceptuales o intuiciones técnicas. Para que sea útil en contextos reales, debe ofrecer criterios claros que permitan evaluar su efectividad, identificar sus límites y detectar tempranamente posibles desvíos. Esta sección aborda precisamente esa dimensión: cómo se valida el marco IA-Native en la práctica cotidiana de una organización.

La validación del marco no se concibe como un experimento aislado ni como una certificación puntual, sino como un proceso continuo. En entornos dinámicos, donde tanto la tecnología como las prácticas evolucionan rápidamente, la evaluación debe integrarse al propio funcionamiento del sistema. El marco propone, por lo tanto, una validación empírica basada en métricas observables, comparaciones antes y después de su adopción y análisis sistemático de incidentes.

Un primer eje de evaluación es la trazabilidad de las decisiones. En un entorno IA-Native gobernado, debería ser posible responder con claridad preguntas como: qué cambios fueron realizados por la IA, bajo qué autorización, con qué nivel de autonomía y quién asumió la responsabilidad humana. La mejora en trazabilidad no se mide solo por la existencia de registros, sino por su utilidad real para reconstruir decisiones cuando surge un problema o se requiere una auditoría.

El segundo eje es la calidad técnica del producto. Esto incluye métricas tradicionales, como cobertura de tests, defectos en producción o estabilidad del sistema, pero interpretadas a la luz del nuevo paradigma. No se trata únicamente de si el sistema funciona, sino de si los cambios introducidos por la IA respetan las decisiones arquitectónicas, mantienen coherencia interna y no introducen deuda técnica oculta. La comparación entre períodos pre y post adopción del marco resulta especialmente reveladora en este punto.

Un tercer eje clave es la gestión del riesgo. El marco se valida en la medida en que permite identificar, clasificar y mitigar riesgos asociados al uso de IA antes de que se materialicen en incidentes graves. Esto implica observar si los dominios de alto riesgo están efectivamente protegidos por políticas más estrictas, si los niveles de autonomía son coherentes con la criticidad del contexto y si las excepciones son tratadas de manera explícita y no informal.

La capacidad organizacional de aprendizaje constituye otro criterio central. Un marco eficaz no elimina el error, pero permite aprender de él. En la práctica, esto se traduce en la posibilidad de analizar patrones de fallo de la IA, ajustar políticas, mejorar mecanismos de verificación y refinar los límites de autonomía. La existencia de retroalimentación sistemática y ajustes iterativos es un indicador de madurez del modelo.

Es importante señalar que la validación del marco también incluye el reconocimiento de sus límites. Existen contextos en los que la introducción de IA con alta autonomía puede no ser apropiada, ya sea por restricciones regulatorias, por la naturaleza del dominio o por la falta de capacidades organizacionales. Un uso responsable del marco implica saber dónde no aplicarlo o hacerlo con extrema cautela.

En conjunto, la evaluación del marco IA-Native se apoya en una combinación de indicadores técnicos, organizacionales y culturales. Su éxito no se mide únicamente por una mayor velocidad de entrega, sino por la capacidad de la organización para aprovechar la potencia de la IA manteniendo control, coherencia y responsabilidad. Esta validación continua es lo que permite que el marco evolucione junto con la tecnología y las prácticas de desarrollo.

---

## Discusión Crítica: Riesgos, Límites y Trade-offs

Todo marco de trabajo que aspire a orientar prácticas reales debe ser analizado no solo desde sus beneficios, sino también desde sus riesgos y limitaciones. El enfoque IA-Native no es una excepción. Esta sección propone una discusión crítica del marco, con el objetivo de identificar los principales trade-offs que introduce y las condiciones bajo las cuales su adopción puede resultar problemática.

Un primer riesgo evidente es la burocratización excesiva. La introducción de Policy Tickets, niveles de autonomía y requisitos de evidencia puede ser percibida como una carga adicional, especialmente en equipos acostumbrados a procesos livianos. Si el marco se aplica de manera rígida o indiscriminada, existe el peligro de frenar la innovación y generar resistencia interna. Este riesgo se mitiga cuando el marco se adopta de forma progresiva y proporcional al riesgo del dominio.

Otro trade-off importante se relaciona con la velocidad de entrega. En ciertos contextos, el uso no gobernado de la IA puede producir resultados más rápidos en el corto plazo. El marco IA-Native introduce fricción deliberada, en forma de controles y verificaciones, que puede reducir la velocidad inicial. Sin embargo, esta desaceleración debe evaluarse en relación con los costos ocultos de una adopción descontrolada, como la acumulación de deuda técnica, la pérdida de trazabilidad y el aumento del riesgo operativo.

La dependencia excesiva de la IA constituye otro punto crítico. Existe el riesgo de que los equipos deleguen demasiado, confiando en que los agentes resolverán problemas complejos sin una comprensión adecuada del dominio. Esto puede llevar a una erosión del conocimiento humano y a una menor capacidad de respuesta ante situaciones imprevistas. El marco enfatiza la necesidad de mantener a las personas en roles de decisión y supervisión, pero su efectividad depende de una cultura organizacional que valore esta responsabilidad.

También deben considerarse los límites técnicos y contextuales. No todos los sistemas ni todos los dominios son igualmente adecuados para una alta autonomía de la IA. Sectores regulados, sistemas críticos o entornos con requisitos de cumplimiento estrictos pueden requerir niveles de control que reduzcan significativamente el beneficio de la automatización. En estos casos, el marco no promete eficiencia máxima, sino seguridad y gobernabilidad.

Un aspecto central de la discusión crítica es la falsabilidad del marco. El enfoque IA-Native debe considerarse refutado si, en la práctica, no logra mejorar la trazabilidad, no reduce incidentes críticos o introduce una carga organizacional desproporcionada sin beneficios claros. Reconocer estas condiciones de fallo es parte de la madurez del modelo y evita su adopción dogmática.

Finalmente, es importante destacar que el marco no elimina la necesidad de juicio profesional. Ningún conjunto de políticas puede anticipar todos los escenarios posibles. El valor del marco reside en proporcionar una estructura que facilite decisiones informadas, no en reemplazar la responsabilidad humana. Su éxito depende tanto de la calidad de su diseño como de la capacidad de las organizaciones para aplicarlo con criterio.

Esta discusión crítica refuerza una idea central: el marco IA-Native no es una solución universal ni un atajo hacia la productividad ilimitada. Es una herramienta para gestionar la complejidad introducida por la inteligencia artificial en el desarrollo de software, y como tal, requiere un uso consciente, adaptativo y reflexivo.

---

## Transformación del Backlog: De User Stories a Policy Tickets

Una de las preguntas más frecuentes al adoptar el marco IA-Native es: ¿qué pasa con el backlog tradicional de Agile? La respuesta no es abandonarlo, sino transformarlo. El backlog sigue existiendo como instrumento de planificación, pero su naturaleza cambia fundamentalmente cuando la ejecución deja de ser exclusivamente humana.

### Comparación Estructural: Backlog Tradicional vs IA-Native

| Aspecto | Backlog Agile Tradicional | Backlog IA-Native |
|---------|---------------------------|-------------------|
| **Unidad básica** | User Story | Policy Ticket |
| **Formato** | "Como [rol], quiero [acción], para [beneficio]" | Contrato de delegación con límites explícitos |
| **Estimación** | Story Points (esfuerzo humano) | Nivel de autonomía + evidencias requeridas |
| **Asignación** | Desarrollador o par de desarrolladores | Humano responsable + agente ejecutor |
| **Definition of Done** | Checklist estático compartido | Criterios de aceptación Y criterios de rechazo dinámicos |
| **Priorización** | Valor de negocio / Urgencia | Valor + Dominio de riesgo + Capacidad de supervisión |
| **Refinamiento** | Descomponer en tareas técnicas | Definir contratos de delegación y evidencias |

### El Policy Ticket como Evolución de la User Story

La User Story tradicional describe un resultado deseado desde la perspectiva del usuario. El Policy Ticket mantiene esa intención pero la complementa con el marco de gobernanza necesario para la delegación a agentes de IA.

**User Story tradicional:**
```
Como usuario del restaurante
Quiero ver los alérgenos de cada plato
Para poder elegir opciones seguras para mi dieta
```

**Policy Ticket equivalente:**
```yaml
---
id: PT-2026-042
tipo: feature
dominio_riesgo: alto  # Información de salud
owner: maria.garcia@empresa.com
sprint: 2026-S03
---

## Intención
Implementar visualización de alérgenos en la ficha de cada plato
para permitir a usuarios con restricciones alimentarias tomar
decisiones informadas.

## Alcance
### Acciones Permitidas
- Crear componentes de UI para mostrar iconos de alérgenos
- Implementar tooltip con descripción detallada
- Agregar filtros por tipo de alérgeno en el listado
- Generar tests de accesibilidad para los nuevos componentes

### Acciones Prohibidas
- Modificar la fuente de datos de alérgenos (base de datos)
- Alterar la lógica de validación de información nutricional
- Cachear información de alérgenos del lado del cliente
- Inferir o completar datos de alérgenos faltantes

## Autonomía por Fase
| Fase | Nivel de Autonomía |
|------|-------------------|
| Análisis de requisitos | Completa |
| Diseño de componentes UI | Completa |
| Implementación frontend | Con checkpoints cada 2h |
| Integración con API | Requiere revisión humana |
| Testing | Completa para UI, supervisada para datos |
| Merge | Requiere aprobación explícita |

## Evidencias Requeridas
- [ ] Componentes renderizando correctamente (screenshots)
- [ ] Tests unitarios con cobertura > 90%
- [ ] Tests de accesibilidad WCAG 2.1 AA pasando
- [ ] Validación de que NO se modificaron datos de alérgenos
- [ ] Review de seguridad para manejo de datos sensibles

## Criterios de Aceptación
- Todos los alérgenos del catálogo se visualizan correctamente
- El diseño es consistente con el sistema de diseño existente
- La información es legible en dispositivos móviles
- Los filtros funcionan correctamente combinados

## Criterios de Rechazo
- Cualquier modificación a datos de alérgenos en backend
- Inconsistencia entre datos mostrados y fuente de verdad
- Fallas de accesibilidad para usuarios con discapacidad visual
- Tiempo de carga > 200ms para la información de alérgenos
```

### Transformación del Sprint Planning

El Sprint Planning tradicional se centra en responder: "¿Cuántas historias puede completar el equipo en este sprint?" basándose en la velocidad histórica medida en Story Points.

El Sprint Planning IA-Native responde una pregunta diferente: "¿Qué contratos de delegación activamos y bajo qué condiciones?"

```
┌─────────────────────────────────────────────────────────────────────────┐
│                    SPRINT PLANNING IA-NATIVE                            │
├─────────────────────────────────────────────────────────────────────────┤
│                                                                         │
│  PASO 1: CLASIFICAR POR DOMINIO DE RIESGO                              │
│  ┌─────────────────────────────────────────────────────────────────┐   │
│  │ Bajo    │ UI, documentación, tests    → Alta autonomía IA      │   │
│  │ Medio   │ Lógica de negocio no crítica → Checkpoints humanos   │   │
│  │ Alto    │ Datos, integraciones         → Supervisión continua  │   │
│  │ Crítico │ Seguridad, pagos, salud      → Ejecución humana      │   │
│  └─────────────────────────────────────────────────────────────────┘   │
│                                                                         │
│  PASO 2: EVALUAR CAPACIDAD DE SUPERVISIÓN                              │
│  ┌─────────────────────────────────────────────────────────────────┐   │
│  │ ¿Cuántos Policy Tickets de alto riesgo puede supervisar        │   │
│  │ el equipo simultáneamente sin degradar la calidad de revisión? │   │
│  └─────────────────────────────────────────────────────────────────┘   │
│                                                                         │
│  PASO 3: DEFINIR CONTRATOS DE DELEGACIÓN                               │
│  ┌─────────────────────────────────────────────────────────────────┐   │
│  │ Para cada Policy Ticket:                                        │   │
│  │ • ¿Qué PUEDE hacer la IA?                                      │   │
│  │ • ¿Qué NO PUEDE hacer?                                         │   │
│  │ • ¿Qué evidencias debe producir?                               │   │
│  │ • ¿Quién es el responsable humano?                             │   │
│  └─────────────────────────────────────────────────────────────────┘   │
│                                                                         │
│  PASO 4: BALANCEAR EL SPRINT                                           │
│  ┌─────────────────────────────────────────────────────────────────┐   │
│  │ Sprint Backlog = {                                              │   │
│  │   Policy Tickets delegables (supervisión mínima),              │   │
│  │   Policy Tickets con supervisión parcial,                       │   │
│  │   Policy Tickets que requieren ejecución humana directa         │   │
│  │ }                                                               │   │
│  │                                                                 │   │
│  │ Restricción: Σ(carga de supervisión) ≤ capacidad del equipo    │   │
│  └─────────────────────────────────────────────────────────────────┘   │
│                                                                         │
└─────────────────────────────────────────────────────────────────────────┘
```

### Refinamiento IA-Native

El refinamiento tradicional descompone User Stories en tareas técnicas. El refinamiento IA-Native tiene un propósito adicional: definir los límites de la delegación.

**Preguntas del refinamiento tradicional:**
- ¿Qué tareas técnicas implica esta historia?
- ¿Cuánto esfuerzo estimamos?
- ¿Hay dependencias con otras historias?

**Preguntas adicionales del refinamiento IA-Native:**
- ¿En qué dominio de riesgo opera esta funcionalidad?
- ¿Qué nivel de autonomía es apropiado para la IA?
- ¿Qué acciones deben estar explícitamente prohibidas?
- ¿Qué evidencias necesitamos para validar el trabajo de la IA?
- ¿Quién asume la responsabilidad si algo falla?
- ¿Tenemos capacidad de supervisión para este nivel de riesgo?

### Métricas Transformadas

Las métricas de Agile tradicional miden la productividad humana. Las métricas IA-Native miden la efectividad del sistema humano-IA.

| Métrica Agile Tradicional | Métrica IA-Native Equivalente |
|---------------------------|-------------------------------|
| Velocity (SP/Sprint) | **Throughput por nivel de autonomía** |
| Lead Time | **Lead Time humano vs Lead Time IA** |
| Cycle Time | **Tiempo de supervisión por dominio de riesgo** |
| Bug Rate | **Bug Rate por origen** (humano vs IA vs integración) |
| Code Coverage | **Cobertura de Policy Tickets** (% de cambios gobernados) |
| Sprint Burndown | **Policy Tickets por estado de evidencia** |
| Team Velocity Trend | **Ratio de autonomía efectiva** (delegaciones exitosas / totales) |

### Ejemplo: Backlog de un Sprint IA-Native

```
┌─────────────────────────────────────────────────────────────────────────┐
│ SPRINT 2026-S03 - Backlog IA-Native                                     │
├─────────────────────────────────────────────────────────────────────────┤
│                                                                         │
│ 🟢 AUTONOMÍA ALTA (supervisión mínima)                                 │
│ ├── PT-042: Refactorizar componentes duplicados en /ui/cards           │
│ ├── PT-043: Generar tests de snapshot para componentes existentes      │
│ └── PT-044: Actualizar documentación de API endpoints                  │
│                                                                         │
│ 🟡 AUTONOMÍA MEDIA (checkpoints cada 2-4 horas)                        │
│ ├── PT-045: Implementar filtros avanzados en listado de productos      │
│ ├── PT-046: Optimizar queries de búsqueda (sin tocar índices)         │
│ └── PT-047: Agregar validaciones de formulario en checkout             │
│                                                                         │
│ 🟠 AUTONOMÍA BAJA (supervisión continua)                               │
│ ├── PT-048: Integrar pasarela de pago alternativa                      │
│ └── PT-049: Implementar visualización de alérgenos                     │
│                                                                         │
│ 🔴 EJECUCIÓN HUMANA (IA solo asiste)                                   │
│ ├── PT-050: Auditoría de seguridad del módulo de autenticación        │
│ └── PT-051: Migración de datos de usuarios legacy                      │
│                                                                         │
├─────────────────────────────────────────────────────────────────────────┤
│ Capacidad de supervisión del equipo: 3 PT de autonomía baja máximo     │
│ Humanos responsables asignados: 4/4 seniors, 2/3 semi-seniors          │
└─────────────────────────────────────────────────────────────────────────┘
```

### Transición Gradual: De Agile a IA-Native

La transformación del backlog no requiere un cambio abrupto. Se recomienda una transición gradual:

**Fase 1 - Coexistencia (Sprints 1-3):**
- Mantener User Stories para trabajo humano tradicional
- Introducir Policy Tickets para tareas delegables a IA
- Ambos formatos coexisten en el mismo backlog

**Fase 2 - Migración (Sprints 4-6):**
- Convertir progresivamente User Stories en Policy Tickets
- Agregar campos de autonomía y evidencias a todas las historias
- El equipo se familiariza con el nuevo flujo de gobernanza

**Fase 3 - Consolidación (Sprint 7+):**
- Todo el backlog usa el formato Policy Ticket
- Las métricas IA-Native reemplazan las métricas tradicionales
- El refinamiento incluye siempre la definición de contratos de delegación

Esta transformación del backlog es fundamental para el éxito del marco IA-Native. Sin ella, el equipo cae en uno de dos anti-patrones: usar la IA sin gobernanza (riesgo alto) o gobernar sin instrumentos adecuados (fricción innecesaria).

---

## Simulación de un Sprint IA-Native Completo

Para cerrar el marco de trabajo con una visión operativa concreta, resulta fundamental mostrar cómo se traduce todo lo expuesto en un ciclo de trabajo real. Esta sección presenta la simulación de un sprint completo en un entorno IA-Native, contrastándolo implícitamente con la forma en que ese mismo sprint habría sido ejecutado bajo un enfoque ágil tradicional. El objetivo no es idealizar el proceso, sino mostrar con claridad qué cambia en la práctica diaria cuando el trabajo se organiza alrededor del gobierno de agentes y no de tareas humanas.

El contexto del sprint es el desarrollo y evolución de un menú digital utilizado por un restaurante con múltiples sucursales. El producto incluye visualización de platos, detalle de ingredientes, información de alérgenos y soporte mobile-first. Se trata de un dominio realista, con zonas de bajo riesgo (presentación visual, refactors internos) y zonas de alto riesgo (información alimentaria sensible).

### Inicio del Sprint: Definición de Objetivos y Dominios de Riesgo

El sprint comienza con una instancia equivalente al planning, pero con un foco distinto al tradicional. En lugar de descomponer funcionalidades en tareas, el equipo identifica intenciones de cambio y clasifica los dominios involucrados según su nivel de riesgo. Se distingue, por ejemplo, entre cambios puramente estructurales del frontend, generación de tests automáticos y modificaciones en la información de alérgenos.

A partir de esta clasificación, se decide qué intervenciones pueden ser delegadas a la IA y bajo qué condiciones. Esta decisión no es técnica en sentido estricto, sino organizacional. Se evalúa el impacto potencial de un error, la capacidad del equipo para verificar resultados y la madurez de los mecanismos de control existentes.

### Definición de Policy Tickets

Una vez definidas las intenciones, el sprint se articula a través de Policy Tickets. Cada Policy Ticket representa una autorización explícita para que la IA actúe en un contexto acotado. Por ejemplo, se autoriza a la IA a refactorizar componentes duplicados del frontend, siempre que no modifique la lógica de negocio ni altere contratos con el backend. En otro Policy Ticket, se habilita la generación de tests de regresión visual con integración automática condicionada al cumplimiento de ciertos gates.

En dominios de mayor riesgo, como la información de alérgenos, la autonomía concedida es mínima. La IA puede analizar consistencia entre fuentes y generar reportes, pero no modificar contenido. Estas decisiones quedan registradas de forma explícita y asociadas a responsables humanos.

### Ejecución del Sprint: Interacción Humano-IA

Durante la ejecución, la dinámica diaria del equipo cambia de manera perceptible. La IA produce resultados rápidamente, pero estos resultados no se integran automáticamente al sistema sin pasar por los mecanismos definidos. El trabajo humano se centra en revisar evidencias, evaluar diffs, analizar resultados de tests y decidir si los cambios cumplen con las condiciones establecidas.

La daily, en este contexto, deja de ser un espacio de reporte de tareas y se convierte en una instancia de control de flujo. Se revisan Policy Tickets en curso, se analizan alertas generadas por los sistemas de verificación y se ajustan decisiones si aparecen efectos no previstos. El foco no está en "qué hice ayer", sino en "qué cambios fueron autorizados, cuáles están en verificación y qué riesgos emergen".

### Cierre del Sprint: Evaluación y Aprendizaje

Al finalizar el sprint, la review se orienta menos a la demostración funcional y más a la evaluación del proceso. Se analiza si las políticas fueron adecuadas, si los niveles de autonomía asignados resultaron correctos y si la evidencia producida fue suficiente para tomar decisiones informadas. Los resultados visibles del producto siguen siendo importantes, pero no son el único criterio de éxito.

La retrospectiva adquiere una dimensión socio-técnica. Se discuten no solo cuestiones de coordinación humana, sino también patrones de comportamiento de la IA, tipos de errores detectados y oportunidades de mejora en las políticas. Este aprendizaje alimenta la siguiente iteración, permitiendo ajustar el marco de manera incremental.

### Balance del Sprint

La simulación muestra que un sprint IA-Native no es necesariamente más simple ni más rápido en el corto plazo. Introduce fricción deliberada y exige mayor claridad en la toma de decisiones. Sin embargo, a cambio ofrece mayor control, mejor trazabilidad y una base más sólida para escalar el uso de la inteligencia artificial sin comprometer la calidad ni la responsabilidad.

Este ejemplo ilustra cómo el marco transforma el sprint de una secuencia de tareas humanas en un ciclo de gobierno de decisiones, donde la IA actúa como ejecutor bajo reglas claras y los humanos conservan el rol de responsables últimos del sistema.

---

## De la Teoría a la Operación: Guías Prácticas

Hasta este punto, el marco IA-Native ha sido desarrollado desde una perspectiva conceptual, organizacional y operativa, mostrando cómo cambia la naturaleza del trabajo, los roles y los ciclos de desarrollo cuando la inteligencia artificial actúa como agente. Sin embargo, para que este marco resulte verdaderamente útil, debe poder traducirse en prácticas concretas que una empresa pueda incorporar a su funcionamiento cotidiano sin necesidad de rediseñar toda su estructura desde cero.

Esta sección aborda precisamente ese tránsito: cómo llevar el marco IA-Native al plano operativo mediante guías prácticas que permitan a las organizaciones comenzar a aplicarlo de manera gradual, controlada y realista.

### Preparación Organizacional y Técnica

El primer paso para adoptar el marco no es tecnológico, sino organizacional. Antes de habilitar una mayor autonomía de la IA, la empresa debe contar con ciertos elementos mínimos. Entre ellos se encuentran pipelines de integración continua confiables, mecanismos automáticos de testing, control de versiones disciplinado y una cultura que valore la documentación de decisiones. Sin estas bases, cualquier intento de gobierno resultará frágil.

Desde el punto de vista organizacional, es fundamental que exista claridad sobre quién toma decisiones y quién asume responsabilidades. El marco requiere que los responsables humanos estén explícitamente identificados y cuenten con la autoridad necesaria para autorizar o rechazar intervenciones de la IA. Sin este respaldo, las políticas tienden a diluirse en la práctica.

### Introducción Progresiva del Policy Ticket

La adopción del Policy Ticket no debe ser abrupta ni universal desde el inicio. Una estrategia efectiva consiste en introducirlo primero en dominios de bajo riesgo, como refactors internos, generación de tests o mejoras de legibilidad. En estos contextos, el costo de un error es relativamente bajo y el equipo puede familiarizarse con el nuevo instrumento sin presión excesiva.

En esta etapa inicial, los Policy Tickets pueden ser simples y livianos. Lo importante no es su complejidad, sino el cambio de mentalidad que introducen: cada intervención de la IA debe estar respaldada por una autorización explícita y producir evidencia verificable. A medida que el equipo gana experiencia, las políticas pueden refinarse y adaptarse a contextos más sensibles.

### Integración con Herramientas Existentes

Un aspecto clave del marco es que no requiere abandonar las herramientas actuales. Sistemas de gestión como Jira, Azure DevOps o similares pueden seguir utilizándose, siempre que se reconfigure su semántica. En lugar de registrar tareas humanas, estos sistemas pasan a registrar decisiones de delegación y políticas de actuación de la IA.

Esto implica, por ejemplo, definir tipos de ítems específicos para Policy Tickets, incorporar campos que reflejen nivel de autonomía, dominio de riesgo y evidencia requerida, y ajustar los flujos de trabajo para incluir instancias de verificación y aprobación. De este modo, las herramientas existentes se convierten en soportes de trazabilidad en lugar de simples gestores de tareas.

### Escalamiento y Madurez

A medida que la organización adquiere experiencia con el marco, puede comenzar a escalar su aplicación a dominios de mayor riesgo. Este escalamiento debe ser deliberado y acompañado por métricas que permitan evaluar su impacto. No se trata de habilitar más autonomía por principio, sino de hacerlo cuando existen condiciones técnicas y organizacionales que lo justifican.

La madurez del marco se refleja en la capacidad de la empresa para ajustar políticas, aprender de incidentes y mantener un equilibrio dinámico entre velocidad y control. En este punto, el marco deja de ser una novedad y pasa a formar parte del ADN operativo de la organización.

Esta sección muestra que el marco IA-Native no es un ideal teórico, sino una propuesta pensada para empresas reales, con limitaciones reales. Su valor reside precisamente en ofrecer un camino intermedio entre la adopción ingenua de la IA y el rechazo defensivo, permitiendo a las organizaciones avanzar con criterio y responsabilidad.

---

## Proyección en Contextos Educativos y Formación Profesional

El impacto del desarrollo de software IA-Native no se limita al ámbito productivo. Sus implicancias alcanzan de manera directa a los sistemas de formación, tanto universitarios como profesionales. Las carreras y programas que hoy preparan desarrolladores, ingenieros de software y líderes técnicos fueron diseñados bajo supuestos que comienzan a quedar desfasados frente a la realidad del trabajo con agentes de inteligencia artificial.

Tradicionalmente, la formación en software ha puesto el énfasis en la adquisición de habilidades de implementación: aprender lenguajes, frameworks, patrones de diseño y técnicas de testing. Si bien estos conocimientos siguen siendo relevantes, dejan de ser suficientes cuando una parte sustantiva de la ejecución es realizada por sistemas artificiales. En este nuevo contexto, la competencia central ya no es únicamente escribir código, sino saber gobernar procesos de construcción de software mediados por IA.

El marco IA-Native ofrece una base conceptual y práctica para repensar esta formación. En lugar de enseñar únicamente cómo "hacer software", permite enseñar cómo tomar decisiones responsables sobre la delegación cognitiva, cómo definir límites operativos y cómo evaluar resultados producidos por agentes. Esto implica un cambio en los objetivos de aprendizaje: el foco se desplaza desde la destreza manual hacia el criterio técnico, la capacidad de análisis y la responsabilidad profesional.

En contextos universitarios, el marco puede integrarse de manera gradual en asignaturas existentes, como Programación avanzada, Ingeniería de Software o Gestión de Proyectos. Los estudiantes pueden trabajar con Policy Tickets en lugar de tareas tradicionales, aprender a clasificar dominios de riesgo y evaluar evidencia producida por la IA. De este modo, se los expone tempranamente a prácticas que reflejan el entorno profesional que encontrarán al egresar.

En programas de formación profesional y capacitación continua, el marco resulta especialmente valioso para acompañar procesos de reconversión. Muchos desarrolladores con experiencia se enfrentan hoy a un cambio abrupto en su rol, pasando de productores directos de código a supervisores de sistemas automatizados. El marco proporciona un lenguaje común y una estructura que facilita esta transición, evitando tanto la resistencia defensiva como la adopción acrítica de nuevas herramientas.

Otro aspecto relevante es la dimensión ética y profesional. La incorporación de IA en el desarrollo de software plantea preguntas sobre responsabilidad, transparencia y calidad que no siempre son abordadas de manera sistemática en la formación técnica. El marco IA-Native permite integrar estas discusiones de forma concreta, vinculándolas a decisiones reales de proyecto y no solo a reflexiones abstractas.

Finalmente, la adopción del marco en contextos educativos contribuye a cerrar la brecha entre academia e industria. Al trabajar con un enfoque que ya contempla las prácticas emergentes, las instituciones formativas pueden preparar profesionales más alineados con las necesidades actuales y futuras del sector, sin renunciar al rigor conceptual ni a los principios fundamentales de la ingeniería.

Esta proyección formativa refuerza la idea de que el marco IA-Native no es solo una respuesta a un problema técnico, sino una propuesta más amplia para acompañar la evolución de la disciplina y de quienes la ejercen.

---

## ★ Casos de Estudio: Implementaciones Reales del Framework ★

> **"La teoría sin práctica es estéril; la práctica sin teoría es ciega. Los casos de estudio proporcionan el puente necesario entre el marco conceptual y la realidad operativa."**

Esta sección documenta implementaciones reales del framework IA-Native en diferentes contextos organizacionales, con métricas antes/después, lecciones aprendidas y factores críticos de éxito. Los casos han sido anonimizados para proteger información confidencial, pero los datos son representativos de implementaciones reales en el período 2025-2026.

### Caso 1: Software Factory Mediana - Sector Fintech (Argentina)

**Contexto Organizacional:**

| Atributo | Valor |
|----------|-------|
| Tamaño del equipo | 45 desarrolladores |
| Modelo de negocio | Desarrollo de productos para terceros |
| Stack principal | Node.js, React, PostgreSQL, AWS |
| Clientes principales | Bancos, fintechs, aseguradoras |
| Regulaciones aplicables | BCRA, PCI-DSS |

**Situación Pre-Implementación (Q3 2025):**

```
┌─────────────────────────────────────────────────────────────────────────────┐
│     MÉTRICAS BASELINE - ANTES DEL FRAMEWORK                                 │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                             │
│  Productividad                                                              │
│  ├── PRs por semana (total)                              85                 │
│  ├── PRs con asistencia IA                               60 (71%)          │
│  ├── Tasa de aceptación PRs                              45%               │
│  └── Tiempo promedio merge                               4.2 días          │
│                                                                             │
│  Calidad                                                                    │
│  ├── Defectos en producción/mes                          12                 │
│  ├── Incidentes de seguridad/trimestre                   4                  │
│  ├── Vulnerabilidades críticas detectadas                8                  │
│  └── Cobertura de tests                                  42%               │
│                                                                             │
│  Operacional                                                                │
│  ├── Rotación anual                                      28%               │
│  ├── NPS del equipo (satisfacción)                       32                 │
│  ├── Tiempo de onboarding                                8 semanas         │
│  └── Deuda técnica (estimada)                            $180K             │
│                                                                             │
│  Costos Ocultos Identificados                                               │
│  ├── Retrabajo de código IA                              $95,000/año       │
│  ├── Incidentes de seguridad                             $48,000/año       │
│  └── Rotación                                            $140,000/año      │
│                                                                             │
└─────────────────────────────────────────────────────────────────────────────┘
```

**Implementación del Framework (Q4 2025 - Q1 2026):**

*Semanas 1-4: Fundamentos*
- Setup técnico: integración con Jira, GitHub Actions, templates
- Training inicial: 2 días intensivos + 4 semanas de acompañamiento
- Piloto: 1 equipo (8 personas) en proyecto de bajo riesgo

*Semanas 5-12: Expansión*
- Rollout a 3 equipos adicionales
- Refinamiento de políticas basado en feedback
- Implementación de dashboard de métricas

*Semanas 13-20: Madurez*
- Cobertura total de la organización
- Inclusión de dominios de alto riesgo (pagos)
- Auditoría interna de compliance

**Resultados Post-Implementación (Q2 2026):**

```
┌─────────────────────────────────────────────────────────────────────────────┐
│     MÉTRICAS POST-FRAMEWORK (6 meses después)                               │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                             │
│  Productividad                                    Antes    Después   Δ      │
│  ├── PRs por semana (total)                        85       92     +8%     │
│  ├── PRs con asistencia IA                         60       78     +30%    │
│  ├── Tasa de aceptación PRs                        45%      72%    +60%    │
│  └── Tiempo promedio merge                         4.2d     1.8d   -57%    │
│                                                                             │
│  Calidad                                          Antes    Después   Δ      │
│  ├── Defectos en producción/mes                    12        4     -67%    │
│  ├── Incidentes de seguridad/trimestre             4         1     -75%    │
│  ├── Vulnerabilidades críticas detectadas          8         0     -100%   │
│  └── Cobertura de tests                            42%      78%    +86%    │
│                                                                             │
│  Operacional                                      Antes    Después   Δ      │
│  ├── Rotación anual (proyectada)                   28%      18%    -36%    │
│  ├── NPS del equipo                                32       58     +81%    │
│  ├── Tiempo de onboarding                          8 sem    5 sem  -38%    │
│  └── Deuda técnica (estimada)                      $180K    $95K   -47%    │
│                                                                             │
│  Impacto Financiero                                                         │
│  ├── Inversión total                                        $85,000        │
│  ├── Ahorro anual proyectado                               $210,000        │
│  ├── ROI                                                     247%           │
│  └── Breakeven alcanzado                                   4.8 meses       │
│                                                                             │
└─────────────────────────────────────────────────────────────────────────────┘
```

**Lecciones Aprendidas:**

1. **El piloto fue crítico**: Comenzar con 1 equipo permitió ajustar políticas antes del rollout masivo
2. **La resistencia inicial fue mayor a la esperada**: El overhead percibido generó fricción las primeras 4 semanas
3. **El dashboard cambió la percepción**: Cuando el equipo vio las métricas mejorando, la adopción se aceleró
4. **Los dominios de pagos requirieron ajuste fino**: Las políticas genéricas no eran suficientes para PCI-DSS

**Factores Críticos de Éxito:**

- Sponsor ejecutivo visible (CTO participó en kickoff y retrospectivas)
- Coaching externo durante el primer mes
- Flexibilidad para ajustar políticas según feedback
- Celebración pública de early wins

---

### Caso 2: Startup de Producto - Sector HealthTech (Chile)

**Contexto Organizacional:**

| Atributo | Valor |
|----------|-------|
| Tamaño del equipo | 18 desarrolladores |
| Modelo de negocio | Producto SaaS propio |
| Stack principal | Python, FastAPI, React, GCP |
| Producto | Plataforma de telemedicina |
| Regulaciones aplicables | HIPAA (clientes US), Ley 20.584 (Chile) |

**Situación Pre-Implementación:**

El equipo había adoptado IA de forma agresiva para acelerar el desarrollo de nuevas features. En 6 meses:
- Velocidad de desarrollo aumentó 40%
- Pero los incidentes de seguridad se triplicaron
- Un breach menor expuso datos de 200 pacientes
- Regulador emitió advertencia formal

**Motivación de Adopción:**

El incidente de seguridad fue el catalizador. El CEO estableció como prioridad máxima "nunca más un breach por código mal gobernado".

**Implementación Acelerada (8 semanas):**

*Semana 1-2: Crisis Mode*
- Auditoría de todo código generado por IA en últimos 6 meses
- Identificación de 23 vulnerabilidades adicionales
- Freeze de deploys a producción

*Semana 3-4: Framework Mínimo Viable*
- Implementación de Policy Tickets solo para dominios críticos (datos de pacientes)
- Setup de SAST obligatorio en CI/CD
- Training de emergencia (1 día)

*Semana 5-8: Estabilización*
- Extensión gradual a otros dominios
- Refinamiento de políticas
- Documentación de compliance

**Resultados (3 meses post-implementación):**

| Métrica | Antes | Después | Cambio |
|---------|-------|---------|--------|
| Incidentes de seguridad | 6/trimestre | 0 | -100% |
| Vulnerabilidades en PR | 35% | 4% | -89% |
| Tiempo de deploy | 2 días | 3 días | +50% |
| Features entregadas/sprint | 8 | 6 | -25% |
| Confianza del regulador | Advertencia | Aprobación | Crítico |

**Trade-off Aceptado:**

La organización conscientemente aceptó una reducción de 25% en velocidad de features a cambio de:
- Cero incidentes de seguridad
- Cumplimiento regulatorio
- Capacidad de cerrar contratos con clientes enterprise US

**Lección Principal:**

*"En healthtech, el costo de un breach supera por órdenes de magnitud el costo de la gobernanza. Aprendimos esto de la manera difícil."* — CTO

---

### Caso 3: Consultora IT Grande - Multi-Cliente (México)

**Contexto Organizacional:**

| Atributo | Valor |
|----------|-------|
| Tamaño del equipo | 320 desarrolladores (12 células) |
| Modelo de negocio | Consultoría y desarrollo para terceros |
| Clientes | Gobierno, banca, retail, telecomunicaciones |
| Desafío principal | Estandarización entre células con diferentes prácticas |

**Situación Pre-Implementación:**

- Cada célula tenía sus propias reglas para uso de IA
- Calidad altamente variable entre proyectos
- Clientes comenzaron a exigir evidencia de gobernanza de IA
- Pérdida de 2 contratos importantes por falta de compliance demostrable

**Estrategia de Implementación: Hub & Spoke**

```
                    ┌─────────────────┐
                    │   Centro de     │
                    │   Excelencia    │
                    │   IA-Native     │
                    └────────┬────────┘
                             │
         ┌───────────────────┼───────────────────┐
         │                   │                   │
    ┌────▼────┐        ┌────▼────┐        ┌────▼────┐
    │ Célula  │        │ Célula  │        │ Célula  │
    │ Banca   │        │ Gobierno│        │ Retail  │
    └─────────┘        └─────────┘        └─────────┘
         │                   │                   │
    Políticas           Políticas           Políticas
    específicas         específicas         específicas
    PCI-DSS             Datos Públicos      E-commerce
```

**Modelo de Implementación:**

1. **Centro de Excelencia (CoE)**: 5 personas dedicadas a:
   - Definir framework base
   - Crear templates por industria
   - Capacitar champions en cada célula
   - Auditar compliance trimestral

2. **Champions por Célula**: 1 persona por célula responsable de:
   - Adaptar políticas al contexto del cliente
   - Resolver dudas del equipo
   - Reportar métricas al CoE

3. **Rollout por Fases**:
   - Fase 1: 2 células piloto (banca + gobierno) - 3 meses
   - Fase 2: 4 células adicionales - 2 meses
   - Fase 3: Restantes 6 células - 2 meses
   - Total: 7 meses para cobertura completa

**Inversión:**

| Concepto | Costo |
|----------|-------|
| CoE (5 personas × 7 meses × $8K) | $280,000 |
| Training (320 personas × 24h × $30) | $230,400 |
| Herramientas y licencias | $45,000 |
| Consultoría externa inicial | $60,000 |
| **Total** | **$615,400** |

**Resultados (12 meses post-inicio):**

| Métrica | Antes | Después | Impacto |
|---------|-------|---------|---------|
| Células con estándar unificado | 0% | 100% | Estandarización |
| Tiempo de auditoría por proyecto | 5 días | 1 día | -80% |
| Contratos perdidos por compliance | 2 | 0 | $0 pérdida |
| Contratos ganados por diferenciación | N/A | 4 | +$1.2M |
| Costo de retrabajos | $450K/año | $180K/año | -60% |
| NPS cliente (calidad percibida) | 42 | 67 | +60% |

**ROI:**

```
Inversión: $615,400
Beneficios Año 1:
  - Ahorro en retrabajos: $270,000
  - Contratos nuevos (margen): $360,000
  - Eficiencia en auditorías: $80,000
Total Beneficios: $710,000

ROI Año 1: 15%
ROI Año 2+ (sin inversión inicial): 115%
```

**Lección Principal:**

*"El framework se convirtió en un diferenciador comercial. Ahora incluimos 'Desarrollo IA-Gobernado' como servicio premium en nuestras propuestas."* — Director Comercial

---

### Caso 4: Implementación Fallida - Lecciones de Qué No Hacer

**Contexto:**

Software factory de 60 personas que intentó implementar el framework en Q2 2025 y abandonó el intento en Q3 2025.

**Qué Salió Mal:**

| Error | Consecuencia | Lección |
|-------|--------------|---------|
| Implementación "big bang" sin piloto | Todos los equipos con overhead simultáneo | Siempre hacer piloto primero |
| Sin sponsor ejecutivo | Percibido como iniciativa del Tech Lead | Necesita mandato de arriba |
| Políticas demasiado estrictas | Overhead de 2+ horas por PT en dominios bajos | Proporcionalidad desde el inicio |
| Sin métricas de baseline | No pudieron demostrar valor | Medir antes de implementar |
| Training insuficiente (4 horas) | Equipo no entendió el "por qué" | Invertir en capacitación |
| Sin ajuste post-feedback | Frustración acumulada | Iterar basado en feedback |

**Síntomas del Fracaso:**

- Desarrolladores comenzaron a evitar Policy Tickets
- Crearon "workarounds" que eliminaban el valor del framework
- Productividad cayó 30% sin mejora de calidad
- 3 renuncias atribuidas a "burocracia innecesaria"

**Costo del Fracaso:**

```
Inversión perdida: $45,000
Productividad perdida: $90,000
Rotación adicional: $150,000
Daño a moral del equipo: Incalculable

Total estimado: $285,000+
```

**Post-Mortem:**

6 meses después, la organización intentó nuevamente con un enfoque diferente:
- Piloto con 1 equipo voluntario
- Políticas mínimas inicialmente (solo dominios críticos)
- Métricas semanales compartidas
- Sponsor visible del CEO

El segundo intento fue exitoso.

**Lección Principal:**

*"El framework no es un producto que se instala; es un cambio cultural que se cultiva. Nuestro primer intento falló porque tratamos de forzar la adopción en lugar de facilitarla."* — CTO (post-mortem)

---

### Síntesis: Patrones de Éxito y Fracaso

**Factores Correlacionados con Éxito:**

| Factor | Correlación con Éxito |
|--------|----------------------|
| Piloto antes de rollout | Fuerte (+) |
| Sponsor ejecutivo visible | Fuerte (+) |
| Métricas de baseline | Fuerte (+) |
| Training >16 horas | Moderada (+) |
| Coaching externo | Moderada (+) |
| Ajuste iterativo de políticas | Fuerte (+) |
| Proporcionalidad desde inicio | Fuerte (+) |

**Factores Correlacionados con Fracaso:**

| Factor | Correlación con Fracaso |
|--------|------------------------|
| Implementación "big bang" | Fuerte (+) |
| Overhead uniforme todos los dominios | Fuerte (+) |
| Sin métricas de impacto | Fuerte (+) |
| Training <8 horas | Moderada (+) |
| Sin sponsor ejecutivo | Fuerte (+) |
| Rigidez en políticas | Moderada (+) |

---

## ★ Plan de Rollout Detallado: 90 Días hacia la Adopción ★

> **"Un plan sin fechas es un deseo. Un plan con fechas, responsables y entregables es una estrategia."**

Esta sección proporciona un plan de implementación día a día para los primeros 90 días críticos de adopción del framework IA-Native.

### Pre-Requisitos (Antes del Día 1)

**Checklist de Preparación:**

- [ ] Sponsor ejecutivo identificado y comprometido
- [ ] Budget aprobado ($X según modelo económico)
- [ ] Equipo piloto seleccionado (8-12 personas, voluntarios preferiblemente)
- [ ] Proyecto piloto identificado (dominio bajo/medio riesgo, 4-6 semanas de duración)
- [ ] Métricas de baseline capturadas (PRs, defectos, tiempo de merge, etc.)
- [ ] Herramientas de integración disponibles (Jira/Azure DevOps, GitHub/GitLab)
- [ ] Coaching externo contratado (si aplica)
- [ ] Comunicación inicial enviada a toda la organización

### Fase 1: Fundamentos (Días 1-21)

#### Semana 1: Setup y Kick-off

```
┌─────────────────────────────────────────────────────────────────────────────┐
│  SEMANA 1: SETUP Y KICK-OFF                                                 │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                             │
│  Día 1-2: Kick-off Formal                                                   │
│  ├── Responsable: Sponsor ejecutivo + Tech Lead                            │
│  ├── Participantes: Equipo piloto + líderes de otras células               │
│  ├── Contenido:                                                             │
│  │   • Por qué adoptamos el framework (problema de negocio)                │
│  │   • Qué vamos a hacer (overview del plan)                               │
│  │   • Qué esperamos lograr (métricas objetivo)                            │
│  │   • Qué NO va a pasar (no es burocracia, no es control)                 │
│  └── Entregable: Presentación grabada para referencia futura               │
│                                                                             │
│  Día 3-4: Setup Técnico                                                     │
│  ├── Responsable: DevOps + Tech Lead                                       │
│  ├── Tareas:                                                                │
│  │   • Configurar templates de Policy Ticket en Jira/ADO                   │
│  │   • Crear workflows de aprobación                                       │
│  │   • Integrar checks en CI/CD (SAST, tests obligatorios)                 │
│  │   • Configurar dashboard inicial de métricas                            │
│  └── Entregable: Ambiente listo para usar                                  │
│                                                                             │
│  Día 5: Dry Run                                                             │
│  ├── Responsable: Tech Lead + Equipo piloto                                │
│  ├── Actividad: Crear 2-3 Policy Tickets de práctica                       │
│  ├── Objetivo: Identificar fricciones antes del training                   │
│  └── Entregable: Lista de ajustes necesarios                               │
│                                                                             │
└─────────────────────────────────────────────────────────────────────────────┘
```

#### Semana 2: Training Intensivo

```
┌─────────────────────────────────────────────────────────────────────────────┐
│  SEMANA 2: TRAINING INTENSIVO                                               │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                             │
│  Día 8-9: Training Conceptual (8 horas)                                     │
│  ├── Módulo 1 (2h): Fundamentos del Marco IA-Native                        │
│  │   • Por qué Agile no alcanza con IA                                     │
│  │   • De tareas a decisiones gobernadas                                   │
│  │   • El concepto de Policy Ticket                                        │
│  │                                                                          │
│  ├── Módulo 2 (2h): Niveles de Riesgo y Overhead                           │
│  │   • Matriz de clasificación de dominios                                 │
│  │   • Overhead proporcional                                               │
│  │   • Ejercicio: clasificar 10 cambios típicos                            │
│  │                                                                          │
│  ├── Módulo 3 (2h): Creación de Policy Tickets                             │
│  │   • Anatomía de un buen PT                                              │
│  │   • Ejemplos por nivel de riesgo                                        │
│  │   • Ejercicio: crear PT para caso real                                  │
│  │                                                                          │
│  └── Módulo 4 (2h): Flujo Completo                                         │
│      • De PT a PR: el ciclo completo                                       │
│      • Evidencias y aprobaciones                                           │
│      • Simulación end-to-end                                               │
│                                                                             │
│  Día 10-11: Training Práctico (8 horas)                                     │
│  ├── Hands-on con herramientas configuradas                                │
│  ├── Cada participante crea 3 PTs reales                                   │
│  ├── Feedback inmediato de coach/Tech Lead                                 │
│  └── Q&A abierto                                                           │
│                                                                             │
│  Día 12: Certificación Interna                                              │
│  ├── Quiz de conceptos (20 preguntas)                                      │
│  ├── Ejercicio práctico evaluado                                           │
│  └── Entregable: Equipo piloto certificado                                 │
│                                                                             │
└─────────────────────────────────────────────────────────────────────────────┘
```

#### Semana 3: Operación Supervisada

```
┌─────────────────────────────────────────────────────────────────────────────┐
│  SEMANA 3: OPERACIÓN SUPERVISADA                                            │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                             │
│  Día 15-19: Trabajo Real con Acompañamiento                                 │
│  ├── Equipo piloto trabaja en proyecto real                                │
│  ├── Todos los cambios con Policy Tickets                                  │
│  ├── Coach/Tech Lead revisa cada PT en primeras 24h                        │
│  ├── Daily de 15 min para resolver dudas y ajustar                         │
│  └── Métricas:                                                              │
│      • Nº de PTs creados                                                   │
│      • Tiempo promedio de creación                                         │
│      • Tasa de aprobación primer intento                                   │
│      • Friction points identificados                                       │
│                                                                             │
│  Día 20-21: Retrospectiva Semana 3                                          │
│  ├── Formato: Retro estándar (qué funcionó, qué no, qué ajustar)           │
│  ├── Participantes: Equipo piloto + Sponsor                                │
│  ├── Output: Lista priorizada de ajustes                                   │
│  └── Decisión: Go/No-go para expansión                                     │
│                                                                             │
└─────────────────────────────────────────────────────────────────────────────┘
```

### Fase 2: Expansión Controlada (Días 22-56)

#### Semanas 4-5: Primer Ajuste y Segundo Equipo

```
┌─────────────────────────────────────────────────────────────────────────────┐
│  SEMANAS 4-5: AJUSTE Y EXPANSIÓN                                            │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                             │
│  Día 22-25: Implementar Ajustes de Retro                                    │
│  ├── Responsable: Tech Lead + DevOps                                       │
│  ├── Tareas típicas:                                                        │
│  │   • Simplificar templates que generaron fricción                        │
│  │   • Ajustar umbrales de clasificación de riesgo                         │
│  │   • Automatizar pasos manuales identificados                            │
│  └── Validación con equipo piloto                                          │
│                                                                             │
│  Día 26-28: Onboarding Segundo Equipo                                       │
│  ├── Training condensado (4h - equipo piloto como instructores)            │
│  ├── Buddy system: cada nuevo tiene mentor del piloto                      │
│  └── Arranque en proyecto real                                             │
│                                                                             │
│  Día 29-35: Operación Paralela                                              │
│  ├── 2 equipos operando con framework                                      │
│  ├── Daily cross-equipo (2x/semana) para compartir aprendizajes            │
│  ├── Métricas consolidadas en dashboard                                    │
│  └── Identificación de patrones comunes                                    │
│                                                                             │
└─────────────────────────────────────────────────────────────────────────────┘
```

#### Semanas 6-8: Inclusión de Dominios de Mayor Riesgo

```
┌─────────────────────────────────────────────────────────────────────────────┐
│  SEMANAS 6-8: DOMINIOS DE MAYOR RIESGO                                      │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                             │
│  Día 36-42: Preparación para Dominios Críticos                              │
│  ├── Revisar políticas para dominios alto/crítico                          │
│  ├── Involucrar Security Lead en definición                                │
│  ├── Crear templates específicos (pagos, auth, PII)                        │
│  └── Training adicional para equipos que tocan estos dominios              │
│                                                                             │
│  Día 43-49: Primer Proyecto en Dominio Alto Riesgo                          │
│  ├── Seleccionar cambio acotado pero real                                  │
│  ├── Aplicar proceso completo con todas las evidencias                     │
│  ├── Supervisión cercana de Tech Lead + Security                           │
│  └── Documentar como caso de referencia                                    │
│                                                                             │
│  Día 50-56: Retrospectiva Fase 2                                            │
│  ├── Análisis de métricas acumuladas                                       │
│  ├── Comparación con baseline                                              │
│  ├── Ajustes finales antes de rollout completo                             │
│  └── Preparación de materiales para resto de organización                  │
│                                                                             │
└─────────────────────────────────────────────────────────────────────────────┘
```

### Fase 3: Rollout y Estabilización (Días 57-90)

#### Semanas 9-10: Rollout Organizacional

```
┌─────────────────────────────────────────────────────────────────────────────┐
│  SEMANAS 9-10: ROLLOUT ORGANIZACIONAL                                       │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                             │
│  Día 57: Comunicación de Rollout                                            │
│  ├── Sponsor anuncia extensión a toda la organización                      │
│  ├── Compartir métricas de éxito del piloto                                │
│  ├── Publicar calendario de trainings                                      │
│  └── Responder preguntas frecuentes (FAQ publicado)                        │
│                                                                             │
│  Día 58-63: Training por Oleadas                                            │
│  ├── Oleada 1: Equipos con proyectos iniciando pronto                      │
│  ├── Oleada 2: Equipos con proyectos en curso                              │
│  ├── Oleada 3: Equipos con proyectos finalizando                           │
│  └── Formato: 4h intensivas + buddy del equipo piloto                      │
│                                                                             │
│  Día 64-70: Transición Supervisada                                          │
│  ├── Cada equipo inicia con Policy Tickets en dominios bajos               │
│  ├── Champions disponibles para soporte                                    │
│  ├── Canal de Slack/Teams dedicado para dudas                              │
│  └── Office hours diarias (30 min)                                         │
│                                                                             │
└─────────────────────────────────────────────────────────────────────────────┘
```

#### Semanas 11-12: Estabilización y Certificación

```
┌─────────────────────────────────────────────────────────────────────────────┐
│  SEMANAS 11-12: ESTABILIZACIÓN                                              │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                             │
│  Día 71-77: Monitoreo Intensivo                                             │
│  ├── Dashboard actualizado diariamente                                     │
│  ├── Identificación de equipos con dificultades                            │
│  ├── Intervención proactiva donde métricas no mejoran                      │
│  └── Documentación de edge cases y soluciones                              │
│                                                                             │
│  Día 78-84: Ajustes Finales                                                 │
│  ├── Consolidar feedback de todos los equipos                              │
│  ├── Implementar ajustes de alta prioridad                                 │
│  ├── Publicar versión 1.0 estable de políticas                             │
│  └── Archivar versiones de prueba                                          │
│                                                                             │
│  Día 85-90: Cierre de Fase y Proyección                                     │
│  ├── Retrospectiva organizacional                                          │
│  ├── Reporte ejecutivo con métricas antes/después                          │
│  ├── Celebración de éxito (visible, público)                               │
│  ├── Definición de roadmap Q+1:                                            │
│  │   • Inclusión de dominios pendientes                                    │
│  │   • Automatizaciones adicionales                                        │
│  │   • Preparación para auditoría de compliance                            │
│  └── Transición a modo "business as usual"                                 │
│                                                                             │
└─────────────────────────────────────────────────────────────────────────────┘
```

### Checklist de Entregables por Fase

| Fase | Día | Entregable | Responsable |
|------|-----|------------|-------------|
| 1 | 2 | Kick-off completado y grabado | Sponsor |
| 1 | 5 | Ambiente técnico listo | DevOps |
| 1 | 12 | Equipo piloto certificado | Tech Lead |
| 1 | 21 | Retro Fase 1 + decisión go/no-go | Sponsor |
| 2 | 28 | Segundo equipo operando | Tech Lead |
| 2 | 49 | Primer cambio en dominio crítico | Security Lead |
| 2 | 56 | Retro Fase 2 + métricas | Sponsor |
| 3 | 63 | 100% equipos con training | Tech Lead |
| 3 | 84 | Políticas v1.0 publicadas | Tech Lead |
| 3 | 90 | Reporte ejecutivo + celebración | Sponsor |

### Métricas de Seguimiento del Rollout

**Dashboard de Adopción (actualización diaria en Fase 3):**

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                    DASHBOARD DE ADOPCIÓN - DÍA [X]                          │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                             │
│  Cobertura                                                                  │
│  ├── Equipos con training completado:     [##########] 100%                │
│  ├── Equipos operando con PTs:            [########--]  80%                │
│  └── PRs con PT vinculado (últimos 7d):   [#######---]  72%                │
│                                                                             │
│  Eficiencia                                                                 │
│  ├── Tiempo promedio creación PT:         12 min (target: <15)    ✓        │
│  ├── Tasa aprobación primer intento:      78% (target: >70%)      ✓        │
│  └── PTs rechazados por forma:            8% (target: <10%)       ✓        │
│                                                                             │
│  Calidad                                                                    │
│  ├── Defectos en producción (vs baseline): -45% (target: -30%)    ✓        │
│  ├── Vulnerabilidades pre-merge:           -62% (target: -50%)    ✓        │
│  └── Cobertura de tests:                   68% (target: >70%)     ⚠        │
│                                                                             │
│  Alertas                                                                    │
│  └── ⚠ Equipo Backend-2: 3 días sin PTs - requiere seguimiento             │
│                                                                             │
└─────────────────────────────────────────────────────────────────────────────┘
```

---

## ★ Gestión de la Resistencia al Cambio ★

> **"La resistencia no es el enemigo; es información. Cada objeción es una oportunidad de mejorar la adopción."**

La implementación de cualquier framework de gobernanza genera resistencia. Esta sección proporciona estrategias probadas para anticipar, entender y transformar la resistencia en adopción activa.

### Mapa de Stakeholders y Resistencia Esperada

**Matriz de Stakeholders:**

| Stakeholder | Beneficio Percibido | Costo Percibido | Resistencia Esperada | Estrategia |
|-------------|---------------------|-----------------|---------------------|------------|
| **Desarrolladores Senior** | Menos retrabajo, código más limpio | Overhead de PTs, pérdida de autonomía | Media-Alta | Involucrar en diseño |
| **Desarrolladores Junior** | Guía clara, menos errores | Curva de aprendizaje | Baja-Media | Training + buddy |
| **Tech Leads** | Mejor calidad, menos incidentes | Más aprobaciones, responsabilidad | Media | Empoderarlos como owners |
| **Product Owners** | Menos bugs, compliance | Nuevo rol en aprobaciones | Media | Mostrar ROI |
| **QA** | Herramientas más potentes | Cambio de rol significativo | Media-Alta | Reposicionar como estratégico |
| **Management** | Reducción de costos, compliance | Inversión inicial, riesgo de falla | Baja-Media | Business case claro |
| **Clientes** | Mejor calidad, compliance | Potencial impacto en timelines | Baja | Comunicar valor agregado |

### Objeciones Comunes y Respuestas

#### Objeción 1: "Esto es pura burocracia"

**Síntoma:** Desarrolladores perciben los Policy Tickets como trámite sin valor.

**Respuesta con datos:**
```
"Entiendo la preocupación. Miremos los números:
- Sin gobernanza, el 70% de los PRs con IA requieren retrabajo
- Cada retrabajo cuesta en promedio 2 horas
- Un PT de bajo riesgo toma 7 minutos

Matemática simple:
- Sin PT: 70% × 2h = 84 minutos de retrabajo promedio
- Con PT: 7 minutos + 30% × 30min = 16 minutos total

El 'overhead' en realidad es ahorro neto."
```

**Acción:** Mostrar dashboard con métricas de retrabajo antes/después.

#### Objeción 2: "Vamos a ser más lentos"

**Síntoma:** Preocupación por impacto en velocidad de entrega.

**Respuesta con reframe:**
```
"Velocidad de qué exactamente?
- Velocidad de escribir código: Sí, marginalmente más lento
- Velocidad de entregar valor: Significativamente más rápido

Datos del piloto:
- Tiempo de PR a merge: -57% (de 4.2 días a 1.8 días)
- Features en producción sin rollback: +80%

Ir más 'lento' en un paso para ir más rápido en el total
es exactamente lo que hacemos con tests. Esto es lo mismo."
```

**Acción:** Tracking visible de lead time end-to-end, no solo tiempo de codificación.

#### Objeción 3: "La IA debería hacer las cosas más fáciles, no más difíciles"

**Síntoma:** Frustración porque la promesa de IA era "simplificar".

**Respuesta con analogía:**
```
"Un auto te permite llegar más lejos y más rápido.
Pero también requiere licencia de conducir, seguro, y reglas de tránsito.

¿Diríamos que las reglas de tránsito hacen el auto 'más difícil'?
No. Hacen que el poder del auto sea utilizable de forma segura.

El framework es las 'reglas de tránsito' para la IA.
Sin ellas, el poder de la IA es un riesgo, no un beneficio."
```

**Acción:** Workshop de "horror stories" de IA sin gobernanza (anónimos, de la industria).

#### Objeción 4: "Esto no aplica a mi equipo/proyecto"

**Síntoma:** Percepción de que las reglas son genéricas y no se adaptan al contexto.

**Respuesta con participación:**
```
"Tienes razón en que cada contexto es diferente.
Por eso el framework tiene niveles de riesgo diferenciados.

Propuesta: En las próximas 2 semanas, documenten 5 casos
donde sientan que el framework no aplica bien.
Los revisamos juntos y ajustamos las políticas.

El framework está diseñado para evolucionar con feedback real."
```

**Acción:** Crear mecanismo formal de feedback y actuar visiblemente sobre él.

#### Objeción 5: "Ya intentamos algo así y falló"

**Síntoma:** Escepticismo basado en experiencias previas negativas.

**Respuesta con diferenciación:**
```
"Cuéntame más sobre qué se intentó y qué falló.
[Escuchar activamente]

Basado en lo que describes, las diferencias clave son:
1. [Diferencia específica 1]
2. [Diferencia específica 2]
3. [Diferencia específica 3]

Además, estamos haciendo piloto primero específicamente
para aprender y ajustar antes de escalar.
Si después de 3 semanas de piloto no vemos mejoras,
revisamos el approach."
```

**Acción:** Documentar explícitamente qué es diferente esta vez.

### Estrategias de Transformación de Resistencia

#### Estrategia 1: Early Adopters como Embajadores

**Implementación:**
1. Identificar 2-3 personas influyentes que estén dispuestas a probar
2. Darles acceso anticipado y atención personalizada
3. Documentar sus éxitos y frustraciones
4. Convertirlos en voceros internos

**Ejemplo de comunicación de early adopter:**
```
"Al principio pensé que era otra iniciativa que no iba a durar.
Pero después de 2 semanas, me di cuenta de que:
- Paso menos tiempo en code reviews porque el PT ya aclara el contexto
- Los PRs que hago se mergean en horas, no días
- No he tenido que rehacer nada por falta de especificación

Ahora no volvería al modelo anterior."
— Testimonio de [Nombre], Desarrollador Senior
```

#### Estrategia 2: Quick Wins Visibles

**Implementación:**
1. Seleccionar 3-5 métricas que van a mejorar rápido
2. Crear dashboard público actualizado diariamente
3. Celebrar cada mejora, aunque sea pequeña
4. Vincular mejoras a personas específicas ("gracias al equipo X por...")

**Ejemplo de quick win:**
```
┌───────────────────────────────────────┐
│  🎉 QUICK WIN DE LA SEMANA 🎉        │
├───────────────────────────────────────┤
│                                       │
│  Vulnerabilidades críticas            │
│  detectadas PRE-merge:                │
│                                       │
│  Semana pasada:    0                  │
│  Esta semana:      3                  │
│                                       │
│  = 3 potenciales incidentes           │
│    evitados en producción             │
│                                       │
│  Gracias equipo Backend por           │
│  adoptar los checks de seguridad!     │
│                                       │
└───────────────────────────────────────┘
```

#### Estrategia 3: Reducción Progresiva de Fricción

**Implementación:**
1. Comenzar con la versión más simple posible del framework
2. Recoger feedback sistemáticamente
3. Cada semana, eliminar o automatizar 1 punto de fricción
4. Comunicar cada reducción de fricción visiblemente

**Ejemplo de comunicación:**
```
"Escuchamos su feedback: crear PTs para cambios en tests
estaba tomando demasiado tiempo.

A partir de hoy:
- Tests en carpeta /tests/ → auto-clasificados como bajo riesgo
- Template simplificado de 3 campos (antes 8)
- Auto-aprobación si todos los checks pasan

Tiempo estimado por PT de tests: 2 minutos (antes 10)

Seguimos escuchando. ¿Qué más podemos simplificar?"
```

#### Estrategia 4: Opt-out Temporal con Costo Visible

**Implementación:**
Para casos de resistencia extrema, ofrecer opt-out temporal con condiciones:
1. El equipo puede no usar PTs por 2 semanas
2. Pero debe trackear manualmente: retrabajo, incidentes, tiempo de merge
3. Al final de 2 semanas, comparar métricas
4. Decisión basada en datos

**Resultado típico:** La mayoría de equipos con opt-out terminan adoptando voluntariamente al ver las métricas.

### Plan de Comunicación para Gestión de Cambio

**Calendario de Comunicaciones:**

| Timing | Mensaje | Canal | Responsable |
|--------|---------|-------|-------------|
| D-14 | Anuncio de iniciativa + por qué | All-hands | Sponsor |
| D-7 | FAQ publicado | Wiki/Confluence | Tech Lead |
| D-1 | Reminder + expectativas | Email + Slack | Sponsor |
| D+7 | Primeros resultados del piloto | Slack | Tech Lead |
| D+14 | Testimonio de early adopter | All-hands | Sponsor |
| D+21 | Métricas de Fase 1 | Dashboard | Tech Lead |
| Semanal | Quick wins y ajustes | Slack | Champion |
| D+90 | Celebración + resultados finales | All-hands + evento | Sponsor |

**Template de Comunicación Semanal:**

```
📊 ACTUALIZACIÓN SEMANAL - FRAMEWORK IA-NATIVE

✅ Lo que funcionó esta semana:
- [Logro 1]
- [Logro 2]

🔧 Lo que ajustamos basado en su feedback:
- [Ajuste 1]
- [Ajuste 2]

📈 Métricas clave:
- PRs con PT: XX% (target: YY%)
- Tiempo promedio PT: XX min
- Defectos evitados: XX

❓ Pregunta frecuente de la semana:
P: [Pregunta]
R: [Respuesta]

🎯 Foco de la próxima semana:
- [Prioridad 1]
- [Prioridad 2]

¿Feedback? Responde a este mensaje o usa #framework-feedback
```

### Indicadores de Salud de la Adopción

**Semáforo de Adopción:**

| Indicador | 🟢 Saludable | 🟡 Atención | 🔴 Crítico |
|-----------|-------------|-------------|-----------|
| % PRs con PT | >80% | 60-80% | <60% |
| Satisfacción equipo (NPS) | >40 | 20-40 | <20 |
| Tiempo PT vs target | <120% | 120-150% | >150% |
| Workarounds detectados | 0 | 1-2 | >2 |
| Preguntas sin responder | <24h | 24-48h | >48h |
| Feedback implementado | >50% | 30-50% | <30% |

**Acciones por Estado:**

- **🟢 Saludable:** Continuar plan, celebrar éxitos
- **🟡 Atención:** Investigar causa raíz, ajustar comunicación, aumentar soporte
- **🔴 Crítico:** Intervención inmediata, reunión con equipo afectado, potencial pausa

---

## ★ Dashboard y Métricas de Adopción del Framework ★

> **"Lo que se mide, se gestiona. Lo que se muestra, se mejora."**

Esta sección define las métricas específicas para monitorear la salud de la adopción del framework y los dashboards recomendados para visualizarlas.

### Métricas de Adopción (Leading Indicators)

Estas métricas predicen el éxito futuro de la adopción:

| Métrica | Definición | Cómo Medir | Target |
|---------|------------|------------|--------|
| **Cobertura de Training** | % del equipo que completó capacitación | Registro de asistencia | 100% en D+30 |
| **Tasa de Vinculación PT-PR** | % de PRs que tienen PT asociado | Automatizado via CI | >95% en D+60 |
| **Velocidad de Creación de PT** | Tiempo promedio para crear un PT | Timestamps en sistema | <15 min (bajo riesgo) |
| **Tasa de Aprobación Primer Intento** | % de PTs aprobados sin modificación | Sistema de workflow | >70% |
| **Engagement con Dashboard** | Visitas únicas al dashboard de métricas | Analytics | >50% equipo/semana |
| **Volumen de Feedback** | Número de sugerencias de mejora | Canal de feedback | >5/semana (primeras 8 sem) |

### Métricas de Resultado (Lagging Indicators)

Estas métricas confirman si la adopción está generando valor:

| Métrica | Definición | Cómo Medir | Target |
|---------|------------|------------|--------|
| **Tasa de Retrabajo** | % de PRs que requieren cambios post-merge | Git history + issues | <15% (vs baseline) |
| **Defectos en Producción** | Bugs críticos/altos por mes | Sistema de issues | -50% vs baseline |
| **Incidentes de Seguridad** | Vulnerabilidades explotadas | Security logs | -80% vs baseline |
| **Lead Time de Entrega** | Tiempo de commit a producción | CI/CD metrics | -30% vs baseline |
| **NPS del Equipo** | Satisfacción con proceso de desarrollo | Encuesta mensual | >50 |
| **Rotación** | % de salidas voluntarias anualizadas | HR data | <15% |

### Diseño de Dashboard Principal

```
┌─────────────────────────────────────────────────────────────────────────────────────────┐
│                         DASHBOARD IA-NATIVE GOVERNANCE                                   │
│                              Última actualización: [fecha] [hora]                        │
├─────────────────────────────────────────────────────────────────────────────────────────┤
│                                                                                         │
│  ┌─────────────────────┐  ┌─────────────────────┐  ┌─────────────────────┐             │
│  │   ADOPCIÓN GLOBAL   │  │  POLICY TICKETS     │  │    CALIDAD          │             │
│  │                     │  │                     │  │                     │             │
│  │       94%           │  │      247           │  │      -62%          │             │
│  │   PRs gobernados    │  │   activos hoy      │  │   defectos vs Q-1   │             │
│  │   ↑ 12% vs sem ant  │  │   ↑ 8 vs ayer      │  │   ↓ mejorando       │             │
│  └─────────────────────┘  └─────────────────────┘  └─────────────────────┘             │
│                                                                                         │
│  DISTRIBUCIÓN POR NIVEL DE RIESGO                    TENDENCIA DE ADOPCIÓN (8 sem)     │
│  ┌──────────────────────────────────┐               ┌──────────────────────────────┐   │
│  │                                  │               │ 100%|            ___________│   │
│  │  BAJO    ████████████████  68%   │               │     |        ___/           │   │
│  │  MEDIO   ████████         24%   │               │  50%|    ___/               │   │
│  │  ALTO    ████              6%   │               │     |___/                   │   │
│  │  CRÍTICO ██                2%   │               │   0%|_______________________|   │
│  │                                  │               │     S1  S2  S3  S4  S5  S6  S7 │
│  └──────────────────────────────────┘               └──────────────────────────────┘   │
│                                                                                         │
│  MÉTRICAS POR EQUIPO                                                                    │
│  ┌─────────────────────────────────────────────────────────────────────────────────┐   │
│  │ Equipo          │ PRs con PT │ Tiempo PT │ Tasa Aprob │ Defectos │  Estado      │   │
│  ├─────────────────┼────────────┼───────────┼────────────┼──────────┼──────────────┤   │
│  │ Frontend-1      │    98%     │   8 min   │    82%     │    1     │     🟢       │   │
│  │ Backend-Core    │    96%     │  12 min   │    75%     │    0     │     🟢       │   │
│  │ Backend-API     │    91%     │  15 min   │    71%     │    2     │     🟢       │   │
│  │ Mobile          │    87%     │  18 min   │    68%     │    1     │     🟡       │   │
│  │ Data            │    72%     │  25 min   │    58%     │    3     │     🔴       │   │
│  └─────────────────────────────────────────────────────────────────────────────────┘   │
│                                                                                         │
│  ALERTAS ACTIVAS                                                                        │
│  ┌─────────────────────────────────────────────────────────────────────────────────┐   │
│  │ 🔴 Equipo Data: <80% cobertura - agendar revisión con Tech Lead                 │   │
│  │ 🟡 3 PTs críticos pendientes aprobación >24h - escalar a Security               │   │
│  │ 🟡 Tiempo promedio PT subió 15% - revisar templates                             │   │
│  └─────────────────────────────────────────────────────────────────────────────────┘   │
│                                                                                         │
│  QUICK STATS                                                                            │
│  ├── Vulnerabilidades bloqueadas pre-merge esta semana: 12                             │
│  ├── Horas de retrabajo evitadas (estimado): 48h                                       │
│  └── Ahorro estimado esta semana: $4,800                                               │
│                                                                                         │
└─────────────────────────────────────────────────────────────────────────────────────────┘
```

### Dashboard de Equipo Individual

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                    DASHBOARD EQUIPO: [NOMBRE]                                │
│                         Sprint: [número] | Semana: [X]                       │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                             │
│  MIS POLICY TICKETS ACTIVOS                                                 │
│  ┌─────────────────────────────────────────────────────────────────────┐   │
│  │ PT-2026-0891 │ Refactor auth module  │ ALTO   │ En Review │ @maria  │   │
│  │ PT-2026-0892 │ Add user preferences  │ MEDIO  │ Aprobado  │ @juan   │   │
│  │ PT-2026-0893 │ Fix date validation   │ BAJO   │ Merged    │ @pedro  │   │
│  └─────────────────────────────────────────────────────────────────────┘   │
│                                                                             │
│  MÉTRICAS DEL EQUIPO (últimos 14 días)                                      │
│  ┌───────────────────────────┐  ┌───────────────────────────┐              │
│  │ PRs Creados         23    │  │ Defectos Producción   1   │              │
│  │ PRs con PT          22    │  │ (vs promedio org: 2.3)    │              │
│  │ Cobertura          96%    │  │ Estado: 🟢 Mejor que avg  │              │
│  └───────────────────────────┘  └───────────────────────────┘              │
│                                                                             │
│  DISTRIBUCIÓN DE TIEMPO                                                     │
│  ┌─────────────────────────────────────────────────────────────────────┐   │
│  │ Creación PTs:     ████                    12%                       │   │
│  │ Desarrollo:       ████████████████████████ 65%                      │   │
│  │ Code Review:      ██████                  18%                       │   │
│  │ Retrabajo:        ██                       5%                       │   │
│  └─────────────────────────────────────────────────────────────────────┘   │
│  Comparación: Retrabajo era 18% antes del framework                        │
│                                                                             │
│  APRENDIZAJES DE LA SEMANA (auto-generados)                                 │
│  ├── ✓ 3 vulnerabilidades bloqueadas en PR #456 por SAST                   │
│  ├── ✓ PT-0891 requirió 2 revisiones - considerar split en futuro          │
│  └── ✓ Nuevo patrón de test documentado en PR #458                         │
│                                                                             │
└─────────────────────────────────────────────────────────────────────────────┘
```

### Implementación Técnica del Dashboard

**Stack Recomendado:**

| Componente | Herramienta | Alternativa |
|------------|-------------|-------------|
| Recolección de datos | GitHub Actions + Webhooks | GitLab CI |
| Almacenamiento | PostgreSQL | BigQuery |
| Visualización | Grafana | Metabase, Superset |
| Alertas | Grafana Alerting | PagerDuty |

**Queries Clave:**

```sql
-- Tasa de vinculación PT-PR (últimos 7 días)
SELECT
    DATE(pr.created_at) as date,
    COUNT(*) as total_prs,
    SUM(CASE WHEN pt.id IS NOT NULL THEN 1 ELSE 0 END) as prs_with_pt,
    ROUND(100.0 * SUM(CASE WHEN pt.id IS NOT NULL THEN 1 ELSE 0 END) / COUNT(*), 2) as coverage_pct
FROM pull_requests pr
LEFT JOIN policy_tickets pt ON pr.pt_reference = pt.id
WHERE pr.created_at > NOW() - INTERVAL '7 days'
GROUP BY DATE(pr.created_at)
ORDER BY date;

-- Tiempo promedio de creación de PT por nivel de riesgo
SELECT
    risk_level,
    AVG(EXTRACT(EPOCH FROM (submitted_at - created_at))/60) as avg_creation_minutes,
    PERCENTILE_CONT(0.5) WITHIN GROUP (ORDER BY EXTRACT(EPOCH FROM (submitted_at - created_at))/60) as median_minutes
FROM policy_tickets
WHERE created_at > NOW() - INTERVAL '30 days'
GROUP BY risk_level;

-- Defectos por equipo (comparación con baseline)
SELECT
    t.name as team,
    COUNT(CASE WHEN d.created_at > NOW() - INTERVAL '30 days' THEN 1 END) as defects_current,
    COUNT(CASE WHEN d.created_at BETWEEN NOW() - INTERVAL '60 days' AND NOW() - INTERVAL '30 days' THEN 1 END) as defects_previous,
    ROUND(100.0 * (
        COUNT(CASE WHEN d.created_at > NOW() - INTERVAL '30 days' THEN 1 END) -
        COUNT(CASE WHEN d.created_at BETWEEN NOW() - INTERVAL '60 days' AND NOW() - INTERVAL '30 days' THEN 1 END)
    ) / NULLIF(COUNT(CASE WHEN d.created_at BETWEEN NOW() - INTERVAL '60 days' AND NOW() - INTERVAL '30 days' THEN 1 END), 0), 2) as change_pct
FROM teams t
LEFT JOIN defects d ON d.team_id = t.id
GROUP BY t.name;
```

**GitHub Action para Recolección:**

```yaml
name: Collect Governance Metrics
on:
  schedule:
    - cron: '0 */6 * * *'  # Cada 6 horas
  workflow_dispatch:

jobs:
  collect-metrics:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4

      - name: Collect PR Metrics
        env:
          GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: |
          # PRs de los últimos 7 días
          gh pr list --state all --limit 500 --json number,title,body,createdAt,mergedAt,labels \
            | jq '[.[] | select(.createdAt > (now - 7*24*3600 | todate))]' \
            > pr_metrics.json

          # Extraer PT references
          cat pr_metrics.json | jq '[.[] | {
            pr_number: .number,
            has_pt: ((.title + " " + .body) | test("PT-[0-9]{4}-[0-9]+"; "i")),
            pt_id: ((.title + " " + .body) | capture("(?<pt>PT-[0-9]{4}-[0-9]+)"; "i").pt // null)
          }]' > pt_coverage.json

      - name: Push to Database
        run: |
          # Insertar en PostgreSQL/BigQuery
          ./scripts/push_metrics.sh pt_coverage.json
```

Este conjunto de dashboards y métricas proporciona visibilidad completa sobre la salud de la adopción, permitiendo intervenciones tempranas cuando se detectan problemas y celebración visible cuando se alcanzan metas.

---

# PARTE VI: CONTEXTO ORGANIZACIONAL Y LEGAL

## ★ Marco Regulatorio y Compliance: El Panorama Legal de 2026 ★

> **"La gobernanza ya no es opcional. Entre el EU AI Act, el Colorado AI Act y los requisitos estatales proliferantes, las políticas formalizadas de IA han pasado de mejor práctica a obligación de cumplimiento."**

El marco IA-Native no opera en un vacío legal. Para 2026, el panorama regulatorio ha madurado significativamente, y las organizaciones que ignoren estos requisitos enfrentan riesgos legales, financieros y reputacionales sustanciales. Esta sección mapea el terreno regulatorio actual y establece cómo el marco IA-Native se alinea con las obligaciones legales emergentes.

### El Estado Global de la Regulación de IA

A 2026, aproximadamente 90 países han establecido estrategias nacionales de IA o marcos de gobernanza formales, y al menos 33 países han promulgado legislación vinculante específica de IA. Este no es un fenómeno futuro; es la realidad operativa presente.

**Regulaciones Clave en Vigor:**

| Regulación | Jurisdicción | Fecha de Entrada | Impacto |
|------------|--------------|------------------|---------|
| **EU AI Act** | Unión Europea | Agosto 2026 | Obligatorio para empresas con clientes en UE; aplica incluso a empresas no-UE |
| **Colorado AI Act** | Estados Unidos | Junio 2026 | Primera ley estatal vinculante de IA en EEUU |
| **Texas TRAIGA** | Estados Unidos | 2026 | Requisitos específicos de transparencia algorítmica |
| **NIST AI RMF** | Estados Unidos | Vigente | Marco de referencia federal (no vinculante pero influyente) |
| **ISO/IEC 42001** | Internacional | Vigente | Estándar de gestión de sistemas de IA |

### La Fragmentación Regulatoria: El Desafío Central

Mientras la Unión Europea opera bajo un marco de gobernanza de IA centralizado que incluso aplica a algunas empresas no europeas, Estados Unidos continúa dependiendo de un enfoque fragmentado. Las leyes estatales como Texas TRAIGA y el Colorado AI Act son intentos genuinos de proteger a los ciudadanos, pero son parciales e inconsistentes, sin una base compartida ni mecanismo inter-jurisdiccional.

A nivel federal estadounidense, la Casa Blanca ha emitido órdenes ejecutivas y guías, pero el Congreso aún no ha aprobado legislación vinculante. Esta brecha deja a agencias como la FTC, NIST y el Departamento de Comercio interpretando el cumplimiento regulatorio de IA dentro de sus mandatos existentes, sin un marco legal unificado.

**Implicación práctica:** Las software factories que operan globalmente deben cumplir simultáneamente con múltiples jurisdicciones, cada una con requisitos distintos y a veces contradictorios.

### El EU AI Act: Anatomía de la Regulación Más Comprehensiva

El EU AI Act representa la regulación de IA más ambiciosa y detallada del mundo. Su sistema de clasificación por riesgo tiene implicaciones directas para el desarrollo de software:

**Clasificación de Riesgo según EU AI Act:**

| Nivel de Riesgo | Ejemplos | Requisitos |
|-----------------|----------|------------|
| **Inaceptable** | Scoring social, manipulación subliminal | Prohibido |
| **Alto** | Sistemas de empleo, crédito, infraestructura crítica | Registro, auditoría, supervisión humana obligatoria |
| **Limitado** | Chatbots, deepfakes | Obligaciones de transparencia |
| **Mínimo** | Filtros de spam, videojuegos | Sin requisitos específicos |

**Mapeo con Policy Tickets:**

El marco IA-Native se alinea naturalmente con los requisitos del EU AI Act:

| Requisito EU AI Act | Implementación en IA-Native |
|---------------------|----------------------------|
| Supervisión humana | Responsable humano explícito en cada Policy Ticket |
| Trazabilidad | Vínculo obligatorio PR ↔ Policy Ticket |
| Documentación técnica | Evidencias requeridas antes del merge |
| Gestión de riesgos | Clasificación de dominio (bajo/medio/alto) |
| Auditoría | GitHub como registro probatorio |

### Responsabilidad Legal: ¿Quién Responde Cuando la IA Falla?

Uno de los vacíos legales más significativos es la responsabilidad por daños causados por sistemas de IA autónomos. Las preguntas son complejas y las respuestas aún no están completamente definidas en la jurisprudencia.

**La Cadena de Responsabilidad:**

Cuando un agente de IA introduce un cambio que causa un fallo en producción, la responsabilidad puede distribuirse entre múltiples actores:

1. **El desarrollador que aprobó el merge** - Responsabilidad directa por la decisión de integración
2. **El Tech Lead que habilitó el uso del agente** - Responsabilidad por la política que permitió la acción
3. **La organización que definió el proceso** - Responsabilidad institucional por el marco de gobernanza
4. **El vendor del modelo de IA** - Responsabilidad contractual según términos de servicio

**Precedente Crítico - EEOC y Discriminación Algorítmica:**

La Equal Employment Opportunity Commission (EEOC) de Estados Unidos ha dejado claro que los empleadores son totalmente responsables bajo el Title VII cuando herramientas impulsadas por IA producen resultados discriminatorios. Si un algoritmo genera impacto desproporcionado sobre clases protegidas, la responsabilidad se atribuye independientemente de si la herramienta fue desarrollada internamente o adquirida de un vendor externo.

**Implicación para el marco IA-Native:** La documentación explícita de responsabilidad en cada Policy Ticket no es solo buena práctica; es protección legal. La capacidad de demostrar quién autorizó qué, cuándo, y bajo qué condiciones, es el "firewall de responsabilidad" que las organizaciones necesitan.

### Cláusulas Contractuales con Proveedores de IA

Las organizaciones deben revisar y negociar cláusulas específicas con proveedores de IA:

**Cláusulas Críticas a Incluir:**

| Área | Cláusula Recomendada |
|------|---------------------|
| **Responsabilidad** | Definición clara de quién asume responsabilidad por outputs defectuosos |
| **Derechos de Auditoría** | Capacidad de auditar el modelo y sus decisiones |
| **Derechos de Datos** | Propiedad y uso de datos de entrenamiento y outputs |
| **Derechos de Modelo** | Acceso a información sobre el modelo, versiones, cambios |
| **Indemnización** | Protección contra reclamos de terceros por IP o daños |

**Nota importante:** Las protecciones contractuales solas son insuficientes sin gobernanza operativa y testing. El contrato es la última línea de defensa, no la primera.

### Compliance como Proceso Continuo

La implementación de gobernanza de IA requiere un enfoque por fases que construya capacidad progresivamente:

**Fase 1: Descubrimiento y Evaluación**
- Inventariar iniciativas de IA existentes
- Identificar brechas de gobernanza
- Priorizar áreas de enfoque inicial basándose en niveles de riesgo y exposición regulatoria

**Fase 2: Diseño de Políticas**
- Desarrollar políticas alineadas con regulaciones aplicables
- Mapear Policy Tickets a requisitos regulatorios
- Establecer mecanismos de auditoría

**Fase 3: Implementación Operativa**
- Integrar controles en pipelines de CI/CD
- Entrenar equipos en requisitos de compliance
- Establecer monitoreo continuo

**Fase 4: Mejora Continua**
- Auditorías regulares
- Ajuste de políticas según evolución regulatoria
- Documentación de lecciones aprendidas

### Proyección: Lo que Viene

Para 2026 y más allá, Gartner proyecta que la mitad de los gobiernos mundiales esperarán que las empresas adhieran a leyes, regulaciones y requisitos de privacidad de datos de IA que aseguren el uso seguro y responsable de la inteligencia artificial.

Las organizaciones que implementen el marco IA-Native ahora estarán mejor posicionadas para cumplir con regulaciones futuras, ya que el marco ya incorpora los principios fundamentales que las regulaciones buscan: supervisión humana, trazabilidad, gestión de riesgos y responsabilidad explícita.

---

## ★ Propiedad Intelectual del Código Generado por IA ★

> **"Cuando el código es producido únicamente por IA, las empresas no pueden obtener protección de copyright sobre ese código. Porque dicho output no es elegible para copyright, puede ser usado libremente por cualquiera."**

La cuestión de la propiedad intelectual del código generado por IA representa uno de los vacíos legales más significativos y menos comprendidos en el desarrollo de software moderno. Esta sección examina el estado actual de la ley, los riesgos para las organizaciones, y las estrategias de mitigación.

### El Principio Fundamental: La Autoría Humana

En Estados Unidos, la protección de copyright depende de la autoría humana. Tanto la jurisprudencia como la Oficina de Copyright de EEUU coinciden en que la protección de copyright está disponible únicamente para obras creadas a través de creatividad humana. Las obras predominantemente generadas por IA, sin autoría humana significativa, no son elegibles para protección de copyright.

**Consecuencia directa:** Si una empresa genera código usando únicamente un agente de IA sin participación humana sustancial, ese código:
- No puede ser registrado para copyright
- Puede ser usado por cualquier competidor sin restricción legal
- Solo puede protegerse mediante otros mecanismos (secreto comercial, patentes si aplica)

### Riesgos Legales Específicos

**1. Contaminación de Licencias**

Aproximadamente el 35% de las muestras de código generado por IA contienen irregularidades de licencias que podrían crear responsabilidad legal. Los modelos de IA fueron entrenados con código de múltiples fuentes, incluyendo código open source con diversas licencias (GPL, MIT, Apache, etc.). Cuando un agente genera código, puede incorporar fragmentos que retienen obligaciones de licencia de su fuente original.

**Escenario de riesgo:** Un agente genera código que inadvertidamente incluye fragmentos con licencia GPL. Si la empresa incorpora este código en un producto propietario sin cumplir con las obligaciones de GPL (distribución del código fuente), enfrenta riesgo de litigio.

**2. Infracción de Copyright**

Más allá de las licencias, existe el riesgo de que el código generado reproduzca literalmente código protegido por copyright. Los tribunales aún deben decidir definitivamente si los vendors de IA pueden usar material protegido por copyright para entrenar sus modelos.

**Estado actual:** "A nivel del modelo de lenguaje grande, ya existe un problema de copyright que aún no se ha resuelto." Los problemas legales probablemente no desaparecerán pronto.

**3. Derechos de Propiedad Inciertos**

Cuando un desarrollador usa herramientas de IA para generar código, ¿quién es el dueño?

| Escenario | Probable Propietario | Riesgo |
|-----------|---------------------|--------|
| Código 100% generado por IA | Nadie (dominio público) | Alto - sin protección |
| IA genera, humano modifica sustancialmente | Humano/Empresa | Bajo - si se documenta participación |
| IA asiste, humano dirige y escribe mayormente | Humano/Empresa | Muy bajo |

### La Excepción: Colaboración Humano-IA

La Oficina de Copyright enfatiza la distinción entre obras asistidas por IA y contenido generado enteramente por IA. Cuando desarrolladores humanos participan sustancialmente en el proceso de creación, la protección de copyright puede seguir disponible.

**Criterios para protección:**
- El humano proporciona dirección creativa significativa
- El humano selecciona, arregla o modifica el output de IA
- El humano contribuye expresión original al trabajo final
- La contribución de IA es una herramienta, no el autor

### Estrategias de Mitigación Empresarial

**Enfoque Microsoft:**
Combina supervisión humana con herramientas sofisticadas de detección de licencias que escanean código generado por IA para problemas potenciales de infracción antes de la integración.

**Enfoque Google:**
Mantiene directrices internas estrictas sobre qué componentes pueden desarrollarse usando asistencia de IA mientras restringe su uso en desarrollo de propiedad intelectual core.

**Recomendaciones Prácticas:**

1. **Escaneo de Código**
   - Implementar herramientas que verifiquen código generado por IA para infracción de copyright y violaciones de licencias open source
   - Esto puede hacerse por programadores humanos o por servicios especializados de verificación de IP

2. **Documentación de Participación Humana**
   - Registrar la contribución humana en cada Policy Ticket
   - Documentar modificaciones, decisiones de diseño, y selecciones realizadas por humanos
   - Mantener evidencia de que el output final refleja creatividad humana sustancial

3. **Políticas de Uso por Dominio**
   - Restringir uso de IA en código que constituye IP core
   - Permitir uso más libre en código utilitario, tests, documentación
   - Definir claramente qué dominios permiten qué nivel de asistencia de IA

4. **Protección mediante Secreto Comercial**
   - Para código generado por IA que no puede protegerse con copyright
   - Implementar medidas de confidencialidad
   - Limitar acceso y distribución

5. **Cláusulas de Indemnización con Vendors**
   - Negociar protección contra reclamos de terceros
   - Vendors como Adobe y Microsoft se comprometen a defender a clientes si el uso de sus soluciones genera problemas legales

### Integración con Policy Tickets

El marco IA-Native debe incorporar consideraciones de IP en cada Policy Ticket:

**Campos Adicionales Sugeridos:**

```
PROPIEDAD INTELECTUAL:
├── Nivel de participación humana: [Alto/Medio/Bajo]
├── Modificaciones humanas documentadas: [Sí/No]
├── Escaneo de licencias completado: [Sí/No]
├── Dominio de IP: [Core/Utilitario/Test/Doc]
└── Protección aplicable: [Copyright/Secreto comercial/Ninguna]
```

### La Realidad Operativa

Las organizaciones deben aceptar que, en el estado actual de la ley:

1. El código generado puramente por IA probablemente no tiene protección de copyright
2. La participación humana significativa es el factor clave para obtener protección
3. Los riesgos de contaminación de licencias son reales y deben gestionarse activamente
4. La incertidumbre legal persistirá durante años mientras los tribunales y legisladores resuelven estas cuestiones

El marco IA-Native, con su énfasis en responsabilidad humana explícita y documentación de decisiones, proporciona una base sólida para argumentar participación humana sustancial cuando sea necesario demostrarla legalmente.

---

## ★ Economía de Tokens y FinOps para IA ★

> **"A diferencia de olas previas de inversión tecnológica, el gasto en IA es volátil y no lineal. Los costos de IA no solo aumentan con la adopción, sino con la intensidad de infraestructura. Esta nueva realidad trata sobre navegar la economía de IA con precisión: rastrear, predecir y optimizar el uso de tokens."**

El marco IA-Native no puede implementarse de manera sostenible sin comprender la economía subyacente del uso de agentes de IA. Esta sección examina los modelos de costos, métricas de ROI, y prácticas de FinOps específicas para entornos de desarrollo asistido por IA.

### Comprendiendo la Economía de Tokens

Un token es la unidad fundamental del trabajo de IA—un pequeño fragmento de datos: caracteres de texto, piezas de una imagen, o una porción de audio que un modelo de IA procesa. Las aplicaciones funcionan con tokens, y cada interacción—ya sea entrenamiento, inferencia o razonamiento—se mide en tokens.

**La Nueva Realidad Económica:**

| Aspecto | SaaS Tradicional | IA-First SaaS |
|---------|------------------|---------------|
| Márgenes brutos | 78-85% | 55-70% |
| COGS variable por usuario | <5% de ingresos | 20-40% de ingresos |
| Predictibilidad de costos | Alta | Baja-Media |
| Escalabilidad de costos | Lineal | No lineal |

Para una aplicación B2B procesando 50 millones de tokens mensuales por cliente empresarial, los costos de inferencia solos pueden alcanzar $500-2,000 mensuales por cliente.

### Modelos de Pricing en 2026

**1. Pricing Basado en Tokens (API y Consumo de Modelos)**

Proveedores como Anthropic, OpenAI y Together AI hacen los tokens explícitos. Cada consulta es medida, facturada y expuesta.

*Ventaja:* Transparencia total
*Desventaja:* Volatilidad—los costos varían según diseño de carga de trabajo, longitud de prompts, y decisiones ocultas de proveedores de infraestructura

**2. Soluciones de IA Empaquetadas (Subscripción)**

Productos como Agentforce, ServiceNow Copilot, o Workday AI abstraen los tokens completamente. Los líderes ven una tarifa predecible por subscripción o por asiento, pero poca transparencia en eficiencia de consumo de tokens.

*Ventaja:* Predictibilidad
*Desventaja:* Riesgo de sobrepagar por simplicidad

**3. Pricing Basado en Resultados**

Modelos que vinculan ingresos a métricas que el cliente ya rastrea: reuniones agendadas, facturas cobradas, fraude prevenido, tickets automatizados resueltos.

*Ejemplo:* Intercom permite a clientes comprar su Fin AI Agent y simplemente cobra $0.99 cada vez que Fin resuelve completamente un problema del cliente.

*Ventaja:* Alineación de incentivos
*Desventaja:* Complejidad de medición

### El Costo Total de Propiedad (TCO)

El TCO de agentes de IA tiene múltiples dimensiones:

**Dimensión 1: Costos de Modelo y Uso**
- Tarifas de inferencia vinculadas a consumo de tokens
- Pricing por resolución cuando aplica
- Costos de fine-tuning y entrenamiento personalizado

**Dimensión 2: Infraestructura e Integración**
- Cómputo cloud, transferencia de datos, almacenamiento
- Ejecución en tiempo real
- Integraciones API conectando agentes a CRM, ERP y otros sistemas

**Dimensión 3: Costos Humanos**
- Tiempo de supervisión y validación
- Entrenamiento de equipos
- Gestión de gobernanza

**Dimensión 4: Costos de Calidad**
- Retrabajo de código defectuoso
- Remediación de vulnerabilidades de seguridad
- Gestión de deuda técnica inducida por IA

### La Paradoja del Costo-Beneficio

Un informe de McKinsey indica que el 62% de las organizaciones usando servicios de IA basados en tokens experimentaron al menos un mes de sobrecostos inesperados en su primer año de implementación.

**Factores que causan volatilidad:**
- Prompts no optimizados que consumen tokens innecesarios
- Loops de razonamiento extensos en modelos agénticos
- Retries por errores o resultados insatisfactorios
- Crecimiento de uso no anticipado

### Prácticas de FinOps para IA

**1. Gestión Centralizada de Tokens**

Según Gartner, las empresas con programas centralizados de gestión de tokens de IA reportan costos generales 23-30% menores comparadas con aquellas con enfoques descentralizados.

**Implementación:**
- Dashboard centralizado de consumo de tokens
- Alertas de umbral por equipo/proyecto
- Reportes de costo por Policy Ticket/feature

**2. Optimización de Prompts**

Investigación del laboratorio de IA de Stanford demuestra que prompts optimizados pueden reducir consumo de tokens 30-50% manteniendo calidad de output.

**Técnicas:**
- Prompts concisos y específicos
- Uso de ejemplos few-shot eficientes
- Evitar redundancia en instrucciones
- Caching de contexto cuando sea posible

**3. Selección Estratégica de Modelos**

No todos los modelos tienen el mismo costo. La elección del modelo correcto para cada tarea puede reducir costos dramáticamente:

| Tarea | Modelo Recomendado | Justificación |
|-------|-------------------|---------------|
| Generación de código complejo | Claude Opus / GPT-4 | Precisión crítica |
| Refactoring simple | Claude Haiku / GPT-3.5 | Suficiente para tarea, menor costo |
| Generación de tests | Modelo medio | Balance costo-calidad |
| Documentación | Modelo económico | Tolerancia a imperfección |

**4. Métricas de Costo por Unidad de Valor**

El enfoque de FinOps cambia de costo de tokens a la economía unitaria de un caso de uso. Porque los conteos de tokens varían con cada ejecución, y casos de uso complejos pueden involucrar modelos de razonamiento que "piensan" antes de responder, el costo por unidad no es un precio fijo—es una distribución.

**Métricas recomendadas:**
- Costo por PR generado y aprobado
- Costo por bug corregido
- Costo por feature completada
- Costo por hora de desarrollo ahorrada

### Integración con Policy Tickets

Cada Policy Ticket debería incluir estimación y tracking de costos:

**Campos Económicos Sugeridos:**

```
ECONOMÍA:
├── Tokens estimados: [número]
├── Tokens consumidos: [número]
├── Costo estimado: [$]
├── Costo real: [$]
├── Modelo utilizado: [nombre]
└── Costo por unidad de valor: [$/feature, $/bug, etc.]
```

### Proyección de Tendencias

Para 2026, podemos ver el péndulo oscilando: algunos proveedores SaaS reintroduciendo paquetes basados en asientos o ajustando caps en planes basados en uso para hacer costos más predecibles.

En sectores donde la IA ha reducido COGS en un orden de magnitud, un vendor puede ofrecer una tarifa plana por usuario que supera el pricing de uso complicado de competidores.

Las organizaciones que implementen prácticas de FinOps para IA ahora estarán mejor posicionadas para optimizar costos y demostrar ROI claro de su inversión en desarrollo asistido por IA.

---

## ★ Modelo Económico del Framework IA-Native: ROI, TCO y Business Case ★

> **"Sin un modelo económico claro, la gobernanza se percibe como burocracia. Con números concretos, se convierte en inversión estratégica."**

Esta sección proporciona las herramientas cuantitativas necesarias para justificar la inversión en el marco IA-Native ante stakeholders financieros, calcular el retorno esperado y establecer métricas de éxito económico.

### El Problema Económico de la IA Sin Gobernanza

Antes de calcular el ROI del framework, es necesario cuantificar el costo de **no** implementarlo. La adopción de IA sin gobernanza genera costos ocultos que se acumulan exponencialmente:

**Modelo de Costos Ocultos (IA Sin Gobernanza):**

```
Costo_Total_Anual = Costo_Visible + Costo_Oculto

Donde:
  Costo_Visible = Licencias_IA + Infraestructura + Tokens
  Costo_Oculto = Retrabajo + Incidentes_Seguridad + Deuda_Técnica + Rotación
```

**Cuantificación de Costos Ocultos (basado en datos de industria 2025-2026):**

| Categoría de Costo Oculto | Fórmula | Estimación Típica |
|---------------------------|---------|-------------------|
| **Retrabajo de código IA** | 70% PRs × 2h promedio × $50/h | $70 × Nº PRs IA/mes |
| **Vulnerabilidades no detectadas** | 45% código vulnerable × $15K/incidente | $6,750 × bugs críticos/año |
| **Deuda técnica acelerada** | 20% incremento anual × costo refactoring | Variable según tamaño |
| **Rotación por burnout** | 15% rotación adicional × $50K/reemplazo | $7,500 × Nº devs |
| **Incidentes en producción** | MTTR × costo/hora × frecuencia | Variable según criticidad |

**Ejemplo Concreto - Software Factory de 50 Desarrolladores:**

```
┌─────────────────────────────────────────────────────────────────────────────┐
│     COSTO ANUAL DE IA SIN GOBERNANZA (50 desarrolladores)                   │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                             │
│  Costos Visibles                                                            │
│  ├── Licencias herramientas IA (Copilot, Claude, etc.)      $60,000        │
│  ├── Infraestructura adicional (GPU, APIs)                  $24,000        │
│  └── Tokens/consumo APIs                                    $36,000        │
│                                                             ─────────       │
│  Subtotal Visible                                          $120,000        │
│                                                                             │
│  Costos Ocultos (Sin Gobernanza)                                           │
│  ├── Retrabajo (70% de 200 PRs IA/mes × 2h × $50)         $168,000        │
│  ├── Incidentes seguridad (3 críticos × $15,000)           $45,000        │
│  ├── Deuda técnica (refactoring adicional)                 $80,000        │
│  ├── Rotación adicional (3 devs × $50,000)                $150,000        │
│  └── Incidentes producción (6 × $10,000 MTTR)              $60,000        │
│                                                             ─────────       │
│  Subtotal Oculto                                           $503,000        │
│                                                                             │
│  ════════════════════════════════════════════════════════════════════       │
│  COSTO TOTAL ANUAL SIN GOBERNANZA                         $623,000        │
│  ════════════════════════════════════════════════════════════════════       │
│                                                                             │
└─────────────────────────────────────────────────────────────────────────────┘
```

### Modelo de Inversión del Framework IA-Native

**Estructura de Costos de Implementación:**

#### Costos de Implementación Inicial (One-Time)

| Concepto | Descripción | Rango de Costo | Costo Típico (50 devs) |
|----------|-------------|----------------|------------------------|
| **Setup técnico** | Configuración de herramientas, integración CI/CD, templates | $10K - $30K | $20,000 |
| **Personalización** | Adaptar políticas, dominios de riesgo, workflows al contexto | $5K - $15K | $10,000 |
| **Training inicial** | 40h/persona × costo de capacitación | $30K - $80K | $50,000 |
| **Coaching externo** | Consultoría especializada primeros 3 meses | $15K - $40K | $25,000 |
| **Productividad perdida** | Curva de aprendizaje (estimado 20% primeras 8 semanas) | $20K - $50K | $35,000 |

```
┌─────────────────────────────────────────────────────────────────────────────┐
│     INVERSIÓN INICIAL - IMPLEMENTACIÓN (50 desarrolladores)                 │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                             │
│  Concepto                                              Costo                │
│  ├── Setup técnico y herramientas                     $20,000              │
│  ├── Personalización al contexto organizacional       $10,000              │
│  ├── Training (50 personas × 40h × $25/h)             $50,000              │
│  ├── Coaching externo (3 meses)                       $25,000              │
│  └── Productividad reducida durante adopción          $35,000              │
│                                                        ─────────            │
│  INVERSIÓN INICIAL TOTAL                             $140,000              │
│                                                                             │
└─────────────────────────────────────────────────────────────────────────────┘
```

#### Costos Operativos Continuos (Anuales)

| Concepto | Descripción | Cálculo | Costo Anual |
|----------|-------------|---------|-------------|
| **Overhead de Policy Tickets** | Tiempo adicional por PT | Ver matriz de overhead | $48,000 |
| **Mantenimiento de políticas** | Actualización trimestral de reglas | 8h/trimestre × $100/h | $3,200 |
| **Herramientas adicionales** | Dashboards, integraciones | Licencias + mantenimiento | $12,000 |
| **Training continuo** | Onboarding nuevos + refreshers | 10h/persona/año | $12,500 |
| **Auditorías internas** | Revisión trimestral de compliance | 16h/trimestre × $100/h | $6,400 |

**Cálculo detallado del overhead de Policy Tickets:**

```
Overhead_Anual = Σ (Nº_PTs_nivel × Tiempo_promedio_nivel × Costo_hora)

Distribución típica de PTs por nivel (mensual, 50 devs):
- Bajo:    150 PTs × 7.5 min = 18.75 horas × $50 = $937.50
- Medio:    80 PTs × 22 min  = 29.33 horas × $60 = $1,760
- Alto:     15 PTs × 45 min  = 11.25 horas × $80 = $900
- Crítico:   5 PTs × 90 min  =  7.50 horas × $100 = $750

Total mensual: $4,347.50
Total anual: $52,170 ≈ $48,000 (con eficiencias de madurez)
```

```
┌─────────────────────────────────────────────────────────────────────────────┐
│     COSTOS OPERATIVOS ANUALES (50 desarrolladores)                          │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                             │
│  Concepto                                              Costo Anual          │
│  ├── Overhead de Policy Tickets                        $48,000              │
│  ├── Mantenimiento de políticas                         $3,200              │
│  ├── Herramientas y dashboards                         $12,000              │
│  ├── Training continuo                                 $12,500              │
│  └── Auditorías internas                                $6,400              │
│                                                         ─────────           │
│  COSTO OPERATIVO ANUAL                                 $82,100              │
│                                                                             │
└─────────────────────────────────────────────────────────────────────────────┘
```

### Modelo de Beneficios del Framework

**Beneficios Cuantificables:**

#### Reducción de Costos Ocultos

| Beneficio | Sin Framework | Con Framework | Ahorro |
|-----------|---------------|---------------|--------|
| Retrabajo de código IA | $168,000 | $50,400 (70% reducción) | $117,600 |
| Incidentes de seguridad | $45,000 | $9,000 (80% reducción) | $36,000 |
| Deuda técnica | $80,000 | $24,000 (70% reducción) | $56,000 |
| Rotación por burnout | $150,000 | $75,000 (50% reducción) | $75,000 |
| Incidentes producción | $60,000 | $18,000 (70% reducción) | $42,000 |
| **Total Ahorro** | | | **$326,600** |

#### Beneficios de Productividad

| Beneficio | Cálculo | Valor Anual |
|-----------|---------|-------------|
| Mayor tasa de aceptación de PRs | +30% PRs útiles × valor/PR | $60,000 |
| Reducción de ciclo de review | -40% tiempo review × costo | $40,000 |
| Mejor onboarding (código documentado) | -30% tiempo ramp-up × costo | $25,000 |
| **Total Productividad** | | **$125,000** |

#### Beneficios de Compliance y Riesgo

| Beneficio | Cálculo | Valor Anual |
|-----------|---------|-------------|
| Evitación multas EU AI Act | Probabilidad × impacto | $50,000 (valor esperado) |
| Reducción primas de seguro | -15% prima cyber | $15,000 |
| Evidencia para auditorías | Ahorro tiempo auditoría | $20,000 |
| **Total Compliance** | | **$85,000** |

```
┌─────────────────────────────────────────────────────────────────────────────┐
│     BENEFICIOS ANUALES TOTALES (50 desarrolladores)                         │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                             │
│  Categoría                                             Beneficio Anual      │
│  ├── Reducción de costos ocultos                       $326,600             │
│  ├── Mejoras de productividad                          $125,000             │
│  └── Compliance y reducción de riesgo                   $85,000             │
│                                                         ─────────           │
│  BENEFICIO ANUAL TOTAL                                 $536,600             │
│                                                                             │
└─────────────────────────────────────────────────────────────────────────────┘
```

### Cálculo de ROI y Breakeven

**Fórmula de ROI del Framework:**

```
ROI = (Beneficios_Anuales - Costos_Operativos) / Inversión_Inicial × 100

ROI_Año1 = ($536,600 - $82,100) / $140,000 × 100 = 324.6%
```

**Análisis de Breakeven:**

```
Breakeven_Meses = Inversión_Inicial / (Beneficio_Mensual - Costo_Operativo_Mensual)

Beneficio mensual = $536,600 / 12 = $44,717
Costo operativo mensual = $82,100 / 12 = $6,842
Beneficio neto mensual = $37,875

Breakeven = $140,000 / $37,875 = 3.7 meses
```

```
┌─────────────────────────────────────────────────────────────────────────────┐
│     RESUMEN FINANCIERO - CASO BASE (50 desarrolladores)                     │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                             │
│  INVERSIÓN                                                                  │
│  ├── Inicial (one-time)                               $140,000              │
│  └── Operativa (anual)                                 $82,100              │
│                                                                             │
│  BENEFICIOS                                                                 │
│  └── Anual                                            $536,600              │
│                                                                             │
│  MÉTRICAS CLAVE                                                             │
│  ├── ROI Año 1                                           325%               │
│  ├── ROI Año 2+                                          554%               │
│  ├── Breakeven                                        3.7 meses             │
│  ├── Beneficio Neto Año 1                             $314,500              │
│  └── Beneficio Neto Año 2+                            $454,500              │
│                                                                             │
│  COMPARACIÓN                                                                │
│  ├── Costo con framework (Año 1)                      $222,100              │
│  ├── Costo sin framework                              $623,000              │
│  └── Ahorro vs. no actuar                             $400,900              │
│                                                                             │
└─────────────────────────────────────────────────────────────────────────────┘
```

### Modelo de Escalabilidad por Tamaño de Organización

**Factores de escala:**

| Tamaño | Desarrolladores | Factor de Costo | Factor de Beneficio | ROI Estimado |
|--------|-----------------|-----------------|---------------------|--------------|
| Pequeña | 10-25 | 0.4x | 0.35x | 280% |
| Mediana | 26-75 | 1.0x | 1.0x | 325% |
| Grande | 76-200 | 2.2x | 2.8x | 380% |
| Enterprise | 200+ | 4.0x | 5.5x | 410% |

*El ROI mejora con el tamaño porque los costos de setup se amortizan y los beneficios de estandarización se multiplican.*

### Análisis de Sensibilidad

**¿Qué pasa si los beneficios son menores a lo esperado?**

| Escenario | Beneficio Real | ROI Año 1 | Breakeven |
|-----------|----------------|-----------|-----------|
| Optimista (+20%) | $643,920 | 401% | 2.9 meses |
| Caso base | $536,600 | 325% | 3.7 meses |
| Conservador (-20%) | $429,280 | 248% | 4.8 meses |
| Pesimista (-40%) | $321,960 | 171% | 7.0 meses |

**Punto de quiebre:** El framework sigue siendo rentable incluso si los beneficios son **74% menores** a lo proyectado (ROI = 0% cuando beneficios = $222,100).

### Business Case para Presentación Ejecutiva

**One-Pager para C-Suite:**

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                                                                             │
│              BUSINESS CASE: FRAMEWORK IA-NATIVE                             │
│                                                                             │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                             │
│  PROBLEMA                                                                   │
│  La adopción de IA sin gobernanza genera $500K+ en costos ocultos           │
│  anuales: retrabajo, vulnerabilidades, deuda técnica, rotación.             │
│                                                                             │
│  SOLUCIÓN                                                                   │
│  Framework de gobernanza IA-Native con Policy Tickets, niveles de           │
│  autonomía graduados y trazabilidad completa.                               │
│                                                                             │
│  INVERSIÓN                           RETORNO                                │
│  ┌──────────────────┐               ┌──────────────────┐                    │
│  │                  │               │                  │                    │
│  │    $140,000      │               │    $454,500      │                    │
│  │    inicial       │      →        │    anual neto    │                    │
│  │                  │               │    (Año 2+)      │                    │
│  └──────────────────┘               └──────────────────┘                    │
│                                                                             │
│  MÉTRICAS CLAVE                                                             │
│  • ROI: 325% (Año 1) → 554% (Año 2+)                                       │
│  • Breakeven: 3.7 meses                                                     │
│  • Reducción de incidentes: 70-80%                                          │
│  • Compliance EU AI Act: Incluido                                           │
│                                                                             │
│  RIESGO DE NO ACTUAR                                                        │
│  • Multas EU AI Act: hasta 7% revenue global                               │
│  • Vulnerabilidades: 45% del código IA comprometido                        │
│  • Deuda técnica: crecimiento exponencial sin control                      │
│                                                                             │
│  RECOMENDACIÓN                                                              │
│  Aprobar implementación Q1 2026 con piloto en 1 equipo,                    │
│  rollout completo para Q3 2026 antes de EU AI Act.                         │
│                                                                             │
└─────────────────────────────────────────────────────────────────────────────┘
```

### Plantilla de Cálculo Personalizado

Para calcular el ROI en su contexto específico, use esta plantilla:

```
=== CALCULADORA ROI FRAMEWORK IA-NATIVE ===

INPUTS (completar con datos de su organización):
├── Número de desarrolladores: [___]
├── Salario promedio anual: $[___]
├── PRs con IA por mes (total): [___]
├── Tasa actual de retrabajo: [___]%
├── Incidentes de seguridad por año: [___]
├── Costo promedio por incidente: $[___]
├── Rotación anual actual: [___]%
└── Costo de reemplazo por persona: $[___]

CÁLCULO AUTOMÁTICO:
├── Costo sin framework: $[calculado]
├── Inversión inicial: $[calculado]
├── Costo operativo anual: $[calculado]
├── Beneficios anuales: $[calculado]
├── ROI Año 1: [calculado]%
├── Breakeven: [calculado] meses
└── Beneficio neto 5 años: $[calculado]
```

Este modelo económico proporciona la justificación cuantitativa necesaria para la adopción del framework y establece expectativas claras de retorno que pueden monitorearse durante la implementación.

---

# PARTE VII: CALIDAD Y MÉTRICAS

## ★ Testing y QA de Código Generado por IA ★

> **"El QA ya no solo está testeando software; está validando la IA que cada vez más construye y testea ese software."**

El testing de código generado por IA presenta desafíos fundamentalmente diferentes al testing tradicional. Esta sección examina las metodologías emergentes, los problemas específicos del código generado por IA, y cómo el rol de QA se transforma en entornos IA-Native.

### El Problema Fundamental: Comportamiento Probabilístico

A diferencia de los sistemas deterministas cuyo comportamiento está embebido en el código, el comportamiento de IA emerge de los datos. El testing debe por lo tanto validar no solo el código sino también la calidad de los datos y su sesgo inherente.

**El desafío del pass/fail:**

En testing tradicional, un test pasa o falla. Con sistemas probabilísticos, puede no haber un único output "correcto" sino un rango de comportamientos aceptables que cumplen umbrales de confianza. Esta variabilidad esencialmente desafía la lógica del testing binario.

**Implicación práctica:** Los equipos serios siempre reportan métricas agregadas a través de múltiples ejecuciones. El rendimiento de agentes es estocástico.

### La Realidad del Código Generado por IA

Los datos son contundentes:

| Métrica | Valor |
|---------|-------|
| Código IA que requiere intervención | 60%+ |
| Tasa de vulnerabilidades en código IA | 45-50% |
| Tasa de vulnerabilidades en código humano | 15-20% |
| Desarrolladores que reescriben código IA | 70%+ |

Más de la mitad de las muestras muestran fallas lógicas o de seguridad. En encuestas, más del 70% de desarrolladores dicen que rutinariamente tienen que reescribir o refactorizar código generado por IA antes de que esté listo para producción.

### Nuevas Disciplinas de Testing

**1. Bias Testing**

Verificar que el código generado no incorpore sesgos del modelo o de los datos de entrenamiento.

*Ejemplos:*
- Algoritmos de pricing que discriminen por región
- Sistemas de recomendación con sesgos demográficos
- Lógica de validación con supuestos culturales

**2. Robustness Validation**

Probar cómo el código generado maneja edge cases y condiciones adversariales.

*Áreas críticas:*
- Inputs malformados
- Condiciones de carrera
- Límites de recursos
- Comportamiento bajo carga

**3. Model Drift Monitoring**

El comportamiento de agentes de IA puede cambiar con el tiempo a medida que los modelos subyacentes se actualizan.

*Implementación:*
- Baselines de comportamiento esperado
- Alertas cuando outputs divergen significativamente
- Re-validación periódica de outputs históricos

### Testing Basado en Riesgo vs Cobertura

El objetivo tradicional de maximizar cobertura de tests está dando paso a maximizar cobertura de riesgo. En lugar de testear todo por igual, la orquestación inteligente enfoca el esfuerzo donde más importa.

**Resultado:** Potencial reducción del tiempo total de testing en 40% mientras mejoran los outcomes de calidad.

**Matriz de Priorización:**

| Dominio | Riesgo | Cobertura Requerida |
|---------|--------|---------------------|
| Lógica de negocio core | Alto | Exhaustiva |
| Seguridad y autenticación | Alto | Exhaustiva + Pentest |
| Integraciones externas | Medio-Alto | Funcional + Contrato |
| UI/UX | Medio | Funcional + Regresión visual |
| Código utilitario | Bajo | Unitaria básica |
| Documentación generada | Bajo | Revisión humana spot-check |

### Self-Healing Tests y Autonomous Fixes

Para 2026, el diagnóstico inteligente, tests auto-reparables, y correcciones autónomas son habilitadores clave de releases más rápidos y estables, permitiendo a testers enfocarse en expandir cobertura, optimizar estrategias de test, y trabajo exploratorio de alto valor.

**Capacidades emergentes:**
- Tests que se actualizan automáticamente cuando cambia la UI
- Identificación automática de root cause
- Sugerencias de fix generadas por IA
- Priorización inteligente de tests a ejecutar

### El Modelo Híbrido de QA

Para 2026, el 70% de las organizaciones DevOps-driven adoptarán un modelo híbrido de QA, combinando prevención y validación en el mundo real.

**Componentes del modelo híbrido:**

1. **Prevention (Shift-Left)**
   - Análisis estático en cada PR
   - Gates de calidad automatizados
   - Linting y formateo obligatorio
   - Security scanning integrado

2. **Detection (Continuous)**
   - Tests automatizados en pipeline
   - Monitoring de producción
   - Alertas de anomalías
   - Synthetic monitoring

3. **Response (Shift-Right)**
   - Feature flags para rollback rápido
   - Canary deployments
   - A/B testing de cambios
   - Observabilidad completa

### Integración con el Marco IA-Native

**Checks Obligatorios por Nivel de Riesgo:**

| Nivel de Riesgo | Checks Mínimos |
|-----------------|----------------|
| Bajo | Lint, format, tests unitarios, build |
| Medio | + Tests integración, SAST, cobertura mínima |
| Alto | + DAST, pentest automatizado, revisión manual |
| Crítico | + Revisión de seguridad dedicada, sign-off de arquitecto |

**Campos de QA en Policy Tickets:**

```
QA Y TESTING:
├── Tests requeridos: [lista]
├── Tests ejecutados: [lista]
├── Cobertura alcanzada: [%]
├── Vulnerabilidades detectadas: [número]
├── Vulnerabilidades remediadas: [número]
├── Revisión humana completada: [Sí/No]
└── Aprobación QA: [nombre, fecha]
```

### El Nuevo Rol de QA

En este nuevo paradigma, QA no solo está testeando software, sino validando la IA que cada vez más construye y testea ese software.

**Responsabilidades transformadas:**
- Diseñar objetivos de calidad, no solo escribir tests
- Supervisar resultados generados por IA
- Asegurar que decisiones automatizadas se alineen con prioridades de negocio
- Definir umbrales de aceptabilidad para outputs probabilísticos
- Auditar la calidad de los datos usados por la IA

---

## ★ Ética, Sesgo y Fairness en el Desarrollo IA-Native ★

> **"Las empresas que priorizan la ética construyen confianza del cliente más fuerte. Protegen su reputación de marca y evitan crisis legales costosas. En 2026, la ética es una ventaja competitiva."**

El marco IA-Native no puede considerarse completo sin abordar las dimensiones éticas del desarrollo asistido por inteligencia artificial. Esta sección examina los desafíos de sesgo, las obligaciones de fairness, y cómo integrar consideraciones éticas en la práctica diaria del desarrollo.

### El Problema del Sesgo Algorítmico

Los sistemas de IA aprenden de datos, y si esos datos contienen prejuicios humanos, la IA aprenderá y repetirá esos patrones. El sesgo puede introducirse de múltiples formas:

**Fuentes de Sesgo:**

1. **Datos de Entrenamiento**
   - Si los datos históricos contienen prejuicios humanos o carecen de representación de ciertos grupos, el algoritmo aprenderá esos patrones

2. **Decisiones de Diseño**
   - Las elecciones hechas por desarrolladores durante la fase de diseño, como qué características el modelo debería priorizar

3. **Contexto de Aplicación**
   - Un modelo entrenado en un contexto puede comportarse de manera sesgada en otro

**Ejemplos en Desarrollo de Software:**

| Área | Riesgo de Sesgo |
|------|-----------------|
| Generación de código | Patrones aprendidos de código mayoritariamente de ciertos contextos culturales/geográficos |
| Naming conventions | Sugerencias que reflejan sesgos de género o culturales |
| Documentación | Lenguaje que excluye o asume ciertos perfiles de usuario |
| Testing | Casos de prueba que no consideran diversidad de usuarios |

### Responsabilidad del Desarrollador

Cuando una IA toma una decisión, ¿quién es responsable? ¿El desarrollador, la empresa, o la máquina? Las apuestas son altas, pero la accountability es frecuentemente difusa.

**Medidas en discusión global:**
Mandatar que las organizaciones que usan IA aseguren que la responsabilidad final recaiga en un humano, quien será accountable por daños causados por sesgo, alucinaciones o malas decisiones.

### Integración de Ética en el Ciclo de Desarrollo

**1. Checks Éticos en CI/CD**

Los proveedores cloud ahora ofrecen librerías nativas de fairness en sus pipelines de MLOps, permitiendo a desarrolladores embeber checks de políticas en etapas de CI/CD.

**Implementación práctica:**
- Cada pull request dispara checks automatizados para paridad de grupos, odds igualados, y calibración entre cohortes
- Cualquier PR que falle estos checks es flaggeado
- Los desarrolladores deben proporcionar planes de remediación antes de la aprobación

**2. Diversidad en Equipos de Desarrollo**

Arreglar el sesgo comienza con mejor recolección de datos, diversidad en equipos de desarrollo, y transparencia en cómo los algoritmos son entrenados y testeados.

**Beneficios documentados:**
- Equipos diversos identifican más edge cases
- Perspectivas variadas detectan sesgos no evidentes para grupos homogéneos
- Mejor representación de usuarios finales

**3. Auditorías Regulares**

Establecer auditorías regulares para sistemas de IA para detectar y abordar sesgos, inexactitudes, o brechas éticas. Asegurar que exista un proceso claro de reparación en casos de daño causado por IA.

### Principios de IA Responsable

Microsoft ha articulado principios de IA responsable que sirven como referencia:

| Principio | Descripción | Implementación en IA-Native |
|-----------|-------------|----------------------------|
| **Fairness** | Sistemas que tratan a todas las personas justamente | Checks de paridad en PRs |
| **Reliability & Safety** | Funcionamiento seguro y confiable | Gates de calidad obligatorios |
| **Privacy & Security** | Protección de datos y sistemas | Escaneo de secretos, SAST |
| **Inclusiveness** | Empoderar a todos y engajar | Revisión de accesibilidad |
| **Transparency** | Operación comprensible | Documentación de decisiones |
| **Accountability** | Responsabilidad por sistemas | Policy Tickets con responsable |

### Frameworks de Compliance Ético

Las organizaciones deben integrar frameworks de IA responsable como:

- **NIST AI RMF** (Risk Management Framework)
- **ISO/IEC 42001** (Estándar de gestión de sistemas de IA)
- **IEEE 7000** (Procesos de ingeniería ética)

### Integración con Policy Tickets

**Campos Éticos Sugeridos:**

```
ÉTICA Y FAIRNESS:
├── Evaluación de sesgo completada: [Sí/No]
├── Grupos afectados identificados: [lista]
├── Mitigaciones implementadas: [descripción]
├── Revisión de inclusividad: [Sí/No]
├── Impacto en privacidad evaluado: [Sí/No]
└── Aprobación ética: [nombre, fecha]
```

### El Caso de Negocio para la Ética

Las empresas que priorizan la ética:
- Construyen confianza del cliente más fuerte
- Protegen su reputación de marca
- Evitan crisis legales costosas
- Atraen y retienen mejor talento
- Cumplen proactivamente con regulaciones emergentes

En 2026, la ética no es solo una obligación moral; es una ventaja competitiva tangible.

---

## ★ KPIs y Métricas para Entornos IA-Native ★

> **"Lo que se mide, se gestiona. En entornos IA-Native, las métricas tradicionales de velocidad pierden relevancia frente a métricas de impacto, riesgo y calidad sostenible."**

Un marco de gobernanza sin métricas claras es un marco sin capacidad de mejora. Esta sección define los KPIs específicos necesarios para medir el éxito de la adopción IA-Native y la efectividad del sistema de Policy Tickets.

### Categorías de Métricas

**1. Métricas de Productividad Gobernada**

| Métrica | Definición | Target Sugerido |
|---------|------------|-----------------|
| PRs generados por IA / semana | Volumen de cambios propuestos por agentes | Crecimiento estable |
| Tasa de aceptación de PRs IA | PRs aprobados / PRs generados | >70% |
| Tiempo de validación | Tiempo desde PR hasta merge | <4 horas para bajo riesgo |
| Policy Tickets completados / sprint | Throughput de trabajo gobernado | Según capacidad del equipo |

**2. Métricas de Calidad**

| Métrica | Definición | Target Sugerido |
|---------|------------|-----------------|
| Defectos por KLOC (código IA) | Bugs en código generado por IA | ≤ código humano |
| Defectos por KLOC (código humano) | Baseline de comparación | Histórico del equipo |
| Vulnerabilidades detectadas pre-merge | Issues de seguridad capturados en CI | 100% críticos |
| Vulnerabilidades en producción | Issues que escaparon a producción | 0 críticos |
| Cobertura de tests | % código cubierto por tests | >80% para código crítico |

**3. Métricas de Riesgo**

| Métrica | Definición | Target Sugerido |
|---------|------------|-----------------|
| Distribución por nivel de riesgo | % de Policy Tickets por categoría | Mayoría bajo/medio |
| Incidentes por nivel de riesgo | Fallos correlacionados con clasificación | Alto riesgo < Bajo riesgo |
| Tiempo de detección de incidentes | Tiempo hasta identificar problema | <1 hora |
| Tiempo de remediación | Tiempo hasta resolver incidente | <4 horas críticos |

**4. Métricas de Deuda y Sostenibilidad**

| Métrica | Definición | Target Sugerido |
|---------|------------|-----------------|
| Complejidad cognitiva | Medida de dificultad de comprensión | Estable o decreciente |
| Warnings de análisis estático | Issues detectados por linters/SAST | Decreciente |
| Deuda de comprensión | Código que nadie entiende completamente | <10% del codebase |
| Churn de código IA | % de código IA modificado en 30 días | <20% |

**5. Métricas Económicas**

| Métrica | Definición | Target Sugerido |
|---------|------------|-----------------|
| Costo por PR generado | Tokens consumidos × precio | Decreciente |
| Costo por feature completada | Costo total de IA por feature | ROI positivo |
| Costo de retrabajo | Tiempo × costo de corregir código IA | <30% de ahorro inicial |
| ROI de adopción IA | Valor generado / costo de IA | >2x |

**6. Métricas de Gobernanza**

| Métrica | Definición | Target Sugerido |
|---------|------------|-----------------|
| Compliance de Policy Tickets | PRs con ticket válido / total PRs | 100% |
| Trazabilidad completa | Cambios rastreables a decisión | 100% |
| Auditorías sin hallazgos críticos | Auditorías limpias / total | 100% |
| Tiempo de respuesta a auditoría | Tiempo para producir evidencia | <24 horas |

### Dashboard Sugerido

**Vista Ejecutiva:**
```
┌─────────────────────────────────────────────────────────────┐
│                    DASHBOARD IA-NATIVE                       │
├─────────────────┬─────────────────┬─────────────────────────┤
│   PRODUCTIVIDAD │     CALIDAD     │        RIESGO           │
├─────────────────┼─────────────────┼─────────────────────────┤
│ PRs/semana: 47  │ Defectos: 2.1/K │ Alto: 5% │ Medio: 25%  │
│ Aceptación: 78% │ Vulns pre: 12   │ Bajo: 70%              │
│ T.validación: 2h│ Vulns prod: 0   │ Incidentes: 0          │
├─────────────────┴─────────────────┴─────────────────────────┤
│                      ECONOMÍA                                │
├─────────────────┬─────────────────┬─────────────────────────┤
│ Costo/PR: $2.30 │ Costo/feature:  │ ROI mensual: 2.3x      │
│                 │ $45             │                         │
├─────────────────┴─────────────────┴─────────────────────────┤
│                     GOBERNANZA                               │
├─────────────────┬─────────────────┬─────────────────────────┤
│ Compliance: 100%│ Trazabilidad:   │ Auditorías OK: 100%    │
│                 │ 100%            │                         │
└─────────────────┴─────────────────┴─────────────────────────┘
```

### Frecuencia de Medición

| Métrica | Frecuencia | Responsable |
|---------|------------|-------------|
| Productividad | Diaria | Tech Lead |
| Calidad | Por PR + Semanal agregado | QA |
| Riesgo | Por PR + Diaria agregado | Tech Lead |
| Economía | Semanal | FinOps/Manager |
| Gobernanza | Continua + Mensual auditoría | Compliance |

### Anti-métricas: Lo que NO Medir

| Anti-métrica | Por qué evitarla |
|--------------|------------------|
| Líneas de código generadas | Incentiva volumen sobre valor |
| Velocidad bruta de generación | Ignora calidad y retrabajo |
| % de código escrito por IA | No correlaciona con éxito |
| Tokens consumidos (sin contexto) | No indica eficiencia |

---

## ★ Casos de Fallo y Anti-patrones ★

> **"El marco IA-Native debe considerarse refutado si, en la práctica, no logra mejorar la trazabilidad, no reduce incidentes críticos, o introduce una carga organizacional desproporcionada sin beneficios claros."**

Aprender de los fallos es esencial para la madurez del marco. Esta sección documenta anti-patrones comunes, señales de alarma temprana, y estrategias de recuperación.

### Anti-patrones de Adopción

**1. Over-Delegation (Delegación Excesiva)**

*Descripción:* Confiar demasiado en la IA, delegando tareas críticas sin supervisión adecuada.

*Síntomas:*
- PRs de alto riesgo aprobados sin revisión humana sustancial
- Reducción de expertise del equipo porque "la IA lo hace"
- Incapacidad de explicar por qué el código hace lo que hace

*Consecuencias:*
- Deuda de comprensión acelerada
- Vulnerabilidades no detectadas
- Pérdida de capacidad de respuesta ante incidentes

*Mitigación:*
- Mantener requisitos de revisión humana proporcionales al riesgo
- Rotar quién revisa código de IA para distribuir conocimiento
- Documentar decisiones arquitectónicas explícitamente

**2. Audit Theater (Teatro de Auditoría)**

*Descripción:* Cumplir formalmente con Policy Tickets sin que representen gobernanza real.

*Síntomas:*
- Policy Tickets genéricos copy-paste
- Campos obligatorios llenados con valores placeholder
- Evidencias que no corresponden a los cambios reales

*Consecuencias:*
- Falsa sensación de control
- Trazabilidad inútil cuando se necesita
- Exposición legal no mitigada

*Mitigación:*
- Auditorías spot-check regulares de calidad de tickets
- Templates que requieran información específica no genérica
- Consecuencias reales por tickets de baja calidad

**3. Speed Addiction (Adicción a la Velocidad)**

*Descripción:* Optimizar obsesivamente por velocidad de generación ignorando costos downstream.

*Síntomas:*
- Métricas de éxito basadas solo en volumen de PRs
- Presión para reducir tiempo de validación a costa de calidad
- Celebración de "X líneas generadas" sin mencionar retrabajo

*Consecuencias:*
- Deuda técnica exponencial
- Burnout del equipo por retrabajo constante
- Degradación de la base de código

*Mitigación:*
- Métricas balanceadas que incluyan calidad y sostenibilidad
- Tracking visible de costo de retrabajo
- Celebrar cambios de alto impacto, no alto volumen

**4. Governance Fatigue (Fatiga de Gobernanza)**

*Descripción:* La carga de cumplir con el marco supera los beneficios percibidos.

*Síntomas:*
- Quejas constantes sobre "burocracia"
- Intentos de evadir el proceso
- Disminución de calidad de documentación con el tiempo

*Consecuencias:*
- Resistencia organizacional
- Adopción superficial
- Retorno a prácticas no gobernadas

*Mitigación:*
- Simplificar proceso para dominios de bajo riesgo
- Automatizar todo lo automatizable
- Demostrar valor del marco con casos de éxito específicos

**5. Tool Obsession (Obsesión con Herramientas)**

*Descripción:* Enfocarse en qué agente o framework usar en lugar de cómo gobernar su uso.

*Síntomas:*
- Debates interminables sobre Cursor vs Claude Code
- Cambio constante de herramientas
- Expectativa de que la herramienta correcta resolverá problemas de proceso

*Consecuencias:*
- Inconsistencia en prácticas
- Curvas de aprendizaje repetidas
- Problemas de gobernanza sin resolver

*Mitigación:*
- Establecer marco de gobernanza primero, seleccionar herramientas después
- Estandarizar en un set limitado de herramientas
- Evaluar herramientas por capacidad de gobernanza, no solo features

### Señales de Alarma Temprana

| Señal | Indicador | Acción |
|-------|-----------|--------|
| 🚨 Deuda creciente | Complejidad cognitiva +20% en 30 días | Pausa de generación, refactoring |
| 🚨 Calidad decreciente | Defectos/KLOC duplicados | Revisión de proceso, training |
| 🚨 Evasión de proceso | PRs sin Policy Ticket | Enforcement técnico, conversación |
| 🚨 Conocimiento concentrado | <2 personas entienden área crítica | Distribución forzada de reviews |
| 🚨 Costos desbocados | >50% sobre presupuesto de tokens | Auditoría de uso, optimización |

### Playbook de Recuperación

**Cuando un Incidente Ocurre:**

```
1. CONTENER
   ├── Identificar alcance del impacto
   ├── Rollback si es posible
   └── Comunicar a stakeholders

2. INVESTIGAR
   ├── Rastrear hasta Policy Ticket origen
   ├── Identificar qué checks fallaron
   ├── Determinar causa raíz

3. REMEDIAR
   ├── Corregir el problema inmediato
   ├── Identificar cambios similares en riesgo
   └── Verificar corrección con tests

4. APRENDER
   ├── Documentar lecciones aprendidas
   ├── Actualizar checks/gates si aplica
   ├── Compartir con equipo

5. MEJORAR
   ├── Ajustar clasificación de riesgo si subestimada
   ├── Añadir checks que habrían detectado el problema
   └── Actualizar training si hubo error humano
```

### Casos Documentados (Anonimizados)

**Caso 1: El Refactor Que Rompió Producción**

*Contexto:* Software factory mediana, 6 meses usando IA intensivamente.

*Qué pasó:* Un agente propuso un refactor "de mejora de legibilidad" que tocó 47 archivos. El PR fue aprobado rápidamente porque "solo mejoraba nombres". En producción, una dependencia circular introducida causó memory leaks que escalaron hasta outage.

*Causa raíz:*
- Refactor multi-archivo clasificado como bajo riesgo
- Revisión superficial por tamaño del diff
- Tests no cubrían escenario de carga

*Lección:* Cambios multi-archivo siempre requieren revisión proporcional al alcance, no a la intención declarada.

**Caso 2: La Vulnerabilidad Persistente**

*Contexto:* Startup fintech, uso agresivo de generación de código.

*Qué pasó:* Un agente generó código de autenticación con una vulnerabilidad de timing attack. SAST no la detectó. La vulnerabilidad estuvo en producción 3 meses hasta auditoría externa.

*Causa raíz:*
- Código de seguridad generado por IA sin revisión especializada
- SAST no configurado para detectar timing attacks
- No había requisito de revisión de seguridad para código auth

*Lección:* Dominios de seguridad requieren revisión humana especializada obligatoria, independientemente de quién (o qué) escribió el código.

**Caso 3: La Factura Sorpresa**

*Contexto:* Empresa mediana, primer trimestre de adopción de agentes.

*Qué pasó:* La factura de API de IA fue 340% mayor a lo presupuestado. Investigación reveló que un agente había entrado en un loop de retry exponencial durante un fin de semana, consumiendo millones de tokens sin producir resultados útiles.

*Causa raíz:*
- Sin límites de consumo por agente
- Sin alertas de uso anómalo
- Sin monitoreo de fin de semana

*Lección:* FinOps para IA requiere límites hard, alertas, y monitoreo 24/7.

---

## ★ Sistema de Skills de Gobernanza ★

> **"Así como el código se organiza en módulos reutilizables, el conocimiento de gobernanza debe estructurarse en unidades autocontenidas que puedan invocarse según el contexto."**

El marco IA-Native adopta el concepto de **skills**—unidades modulares de conocimiento técnico—para organizar y reutilizar las políticas de gobernanza. Este enfoque evita que cada equipo reinvente cómo estructurar sus Policy Tickets y asegura consistencia a través de la organización.

### Anatomía de un Skill de Gobernanza

Cada skill vive en su propio directorio y sigue una estructura consistente:

```
skills-governance/
├── SKILL.md              # Obligatorio: Metadatos + Instrucciones
├── assets/               # Opcional: Templates, ejemplos, scripts
└── references/           # Opcional: Enlaces a documentación externa
```

### Estructura del SKILL.md

Todo archivo SKILL.md comienza con un bloque de frontmatter YAML:

```yaml
---
name: refactor-policy
description: >
  Políticas para autorizar refactorizaciones de código existente.
  Trigger: Cuando se planifica cualquier refactorización.
license: MIT
metadata:
  author: Equipo de Gobernanza
  version: "1.0"
  scope: [root, backend, frontend]
  auto_invoke:
    - "Refactorizando código existente"
    - "Unificando componentes duplicados"
    - "Extrayendo funciones a módulos"
  risk_level: bajo-medio
allowed-tools: Read, Edit, Write, Glob, Grep, Bash
---

## Cuándo Usar Este Skill

- Refactorizaciones que no cambian comportamiento observable
- Extracción de funciones o componentes
- Renombrado de variables/funciones/clases
- Eliminación de código duplicado

## Patrones Críticos

### SIEMPRE
- Verificar cobertura de tests antes de refactorizar
- Ejecutar tests después de cada cambio atómico
- Documentar la intención del refactor en el Policy Ticket

### NUNCA
- Cambiar comportamiento observable sin Policy Ticket separado
- Refactorizar código sin tests (primero agregar tests)
- Combinar refactor con features nuevas en el mismo PR

## Niveles de Autonomía

| Tipo de Refactor | Autonomía IA |
|------------------|--------------|
| Renombrado simple | Completa con verificación automática |
| Extracción de función | Con revisión humana |
| Cambio de estructura de datos | Requiere aprobación explícita |
| Cambio de API | Solo propuesta, ejecución humana |

## Evidencias Requeridas

- [ ] Tests existentes pasan antes del cambio
- [ ] Tests existentes pasan después del cambio
- [ ] Cobertura no disminuye
- [ ] Análisis estático sin nuevos warnings
```

### Skills de Gobernanza Incluidos

El marco propone un conjunto base de skills:

| Skill | Scope | Descripción | Riesgo |
|-------|-------|-------------|--------|
| `refactor-policy` | root, frontend, backend | Políticas para refactorizaciones | Bajo-Medio |
| `test-generation` | root, frontend, backend | Generación automática de tests | Bajo |
| `documentation-policy` | root | Generación de documentación | Bajo |
| `security-change` | root, backend | Cambios en código de seguridad | Alto |
| `database-migration` | root, backend | Migraciones de esquema | Alto |
| `api-change` | root, backend | Modificaciones de APIs | Medio-Alto |
| `ui-component` | root, frontend | Nuevos componentes de UI | Bajo-Medio |
| `dependency-update` | root | Actualización de dependencias | Medio |
| `config-change` | root | Cambios de configuración | Medio-Alto |
| `emergency-fix` | root | Hotfixes en producción | Alto (con excepciones) |

### Auto-Invocación de Skills

Cuando un agente detecta que está realizando una acción, debe consultar el skill correspondiente:

| Acción | Skill a Invocar |
|--------|-----------------|
| Refactorizando código | `refactor-policy` |
| Generando tests | `test-generation` |
| Modificando endpoints | `api-change` |
| Tocando autenticación/autorización | `security-change` |
| Actualizando package.json/requirements.txt | `dependency-update` |
| Modificando esquema de BD | `database-migration` |
| Creando componentes React/Vue | `ui-component` |
| Generando documentación | `documentation-policy` |
| Cambiando variables de entorno | `config-change` |
| Fix urgente en producción | `emergency-fix` |

### Creación de Nuevos Skills

Para crear un nuevo skill de gobernanza:

```bash
# 1. Crear directorio
mkdir skills-governance/mi-nuevo-skill

# 2. Copiar template
cp skills-governance/SKILL-TEMPLATE.md skills-governance/mi-nuevo-skill/SKILL.md

# 3. Editar SKILL.md con contenido específico
# - Definir scope, auto_invoke, risk_level
# - Documentar patrones SIEMPRE/NUNCA
# - Especificar niveles de autonomía
# - Listar evidencias requeridas

# 4. Sincronizar con GOVERNANCE.md
./skills-governance/governance-sync.sh
```

---

## ★ Arquitectura de Documentación por Capas ★

> **"La documentación de gobernanza debe reflejar la estructura de riesgos del sistema. Cada dominio crítico merece sus propias reglas explícitas."**

Inspirado en la arquitectura de AGENTS.md del template de contexto para agentes, el marco IA-Native propone una estructura jerárquica de documentación de gobernanza.

### Estructura de GOVERNANCE.md

```
GOVERNANCE.md (raíz)              → Reglas universales de gobernanza IA-Native
│
├── domains/
│   ├── payments/GOVERNANCE.md   → Reglas para dominio de pagos (ALTO RIESGO)
│   ├── security/GOVERNANCE.md   → Reglas para código de seguridad (ALTO RIESGO)
│   ├── backend/GOVERNANCE.md    → Reglas para servicios backend (MEDIO RIESGO)
│   ├── frontend/GOVERNANCE.md   → Reglas para UI/UX (BAJO-MEDIO RIESGO)
│   ├── testing/GOVERNANCE.md    → Reglas para generación de tests (BAJO RIESGO)
│   └── documentation/GOVERNANCE.md → Reglas para docs (BAJO RIESGO)
│
└── skills-governance/           → Skills modulares de gobernanza
    ├── refactor-policy/SKILL.md
    ├── security-change/SKILL.md
    └── ...
```

### Contenido de GOVERNANCE.md (Raíz)

El archivo raíz define reglas universales que aplican a todo el proyecto:

```markdown
# GOVERNANCE.md

> Instrucciones de gobernanza IA-Native para este proyecto.
> Para documentación humana, ver [README.md](README.md).

## Reglas Universales

### SIEMPRE
- Vincular cada PR a un Policy Ticket
- Identificar responsable humano antes de la acción
- Producir evidencia verificable
- Respetar los niveles de autonomía por dominio

### NUNCA
- Ejecutar cambios sin Policy Ticket
- Mergear sin checks automáticos pasando
- Modificar código de alto riesgo con autonomía completa
- Ignorar los skills de gobernanza aplicables

## Dominios y Niveles de Riesgo

| Dominio | Riesgo | GOVERNANCE.md | Autonomía Máxima |
|---------|--------|---------------|------------------|
| Pagos | Alto | [domains/payments/](domains/payments/GOVERNANCE.md) | Solo propuesta |
| Seguridad | Alto | [domains/security/](domains/security/GOVERNANCE.md) | Solo propuesta |
| Backend | Medio | [domains/backend/](domains/backend/GOVERNANCE.md) | Con revisión |
| Frontend | Bajo-Medio | [domains/frontend/](domains/frontend/GOVERNANCE.md) | Con revisión |
| Testing | Bajo | [domains/testing/](domains/testing/GOVERNANCE.md) | Completa |
| Docs | Bajo | [domains/documentation/](domains/documentation/GOVERNANCE.md) | Completa |

## Skills de Gobernanza

| Skill | Descripción |
|-------|-------------|
| [refactor-policy](skills-governance/refactor-policy/SKILL.md) | Refactorizaciones |
| [security-change](skills-governance/security-change/SKILL.md) | Cambios de seguridad |
| [api-change](skills-governance/api-change/SKILL.md) | Modificaciones de API |
| ... | ... |

## Auto-invocación

| Acción | Skill |
|--------|-------|
| Refactorizando código | `refactor-policy` |
| Modificando autenticación | `security-change` |
| Cambiando endpoints | `api-change` |
| ... | ... |

<!-- Tabla auto-generada por governance-sync.sh -->
```

### Contenido de GOVERNANCE.md por Dominio

Cada dominio tiene su propio archivo que hereda las reglas raíz y agrega restricciones específicas:

```markdown
# domains/payments/GOVERNANCE.md

> Gobernanza específica para el dominio de PAGOS.
> Nivel de riesgo: ALTO

## Herencia

Este archivo extiende [GOVERNANCE.md](../../GOVERNANCE.md) con reglas adicionales.

## Reglas Específicas de Pagos

### SIEMPRE
- Revisión de seguridad obligatoria antes del merge
- Auditoría de cada cambio por compliance officer
- Tests de regresión de transacciones completos
- Logging de todas las operaciones

### NUNCA
- Autonomía completa de la IA (máximo: propuesta)
- Cambios sin revisión humana doble
- Modificar lógica de cálculo sin sign-off de finanzas
- Almacenar datos sensibles en logs

## Niveles de Autonomía

| Tipo de Cambio | Autonomía Permitida |
|----------------|---------------------|
| Refactor de UI de pagos | Con revisión humana |
| Lógica de cálculo | Solo propuesta |
| Integración con pasarelas | Solo propuesta + security review |
| Esquema de datos de transacciones | Prohibido sin aprobación múltiple |

## Responsables

| Rol | Persona | Autoridad |
|-----|---------|-----------|
| Domain Owner | [Nombre] | Aprobación final |
| Security Lead | [Nombre] | Sign-off de seguridad |
| Compliance | [Nombre] | Validación regulatoria |

## Evidencias Obligatorias

- [ ] Tests de transacciones end-to-end
- [ ] Escaneo de seguridad DAST
- [ ] Revisión de código por 2 humanos
- [ ] Sign-off de compliance
- [ ] Verificación de cifrado de datos
```

---

## ★ POLICY-TICKET-TEMPLATE: Template Operativo Estandarizado ★

> **"Un Policy Ticket sin estructura es una intención sin forma. El template estandarizado garantiza que ningún elemento crítico quede sin definir."**

El siguiente template operativo materializa los conceptos del marco IA-Native en un artefacto concreto y reutilizable.

### Template Completo

```yaml
---
# ═══════════════════════════════════════════════════════════════
#                      POLICY TICKET
# ═══════════════════════════════════════════════════════════════
id: PT-2026-001
created: 2026-01-24
status: draft | active | in_progress | completed | rejected
priority: low | medium | high | critical

# ─────────────────────────────────────────────────────────────────
#                     RESPONSABILIDAD
# ─────────────────────────────────────────────────────────────────
owner:
  name: "Nombre Apellido"
  email: "nombre.apellido@empresa.com"
  role: "Tech Lead"

approvers:
  - name: "Aprobador 1"
    role: "Security Lead"
    required: true
  - name: "Aprobador 2"
    role: "Domain Owner"
    required: false

# ─────────────────────────────────────────────────────────────────
#                     CLASIFICACIÓN
# ─────────────────────────────────────────────────────────────────
domain: payments | security | backend | frontend | testing | docs
risk_level: low | medium | high | critical
skill_invoked: refactor-policy | security-change | api-change | ...

# ─────────────────────────────────────────────────────────────────
#                     AUTONOMÍA
# ─────────────────────────────────────────────────────────────────
autonomy:
  analysis: full | supervised | human_only
  generation: full | with_review | proposal_only
  testing: full | with_review | human_only
  integration: with_approval | multi_approval | human_only
---

## Intención

[Descripción clara, específica y acotada de qué se busca lograr.
Debe responder: ¿Qué problema resuelve? ¿Por qué ahora?]

## Alcance

### Acciones Permitidas
- [ ] [Acción específica 1]
- [ ] [Acción específica 2]
- [ ] [Acción específica 3]

### Acciones Prohibidas
- ❌ [Anti-patrón explícito 1]
- ❌ [Límite que no debe cruzarse]
- ❌ [Acción fuera de scope]

### Archivos/Módulos Afectados
```
src/
├── module-a/     ✓ En scope
├── module-b/     ✓ En scope
├── module-c/     ❌ Fuera de scope
└── core/         ⚠️ Solo lectura
```

## Evidencias Requeridas

### Antes de Ejecutar
- [ ] Tests existentes pasan (cobertura actual: __%)
- [ ] Análisis estático sin warnings críticos
- [ ] Revisión de dependencias completada

### Después de Ejecutar
- [ ] Tests pasan (cobertura: __% → __%)
- [ ] Análisis estático sin nuevos warnings
- [ ] Escaneo de seguridad completado
- [ ] Documentación actualizada
- [ ] PR creado con referencia a este Policy Ticket

### Para Dominios de Alto Riesgo
- [ ] Revisión de seguridad dedicada
- [ ] Sign-off de compliance
- [ ] Auditoría de datos sensibles

## Criterios de Aceptación

1. [Condición específica y verificable 1]
2. [Condición específica y verificable 2]
3. [Condición específica y verificable 3]

## Criterios de Rechazo

El cambio será rechazado si:
- [ ] Introduce regresiones en tests existentes
- [ ] Aumenta complejidad ciclomática significativamente
- [ ] Contiene vulnerabilidades de seguridad
- [ ] No produce las evidencias requeridas
- [ ] Excede el alcance definido

## Economía

```
PRESUPUESTO:
├── Tokens estimados: [número]
├── Costo estimado: $[valor]
├── Modelo autorizado: [claude-opus | claude-sonnet | gpt-4 | ...]
└── Límite hard: $[máximo]

POST-EJECUCIÓN:
├── Tokens consumidos: [número]
├── Costo real: $[valor]
└── Variación: [%]
```

## Trazabilidad

```
REFERENCIAS:
├── Issue/Ticket: [JIRA-123 | GH-456]
├── PRs asociados: [#PR1, #PR2]
├── Commits: [hash1, hash2]
└── Policy Tickets relacionados: [PT-2025-xxx]

TIMELINE:
├── Creado: [fecha]
├── Aprobado: [fecha]
├── Iniciado: [fecha]
├── Completado: [fecha]
└── Duración total: [horas/días]
```

## Notas y Contexto

[Cualquier información adicional relevante para entender la decisión.
Historia, trade-offs considerados, alternativas descartadas.]

---
*Policy Ticket generado bajo el Marco IA-Native v1.0*
*Responsable último: [Nombre del Owner]*
```

### Versión Simplificada para Dominios de Bajo Riesgo

```yaml
---
id: PT-2026-002
owner: nombre.apellido@empresa.com
domain: testing
risk_level: low
skill_invoked: test-generation
autonomy: full | with_review
---

## Intención
Generar tests unitarios para el módulo X

## Alcance
- ✓ Archivos en src/module-x/**/*.ts
- ❌ No modificar código de producción

## Evidencias
- [ ] Cobertura aumenta de X% a Y%
- [ ] Tests pasan en CI

## Aceptación
Tests generados cubren casos edge identificados
```

---

## ★ Meta-Instrumentos de Gobernanza ★

> **"Así como existen meta-skills que gestionan el sistema de skills, deben existir meta-instrumentos que gestionen el propio sistema de gobernanza."**

El marco IA-Native incluye instrumentos especiales que facilitan la creación, mantenimiento y auditoría de la gobernanza misma.

### policy-creator

**Propósito:** Guiar la creación de Policy Tickets correctamente estructurados.

**Uso:**
```
@policy-creator Crear Policy Ticket para refactorizar el módulo de autenticación
```

**Flujo:**
1. Identifica el dominio y nivel de riesgo
2. Selecciona el skill de gobernanza aplicable
3. Genera el Policy Ticket con el template apropiado
4. Sugiere responsables y aprobadores según el dominio
5. Pre-completa evidencias requeridas según el riesgo

### policy-sync

**Propósito:** Sincronizar Policy Tickets con sistemas externos y mantener coherencia.

**Uso:**
```bash
./governance/policy-sync.sh --source jira --target github
./governance/policy-sync.sh --validate  # Solo verificar coherencia
./governance/policy-sync.sh --report    # Generar reporte de estado
```

**Funciones:**
- Sincroniza Policy Tickets entre Jira/Azure DevOps y GitHub
- Verifica que cada PR tenga Policy Ticket vinculado
- Detecta Policy Tickets huérfanos (sin PRs asociados)
- Genera alertas de inconsistencias

### evidence-validator

**Propósito:** Verificar que las evidencias requeridas existen antes del merge.

**Implementación como GitHub Action:**
```yaml
name: Evidence Validator
on:
  pull_request:
    types: [opened, synchronize, ready_for_review]

jobs:
  validate-evidence:
    runs-on: ubuntu-latest
    steps:
      - name: Extract Policy Ticket ID
        run: |
          PT_ID=$(echo "${{ github.event.pull_request.body }}" | grep -oP 'PT-\d{4}-\d+')
          echo "policy_ticket=$PT_ID" >> $GITHUB_OUTPUT

      - name: Validate Evidence
        run: |
          # Verificar que existen las evidencias requeridas según el Policy Ticket
          ./governance/evidence-validator.sh ${{ steps.extract.outputs.policy_ticket }}

      - name: Block if Missing Evidence
        if: failure()
        run: |
          echo "::error::Evidencias requeridas no encontradas para $PT_ID"
          exit 1
```

### governance-audit

**Propósito:** Auditar la coherencia entre políticas declaradas y prácticas reales.

**Reporte generado:**
```
═══════════════════════════════════════════════════════════════
                   GOVERNANCE AUDIT REPORT
                      2026-01-24
═══════════════════════════════════════════════════════════════

RESUMEN EJECUTIVO:
├── Policy Tickets activos: 47
├── PRs sin Policy Ticket: 3 ⚠️
├── Policy Tickets sin PR: 5
├── Tasa de cumplimiento: 94%
└── Dominios auditados: 6

VIOLACIONES DETECTADAS:
┌─────────────────────────────────────────────────────────────┐
│ PR #1234: Sin Policy Ticket vinculado                       │
│ PR #1238: Dominio alto riesgo con autonomía completa        │
│ PR #1245: Evidencias incompletas                            │
└─────────────────────────────────────────────────────────────┘

MÉTRICAS POR DOMINIO:
├── payments: 100% cumplimiento ✓
├── security: 100% cumplimiento ✓
├── backend: 91% cumplimiento
├── frontend: 95% cumplimiento
├── testing: 88% cumplimiento
└── docs: 85% cumplimiento

RECOMENDACIONES:
1. Revisar configuración de branch protection para frontend
2. Entrenar equipo de docs en flujo de Policy Tickets
3. Implementar alertas automáticas para PRs sin PT
```

---

## ★ Sincronización Automática de Gobernanza ★

> **"La documentación que no se sincroniza automáticamente se desactualiza inevitablemente. La gobernanza efectiva requiere automatización."**

### governance-sync.sh

Script central que mantiene la coherencia del sistema de gobernanza:

```bash
#!/bin/bash
# governance-sync.sh - Sincronización de gobernanza IA-Native

set -euo pipefail

# ═══════════════════════════════════════════════════════════════
# CONFIGURACIÓN
# ═══════════════════════════════════════════════════════════════

declare -A SCOPE_PATHS=(
    [root]="./GOVERNANCE.md"
    [payments]="./domains/payments/GOVERNANCE.md"
    [security]="./domains/security/GOVERNANCE.md"
    [backend]="./domains/backend/GOVERNANCE.md"
    [frontend]="./domains/frontend/GOVERNANCE.md"
    [testing]="./domains/testing/GOVERNANCE.md"
    [docs]="./domains/documentation/GOVERNANCE.md"
)

SKILLS_DIR="./skills-governance"
DRY_RUN=false
VERBOSE=false

# ═══════════════════════════════════════════════════════════════
# FUNCIONES PRINCIPALES
# ═══════════════════════════════════════════════════════════════

parse_skill_metadata() {
    local skill_file="$1"
    # Extrae name, scope, auto_invoke, risk_level del frontmatter YAML
    # ...
}

generate_auto_invoke_table() {
    local scope="$1"
    # Genera tabla markdown de acciones → skills
    # Ordenado alfabéticamente para diffs limpios
    # ...
}

update_governance_file() {
    local scope="$1"
    local target_file="${SCOPE_PATHS[$scope]}"
    # Inserta/reemplaza sección "Auto-invocación" en el archivo
    # ...
}

validate_policy_tickets() {
    # Verifica que todos los PRs abiertos tienen Policy Ticket
    # ...
}

generate_audit_report() {
    # Genera reporte de cumplimiento
    # ...
}

# ═══════════════════════════════════════════════════════════════
# EJECUCIÓN
# ═══════════════════════════════════════════════════════════════

main() {
    parse_args "$@"

    echo "🔄 Sincronizando gobernanza IA-Native..."

    for scope in "${!SCOPE_PATHS[@]}"; do
        if [[ "$SPECIFIC_SCOPE" == "" || "$SPECIFIC_SCOPE" == "$scope" ]]; then
            echo "  → Procesando scope: $scope"
            update_governance_file "$scope"
        fi
    done

    if [[ "$VALIDATE" == "true" ]]; then
        validate_policy_tickets
    fi

    if [[ "$REPORT" == "true" ]]; then
        generate_audit_report
    fi

    echo "✅ Sincronización completada"
}

main "$@"
```

### Modos de Ejecución

```bash
# Sincronizar todos los scopes
./governance/governance-sync.sh

# Preview sin modificar archivos
./governance/governance-sync.sh --dry-run

# Sincronizar solo un dominio
./governance/governance-sync.sh --scope payments

# Verbose para debugging
./governance/governance-sync.sh --verbose

# Solo validar coherencia
./governance/governance-sync.sh --validate

# Generar reporte de auditoría
./governance/governance-sync.sh --report

# Combinaciones
./governance/governance-sync.sh --dry-run --verbose --scope backend
```

### Integración con CI/CD

```yaml
# .github/workflows/governance-sync.yml
name: Governance Sync

on:
  push:
    paths:
      - 'skills-governance/**'
      - 'domains/**'
  pull_request:
    paths:
      - 'skills-governance/**'
      - 'domains/**'

jobs:
  sync:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4

      - name: Run Governance Sync
        run: ./governance/governance-sync.sh --validate

      - name: Check for Uncommitted Changes
        run: |
          if [[ -n $(git status --porcelain) ]]; then
            echo "::error::Governance files are out of sync. Run governance-sync.sh locally."
            exit 1
          fi
```

---

# PARTE VIII: CONFIGURACIÓN Y HERRAMIENTAS

## ★ Configuración Multi-Agente bajo Marco IA-Native ★

> **"Cada agente de IA tiene su propia forma de descubrir contexto. El marco debe adaptarse para gobernar todos los agentes que la organización utiliza."**

El marco IA-Native debe configurarse específicamente para cada tipo de agente que la organización emplea.

### Matriz de Configuración por Agente

| Agente | Mecanismo de Contexto | Configuración IA-Native |
|--------|----------------------|-------------------------|
| **Claude Code** | `.claude/` + CLAUDE.md | Symlink a skills-governance + reglas en CLAUDE.md |
| **Cursor** | `.cursor/rules` | Archivo de reglas con guardrails |
| **GitHub Copilot** | `.github/copilot-instructions.md` | Instrucciones de gobernanza |
| **OpenAI Codex** | `.codex/` | Symlink a skills-governance |
| **Devin** | Instrucciones Slack/Web | Template de instrucción con Policy Ticket |
| **Gemini CLI** | `.gemini/` | Symlink a skills-governance |

### Configuración para Claude Code

**Estructura:**
```
.claude/
├── skills-governance/   → Symlink a ./skills-governance/
└── CLAUDE.md           → Reglas específicas de gobernanza
```

**Contenido de .claude/CLAUDE.md:**
```markdown
# Gobernanza IA-Native para Claude Code

## Antes de Cualquier Cambio

1. Verificar si existe Policy Ticket activo para la tarea
2. Si no existe, solicitar creación antes de proceder
3. Identificar el dominio y nivel de riesgo
4. Cargar el skill de gobernanza aplicable

## Reglas Obligatorias

- SIEMPRE vincular commits a Policy Ticket: `PT-XXXX-XXX`
- SIEMPRE producir evidencias antes de solicitar merge
- NUNCA exceder el nivel de autonomía definido en el Policy Ticket
- NUNCA modificar dominios de alto riesgo sin supervisión humana

## Niveles de Autonomía

| Dominio | Autonomía Permitida |
|---------|---------------------|
| testing, docs | Completa |
| frontend, backend | Con revisión |
| payments, security | Solo propuesta |

## Al Crear PRs

- Incluir referencia a Policy Ticket en título
- Listar evidencias producidas en descripción
- Solicitar reviewers según dominio
```

### Configuración para Cursor

**Archivo .cursor/rules:**
```
# Cursor Rules - Marco IA-Native

## Governance Rules

1. Every code change requires a Policy Ticket reference
2. Check domain risk level before proceeding
3. Load appropriate governance skill from ./skills-governance/

## Autonomy Levels

- LOW risk domains (testing, docs): Full autonomy
- MEDIUM risk domains (frontend, backend): Generate with review
- HIGH risk domains (payments, security): Proposal only

## Evidence Requirements

Before suggesting merge:
- [ ] Tests pass
- [ ] Static analysis clean
- [ ] Security scan complete
- [ ] Policy Ticket ID in commit message

## Prohibited Actions

- Never modify high-risk code without human approval
- Never skip evidence generation
- Never exceed defined autonomy level
```

### Configuración para GitHub Copilot

**Archivo .github/copilot-instructions.md:**
```markdown
# GitHub Copilot - Instrucciones de Gobernanza IA-Native

## Sistema de Gobernanza

Este proyecto utiliza el Marco IA-Native. Antes de generar código:

1. Verificar que existe Policy Ticket para la tarea
2. Consultar el skill de gobernanza en `./skills-governance/`
3. Respetar el nivel de autonomía del dominio

## Skills Disponibles

| Skill | Cuándo Usar |
|-------|-------------|
| refactor-policy | Refactorizando código |
| security-change | Modificando auth/crypto |
| api-change | Cambiando endpoints |
| test-generation | Generando tests |

## Restricciones

- Código en `src/payments/` → ALTO RIESGO → Solo sugerencias
- Código en `src/security/` → ALTO RIESGO → Solo sugerencias
- Código en `src/tests/` → BAJO RIESGO → Autonomía completa
```

### Configuración para Devin

**Template de Instrucción:**
```
## Contexto de Gobernanza

Este proyecto opera bajo el Marco IA-Native.

### Policy Ticket Activo
- ID: PT-2026-XXX
- Dominio: [dominio]
- Nivel de Riesgo: [nivel]
- Autonomía Permitida: [nivel]

### Acciones Autorizadas
1. [Acción 1]
2. [Acción 2]

### Acciones Prohibidas
- [Prohibición 1]
- [Prohibición 2]

### Evidencias Requeridas
Antes de reportar completado:
- [ ] Tests pasan
- [ ] Análisis estático limpio
- [ ] PR creado con referencia a PT-2026-XXX

### Responsable Humano
[Nombre] - [Email]
Contactar para cualquier ambigüedad o decisión fuera de scope.
```

### Script de Setup Multi-Agente

```bash
#!/bin/bash
# setup-agents.sh - Configura gobernanza para múltiples agentes

setup_claude() {
    echo "Configurando Claude Code..."
    mkdir -p .claude
    ln -sf ../skills-governance .claude/skills-governance
    cp governance/templates/CLAUDE-GOVERNANCE.md .claude/CLAUDE.md
}

setup_cursor() {
    echo "Configurando Cursor..."
    mkdir -p .cursor
    cp governance/templates/cursor-rules .cursor/rules
}

setup_copilot() {
    echo "Configurando GitHub Copilot..."
    mkdir -p .github
    cp governance/templates/copilot-instructions.md .github/copilot-instructions.md
}

setup_codex() {
    echo "Configurando OpenAI Codex..."
    mkdir -p .codex
    ln -sf ../skills-governance .codex/skills-governance
}

setup_gemini() {
    echo "Configurando Gemini CLI..."
    mkdir -p .gemini
    ln -sf ../skills-governance .gemini/skills-governance
}

case "$1" in
    --all)
        setup_claude
        setup_cursor
        setup_copilot
        setup_codex
        setup_gemini
        ;;
    --claude) setup_claude ;;
    --cursor) setup_cursor ;;
    --copilot) setup_copilot ;;
    --codex) setup_codex ;;
    --gemini) setup_gemini ;;
    *)
        echo "Uso: $0 [--all|--claude|--cursor|--copilot|--codex|--gemini]"
        exit 1
        ;;
esac

echo "✅ Configuración completada"
```

---

## ★ Integración con Herramientas Existentes ★

> **"El marco IA-Native no requiere abandonar las herramientas actuales. Las reconfigura para servir a la gobernanza."**

### Integración con Jira

**Tipo de Issue: Policy Ticket**

Crear un nuevo Issue Type en Jira con los campos:

| Campo | Tipo | Requerido |
|-------|------|-----------|
| PT-ID | Text (auto-generado) | Sí |
| Domain | Select (payments, security, backend, frontend, testing, docs) | Sí |
| Risk Level | Select (low, medium, high, critical) | Sí |
| Owner | User | Sí |
| Approvers | Multi-user | Según riesgo |
| Skill Invoked | Select | Sí |
| Autonomy Level | Select | Sí |
| Evidence Checklist | Checklist | Sí |
| Linked PRs | Link | No |
| Token Budget | Number | Opcional |
| Actual Tokens | Number | Opcional |

**Workflow:**
```
Draft → Approved → In Progress → Evidence Review → Completed
         ↓                              ↓
      Rejected                      Rejected
```

### Integración con Azure DevOps

**Work Item Type: Policy Ticket**

```xml
<WORKITEMTYPE name="Policy Ticket">
  <FIELDS>
    <FIELD name="PT-ID" refname="Custom.PTID" type="String" />
    <FIELD name="Domain" refname="Custom.Domain" type="String">
      <ALLOWEDVALUES>
        <LISTITEM value="payments" />
        <LISTITEM value="security" />
        <LISTITEM value="backend" />
        <LISTITEM value="frontend" />
        <LISTITEM value="testing" />
        <LISTITEM value="docs" />
      </ALLOWEDVALUES>
    </FIELD>
    <FIELD name="Risk Level" refname="Custom.RiskLevel" type="String" />
    <FIELD name="Autonomy Level" refname="Custom.AutonomyLevel" type="String" />
    <!-- ... más campos ... -->
  </FIELDS>
  <WORKFLOW>
    <STATES>
      <STATE value="Draft" />
      <STATE value="Approved" />
      <STATE value="In Progress" />
      <STATE value="Evidence Review" />
      <STATE value="Completed" />
      <STATE value="Rejected" />
    </STATES>
    <!-- ... transiciones ... -->
  </WORKFLOW>
</WORKITEMTYPE>
```

### Integración con GitHub Projects

**Template de Issue:**
```markdown
---
name: Policy Ticket
about: Crear un Policy Ticket para autorizar cambio con IA
title: '[PT] '
labels: policy-ticket
assignees: ''
---

## Información del Policy Ticket

**PT-ID:** PT-{{ date | date: '%Y' }}-XXX
**Dominio:** <!-- payments | security | backend | frontend | testing | docs -->
**Nivel de Riesgo:** <!-- low | medium | high | critical -->
**Skill Invocado:** <!-- refactor-policy | security-change | api-change | ... -->

## Responsable

**Owner:** @username
**Aprobadores:** @approver1, @approver2

## Intención

[Descripción clara de qué se busca lograr]

## Alcance

### Permitido
- [ ] Acción 1
- [ ] Acción 2

### Prohibido
- ❌ Límite 1
- ❌ Límite 2

## Autonomía

| Fase | Nivel |
|------|-------|
| Análisis | full / supervised / human_only |
| Generación | full / with_review / proposal_only |
| Testing | full / with_review / human_only |
| Integración | with_approval / multi_approval / human_only |

## Evidencias Requeridas

- [ ] Tests pasan
- [ ] Análisis estático limpio
- [ ] Escaneo de seguridad
- [ ] [Otras según dominio]

## PRs Asociados

<!-- Agregar links a PRs cuando se creen -->
```

### GitHub Actions para Validación

```yaml
# .github/workflows/policy-ticket-validation.yml
name: Policy Ticket Validation

on:
  pull_request:
    types: [opened, synchronize, ready_for_review]

jobs:
  validate-policy-ticket:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4

      - name: Check Policy Ticket Reference
        id: check-pt
        run: |
          PT_PATTERN="PT-[0-9]{4}-[0-9]+"
          PR_BODY="${{ github.event.pull_request.body }}"
          PR_TITLE="${{ github.event.pull_request.title }}"

          if [[ "$PR_TITLE" =~ $PT_PATTERN ]] || [[ "$PR_BODY" =~ $PT_PATTERN ]]; then
            PT_ID=$(echo "$PR_TITLE $PR_BODY" | grep -oE "$PT_PATTERN" | head -1)
            echo "pt_id=$PT_ID" >> $GITHUB_OUTPUT
            echo "✅ Policy Ticket encontrado: $PT_ID"
          else
            echo "::error::No se encontró referencia a Policy Ticket (PT-XXXX-XXX)"
            exit 1
          fi

      - name: Validate Domain and Approvers
        run: |
          # Verificar que los reviewers asignados corresponden al dominio
          ./governance/validate-approvers.sh ${{ steps.check-pt.outputs.pt_id }}

      - name: Check Evidence
        run: |
          # Verificar que las evidencias requeridas están presentes
          ./governance/evidence-validator.sh ${{ steps.check-pt.outputs.pt_id }}
```

### Dashboard de Gobernanza

**Métricas a visualizar:**

```
┌─────────────────────────────────────────────────────────────────┐
│                 GOVERNANCE DASHBOARD                            │
├─────────────────────────────────────────────────────────────────┤
│                                                                 │
│  Policy Tickets     PRs Gobernados    Tasa Cumplimiento        │
│  ┌─────────┐        ┌─────────┐       ┌─────────────┐          │
│  │   47    │        │   52    │       │    94%      │          │
│  │ activos │        │ /semana │       │             │          │
│  └─────────┘        └─────────┘       └─────────────┘          │
│                                                                 │
│  Por Dominio:                                                   │
│  ████████████████████ payments: 100%                           │
│  ████████████████████ security: 100%                           │
│  ██████████████████░░ backend: 91%                             │
│  ███████████████████░ frontend: 95%                            │
│  █████████████████░░░ testing: 88%                             │
│  ████████████████░░░░ docs: 85%                                │
│                                                                 │
│  Alertas Activas:                                               │
│  ⚠️ 3 PRs sin Policy Ticket                                    │
│  ⚠️ 5 Policy Tickets sin actividad > 7 días                    │
│                                                                 │
└─────────────────────────────────────────────────────────────────┘
```

---

## GitHub como Sistema Nervioso del Framework

En un entorno tradicional, GitHub cumple una función principalmente logística: alojar repositorios, gestionar ramas, recibir pull requests y ejecutar pipelines básicos. En un marco IA-Native, esa visión es insuficiente. Cuando agentes de IA generan cambios a alta velocidad, el repositorio deja de ser un "depósito" y se convierte en el lugar donde ocurre el gobierno real del sistema. GitHub (o GitLab) pasa a ser el plano donde se implementan políticas, se capturan evidencias, se ejerce control y se deja trazabilidad auditable.

Esto ocurre por una razón simple: si la IA produce código en minutos, el cuello de botella ya no está en producir cambios, sino en integrarlos de forma segura. Y la integración segura se decide en el punto donde confluyen: PRs, checks, aprobaciones, ownership y protecciones de rama. Ese punto es GitHub.

### PR como Unidad Mínima de Cambio Gobernado

Dentro del framework, la unidad básica es el cambio gobernado. En la práctica operativa, ese cambio se materializa como un Pull Request. El PR deja de ser solo una solicitud de merge y pasa a ser: el contenedor formal de la intervención, el lugar donde se evidencia el alcance real, la superficie de auditoría, y el evento donde se decide si algo entra o no entra.

En IA-Native, el PR tiene que ser atómico. Un agente puede generar 20 cambios en minutos; si se consolidan en un PR gigante, se pierde control. Por eso, el marco empuja a PRs pequeños y trazables, con una relación clara entre intención y diff.

### Policy Ticket y PR: Vínculo Obligatorio

Para que el sistema sea gobernable, cada PR debe vincularse a un Policy Ticket. No importa si lo gestionás en Jira o en otra herramienta: el vínculo debe existir y ser visible.

Esto puede implementarse de forma simple y realista: incluir el identificador del Policy Ticket en el título del PR, o en el cuerpo del PR, o como label obligatorio, o mediante un check automático que bloquea el merge si no hay referencia.

El objetivo es que, ante cualquier incidente, se pueda reconstruir: qué intención autorizó el cambio, qué agente lo produjo, quién fue responsable de autorizarlo, y qué evidencias se generaron antes del merge.

### Checks y Gates: El Sistema Inmunológico

En el framework IA-Native, el control no se basa en "confiar en el criterio del equipo", sino en evidencia verificable. GitHub implementa esto mediante checks y pipelines que actúan como un sistema inmunológico: tests unitarios e integración, lint y formateo, cobertura mínima, análisis estático (SAST), escaneo de dependencias y supply chain, detección de secretos, y, cuando aplica, tests e2e.

La lógica es clara: la IA puede producir cambios rápido; GitHub decide si esos cambios "sobreviven" al sistema inmunológico. Esto transforma el rol del CI/CD: deja de ser "un paso" y pasa a ser la frontera entre lo posible y lo permitido.

### Branch Protection: Política Ejecutable

En entornos IA-Native, las reglas no pueden ser solo recomendaciones. Deben ser políticas ejecutables. GitHub lo permite con branch protection: prohibir merge directo a main, exigir PR, exigir checks exitosos, exigir aprobación de code owners, impedir force push, exigir firmas/verified commits si el contexto lo pide, y definir reglas distintas por rama o por repositorio.

Esto vuelve tangible un principio central del framework: lo que no está gobernado, no entra.

### CODEOWNERS: Límites por Dominio

Uno de los mayores riesgos de la IA es que haga refactors transversales que rompan invariantes de dominio. CODEOWNERS funciona como un mecanismo de frontera estructural: cada carpeta o dominio tiene responsables, ciertas áreas requieren aprobación obligatoria, y los dominios críticos tienen dueños más estrictos.

Esto implementa "responsabilidad indelegable" en forma de infraestructura: no depende de memoria ni buena voluntad.

### Auditoría: GitHub como Registro Probatorio

Una de las ventajas más fuertes de usar GitHub como núcleo de gobierno es que deja evidencia: quién aprobó qué, cuándo, con qué checks, con qué cambios, y qué discusiones ocurrieron.

En un marco IA-Native esto no es secundario. Es parte del producto. La trazabilidad ya no es "documentación extra": es la forma de sostener calidad, seguridad y responsabilidad en un sistema acelerado.

Por eso, la organización debe considerar GitHub como un componente de compliance interno, no solo un repositorio.

### Integración con Agentes

Cuando un agente genera cambios, idealmente opera así: recibe un Policy Ticket (o su equivalente), produce cambios acotados, abre PRs atómicos con referencia obligatoria, espera el resultado de checks, si falla, corrige o escala a humano, si pasa, queda listo para aprobación humana según riesgo.

Este flujo hace que la IA sea un ejecutor poderoso, pero no un actor soberano.

### Qué Cambia en la Vida Diaria del Equipo

Con GitHub como sistema nervioso, las prácticas cambian: la daily se vuelve revisión de PRs, checks fallidos y bloqueos; QA se centra en diseñar gates más que en testear manualmente; Tech Leads definen límites de dominio via CODEOWNERS y políticas de protección; el equipo aprende a trabajar con PRs pequeños, verificables y trazables.

La velocidad real deja de ser "cuánto código producimos" y pasa a ser: cuánto cambio seguro podemos integrar por unidad de tiempo.

---

# PARTE IX: INGENIERÍA DE PROMPTS

## ★ Ingeniería de Prompts: El Lenguaje de la Delegación Cognitiva ★

> **"El prompt no es una instrucción casual; es un contrato de comunicación entre el humano y la IA. La precisión del prompt determina la calidad del output y la gobernabilidad del proceso."**

En el marco IA-Native, la ingeniería de prompts trasciende la simple redacción de instrucciones. Se convierte en una disciplina fundamental que conecta la intención humana con la ejecución de agentes, y los Policy Tickets con resultados verificables.

### Fundamentos de la Ingeniería de Prompts

**Definición Operativa:**

La ingeniería de prompts es el diseño sistemático de instrucciones que maximizan la probabilidad de obtener outputs predecibles, verificables y alineados con la intención declarada.

**Componentes de un Prompt Efectivo:**

```
┌─────────────────────────────────────────────────────────────────┐
│                    ANATOMÍA DEL PROMPT                          │
├─────────────────────────────────────────────────────────────────┤
│                                                                 │
│  1. CONTEXTO           ← Quién eres, dónde operas               │
│  2. ROL                ← Qué papel asumes                       │
│  3. TAREA              ← Qué debes hacer específicamente        │
│  4. FORMATO            ← Cómo debe verse el output              │
│  5. RESTRICCIONES      ← Qué NO debes hacer                     │
│  6. EJEMPLOS           ← Casos concretos (few-shot)             │
│  7. CRITERIOS          ← Cómo evaluar el éxito                  │
│                                                                 │
└─────────────────────────────────────────────────────────────────┘
```

### Técnicas Fundamentales

#### 1. Zero-Shot Prompting

Instrucción directa sin ejemplos previos. Útil para tareas simples y bien definidas.

```
PROMPT:
"Genera una función TypeScript que valide emails usando regex.
Debe retornar true/false y manejar casos edge como dominios con guiones."

USO: Tareas atómicas, código utilitario, operaciones estándar
RIESGO: Bajo
AUTONOMÍA: Completa con verificación
```

#### 2. Few-Shot Prompting

Proporcionar ejemplos que demuestren el patrón esperado.

```
PROMPT:
"Convierte estos mensajes de error a formato user-friendly:

Ejemplo 1:
- Input: "NullPointerException at line 45"
- Output: "Ocurrió un error inesperado. Por favor intenta de nuevo."

Ejemplo 2:
- Input: "Connection timeout after 30000ms"
- Output: "No pudimos conectar con el servidor. Verifica tu conexión."

Ahora convierte:
- Input: "Invalid token: JWT expired"
- Output: ???"

USO: Transformaciones con estilo específico, formateo consistente
RIESGO: Bajo-Medio
AUTONOMÍA: Con revisión
```

#### 3. Chain-of-Thought (CoT)

Solicitar razonamiento explícito paso a paso antes de la respuesta final.

```
PROMPT:
"Analiza esta función para identificar vulnerabilidades de seguridad.

Piensa paso a paso:
1. Identifica los inputs del usuario
2. Rastrea cómo fluyen a través de la función
3. Busca puntos donde podrían inyectarse datos maliciosos
4. Evalúa las validaciones existentes
5. Lista las vulnerabilidades encontradas con severidad

Código a analizar:
[código]"

USO: Análisis de seguridad, debugging complejo, decisiones arquitectónicas
RIESGO: Medio-Alto
AUTONOMÍA: Propuesta con validación humana
```

#### 4. Self-Consistency

Generar múltiples respuestas y seleccionar la más consistente.

```
PROMPT:
"Genera 3 implementaciones diferentes de esta función.
Para cada una, explica trade-offs de rendimiento vs legibilidad.
Luego selecciona la mejor opción justificando la decisión.

Función requerida: [especificación]"

USO: Decisiones de diseño, optimizaciones, refactoring
RIESGO: Medio
AUTONOMÍA: Con revisión
```

#### 5. ReAct (Reasoning + Acting)

Combinar razonamiento con acciones, útil para agentes que interactúan con herramientas.

```
PROMPT:
"Para resolver este bug, sigue el patrón ReAct:

THOUGHT: Analiza qué información necesitas
ACTION: Ejecuta una acción (leer archivo, buscar, etc.)
OBSERVATION: Observa el resultado
THOUGHT: Razona sobre lo observado
ACTION: Siguiente acción basada en el razonamiento
...
ANSWER: Solución final con justificación

Bug a resolver: [descripción]"

USO: Debugging, investigación de código, exploración de codebase
RIESGO: Bajo-Medio
AUTONOMÍA: Completa para investigación, supervisada para cambios
```

#### 6. Tree of Thoughts (ToT)

Explorar múltiples caminos de razonamiento en paralelo.

```
PROMPT:
"Diseña la arquitectura para este feature usando Tree of Thoughts:

RAMA 1: Enfoque monolítico
├── Pros: [analiza]
├── Cons: [analiza]
└── Viabilidad: [1-10]

RAMA 2: Enfoque microservicios
├── Pros: [analiza]
├── Cons: [analiza]
└── Viabilidad: [1-10]

RAMA 3: Enfoque híbrido
├── Pros: [analiza]
├── Cons: [analiza]
└── Viabilidad: [1-10]

SÍNTESIS: Recomendación final con justificación"

USO: Decisiones arquitectónicas, planificación de features complejos
RIESGO: Medio-Alto
AUTONOMÍA: Solo propuesta
```

### Ingeniería de Prompts para Policy Tickets

El marco IA-Native requiere que los Policy Tickets se traduzcan en prompts efectivos para los agentes. Esta traducción sigue un patrón estructurado:

#### Template: De Policy Ticket a Prompt de Agente

```markdown
## PROMPT GENERADO DESDE POLICY TICKET

### CONTEXTO DEL SISTEMA
Eres un agente de desarrollo operando bajo el Marco IA-Native.
- Policy Ticket: {PT-ID}
- Dominio: {domain}
- Nivel de Riesgo: {risk_level}
- Autonomía Permitida: {autonomy_level}

### TU ROL
Actúas como {rol} especializado en {dominio}.
Tu responsable humano es {owner_name} ({owner_email}).

### TAREA AUTORIZADA
{intención del Policy Ticket}

### ALCANCE PERMITIDO
Puedes:
{lista de acciones permitidas}

### RESTRICCIONES ABSOLUTAS
NO puedes bajo ninguna circunstancia:
{lista de acciones prohibidas}

### ARCHIVOS EN SCOPE
```
{estructura de archivos permitidos}
```

### FORMATO DE OUTPUT
1. Código con comentarios explicativos
2. Tests que validen el cambio
3. Documentación de decisiones tomadas

### EVIDENCIAS A PRODUCIR
Antes de considerar la tarea completa:
{checklist de evidencias requeridas}

### CRITERIOS DE ÉXITO
El cambio será válido si:
{criterios de aceptación}

### CRITERIOS DE RECHAZO
El cambio será rechazado si:
{criterios de rechazo}

### AL FINALIZAR
- Vincular commit a {PT-ID}
- Listar evidencias producidas
- Solicitar revisión a {approvers}
```

### Patrones de Prompts por Dominio

#### Prompts para Dominio de Bajo Riesgo (Testing, Docs)

```
CONTEXTO: Proyecto TypeScript con Jest
ROL: Generador de tests unitarios
AUTONOMÍA: Completa

TAREA:
Genera tests unitarios para la función {nombre} en {archivo}.

REQUISITOS:
- Cobertura mínima: 80%
- Incluir casos edge
- Seguir naming convention: "should [verbo] when [condición]"
- Un archivo de test por función

RESTRICCIONES:
- No modificar código de producción
- No agregar dependencias

OUTPUT:
- Archivo {nombre}.test.ts
- Resumen de casos cubiertos
```

#### Prompts para Dominio de Medio Riesgo (Backend, Frontend)

```
CONTEXTO: API REST con Express/TypeScript
ROL: Desarrollador backend con supervisión
AUTONOMÍA: Con revisión humana

TAREA:
Implementar endpoint {método} {ruta} según especificación OpenAPI adjunta.

REQUISITOS:
- Validación de input con Zod
- Manejo de errores consistente
- Logging estructurado
- Tests de integración

RESTRICCIONES:
- No modificar esquema de BD sin Policy Ticket separado
- No exponer datos sensibles en respuestas
- No usar dependencias no aprobadas

ANTES DE IMPLEMENTAR:
1. Explicar approach propuesto
2. Listar archivos a modificar
3. Identificar posibles impactos

ESPERAR APROBACIÓN ANTES DE PROCEDER
```

#### Prompts para Dominio de Alto Riesgo (Security, Payments)

```
CONTEXTO: Sistema de pagos con PCI-DSS compliance
ROL: Consultor de seguridad (solo propuestas)
AUTONOMÍA: Solo propuesta - NUNCA ejecutar cambios

TAREA:
Analizar la implementación actual de {componente} e identificar
mejoras de seguridad.

ENTREGABLES:
1. Análisis de vulnerabilidades actuales
2. Propuestas de mejora priorizadas por impacto
3. Estimación de esfuerzo por propuesta
4. Riesgos de cada cambio propuesto

RESTRICCIONES ABSOLUTAS:
- NO generar código ejecutable
- NO modificar ningún archivo
- NO acceder a datos de producción
- NO proponer cambios sin análisis de impacto

FORMAT:
Documento markdown con secciones:
- Executive Summary
- Hallazgos Detallados
- Recomendaciones
- Next Steps (para humanos)

DESPUÉS DE ENTREGAR:
Security Lead revisará y decidirá qué implementar.
```

### Anti-patrones en Ingeniería de Prompts

| Anti-patrón | Ejemplo | Por qué es Problemático | Corrección |
|-------------|---------|------------------------|------------|
| **Prompt Vago** | "Mejora este código" | Sin criterios, imposible verificar éxito | "Mejora el rendimiento de esta función reduciendo complejidad de O(n²) a O(n log n)" |
| **Sin Restricciones** | "Refactoriza el módulo de auth" | La IA podría cambiar cualquier cosa | "Refactoriza SOLO el módulo de auth. NO modificar: API contracts, esquema BD, tests existentes" |
| **Contexto Insuficiente** | "Agrega validación" | ¿Validación de qué? ¿Dónde? ¿Cómo? | "Agrega validación de email en el formulario de registro usando regex estándar RFC 5322" |
| **Autonomía Implícita** | "Soluciona el bug y despliega" | Asume permisos no otorgados | "Soluciona el bug. DETENTE antes de cualquier deploy y solicita aprobación" |
| **Sin Formato de Output** | "Analiza la arquitectura" | Output impredecible y no procesable | "Analiza la arquitectura. Output: JSON con {componentes: [], dependencias: [], riesgos: []}" |
| **Ejemplos Contradictorios** | Few-shot con estilos inconsistentes | Confunde al modelo sobre el patrón esperado | Asegurar consistencia en todos los ejemplos |
| **Cadena Infinita** | "Sigue mejorando hasta que esté perfecto" | Sin criterio de parada, loop infinito | "Realiza máximo 3 iteraciones. Criterio de parada: tests pasan Y coverage > 80%" |

### Métricas de Calidad de Prompts

```
┌─────────────────────────────────────────────────────────────────┐
│              PROMPT QUALITY SCORECARD                           │
├─────────────────────────────────────────────────────────────────┤
│                                                                 │
│  Especificidad        [████████░░] 80%                         │
│  ├── Tarea definida claramente                                 │
│  ├── Criterios de éxito medibles                               │
│  └── Formato de output especificado                            │
│                                                                 │
│  Restricciones        [██████████] 100%                        │
│  ├── Límites explícitos                                        │
│  ├── Anti-patrones documentados                                │
│  └── Scope de archivos definido                                │
│                                                                 │
│  Gobernabilidad       [█████████░] 90%                         │
│  ├── Policy Ticket referenciado                                │
│  ├── Responsable identificado                                  │
│  └── Evidencias requeridas listadas                            │
│                                                                 │
│  Verificabilidad      [███████░░░] 70%                         │
│  ├── Output es verificable automáticamente                     │
│  ├── Criterios de rechazo claros                               │
│  └── Tests incluidos en entregables                            │
│                                                                 │
│  SCORE TOTAL: 85/100                                           │
│  RECOMENDACIÓN: Mejorar verificabilidad con criterios          │
│                 cuantitativos y checks automáticos             │
│                                                                 │
└─────────────────────────────────────────────────────────────────┘
```

### Prompts para Orquestación de Agentes

Cuando se orquestan múltiples agentes, los prompts deben definir claramente la comunicación entre ellos:

#### Patrón: Supervisor-Worker

```
=== PROMPT PARA AGENTE SUPERVISOR ===

ROL: Supervisor de equipo de agentes
WORKERS DISPONIBLES:
- CodeGenerator: Genera código según especificación
- TestWriter: Escribe tests para código existente
- CodeReviewer: Revisa código buscando issues
- DocWriter: Genera documentación

TAREA: Implementar feature {descripción}

WORKFLOW REQUERIDO:
1. Descomponer tarea en subtareas
2. Asignar cada subtarea al worker apropiado
3. Verificar output de cada worker antes de continuar
4. Consolidar resultados
5. Validar que todos los criterios se cumplen

RESTRICCIONES:
- No ejecutar workers en paralelo para el mismo archivo
- Verificar tests pasan después de CodeGenerator
- CodeReviewer debe aprobar antes de DocWriter

OUTPUT FINAL:
- Código implementado
- Tests pasando
- Review aprobada
- Documentación generada
```

#### Patrón: Pipeline Secuencial

```
=== PROMPT PARA AGENTE EN PIPELINE ===

ETAPA: 2 de 4 (TestWriter)
ENTRADA: Código del agente anterior (CodeGenerator)
SALIDA: Tests + Código original → Agente siguiente (CodeReviewer)

CONTEXTO HEREDADO:
{output del agente anterior}

TU TAREA:
Escribir tests para el código recibido.

VALIDACIÓN ANTES DE PASAR:
- [ ] Tests ejecutan sin errores
- [ ] Cobertura > 80%
- [ ] Casos edge cubiertos

SI VALIDACIÓN FALLA:
- Retornar a etapa anterior con feedback específico
- NO pasar a siguiente etapa

SI VALIDACIÓN PASA:
- Empaquetar código + tests
- Pasar a CodeReviewer
```

### Prompt Engineering Toolkit

#### Generador de Prompts desde Policy Ticket

```python
def generate_prompt_from_policy_ticket(pt: PolicyTicket) -> str:
    """
    Transforma un Policy Ticket en un prompt estructurado para agentes.
    """
    template = f"""
## PROMPT AUTORIZADO - {pt.id}

### CONTEXTO
- Dominio: {pt.domain}
- Riesgo: {pt.risk_level}
- Autonomía: {pt.autonomy.generation}

### ROL
{get_role_for_domain(pt.domain)}

### TAREA
{pt.intention}

### SCOPE
Permitido:
{format_list(pt.allowed_actions)}

Prohibido:
{format_list(pt.prohibited_actions)}

### EVIDENCIAS REQUERIDAS
{format_checklist(pt.required_evidence)}

### CRITERIOS
Éxito: {pt.acceptance_criteria}
Rechazo: {pt.rejection_criteria}

### RESPONSABLE
{pt.owner.name} <{pt.owner.email}>
"""
    return template
```

#### Validador de Prompts

```python
def validate_prompt(prompt: str) -> PromptQualityReport:
    """
    Evalúa la calidad de un prompt según criterios del marco IA-Native.
    """
    checks = {
        "has_context": bool(re.search(r"CONTEXTO|CONTEXT", prompt)),
        "has_task": bool(re.search(r"TAREA|TASK", prompt)),
        "has_restrictions": bool(re.search(r"RESTRICCION|PROHIB|NEVER|NO DEBE", prompt)),
        "has_output_format": bool(re.search(r"OUTPUT|FORMAT|ENTREGABLE", prompt)),
        "has_success_criteria": bool(re.search(r"CRITERIO|SUCCESS|ÉXITO", prompt)),
        "has_policy_ticket": bool(re.search(r"PT-\d{4}-\d+", prompt)),
        "has_owner": bool(re.search(r"RESPONSABLE|OWNER|@\w+", prompt)),
    }

    score = sum(checks.values()) / len(checks) * 100

    return PromptQualityReport(
        score=score,
        checks=checks,
        recommendation=get_recommendations(checks)
    )
```

### Evolución del Prompt: Iteración Controlada

Los prompts no son estáticos. Deben evolucionar basándose en resultados:

```
┌─────────────────────────────────────────────────────────────────┐
│              CICLO DE MEJORA DE PROMPTS                         │
├─────────────────────────────────────────────────────────────────┤
│                                                                 │
│  ┌──────────┐      ┌──────────┐      ┌──────────┐              │
│  │  DISEÑO  │ ──→  │EJECUCIÓN │ ──→  │EVALUACIÓN│              │
│  │  Prompt  │      │  Agente  │      │ Resultado│              │
│  └──────────┘      └──────────┘      └──────────┘              │
│       ↑                                    │                    │
│       │                                    │                    │
│       │            ┌──────────┐            │                    │
│       └─────────── │  AJUSTE  │ ←──────────┘                    │
│                    │  Prompt  │                                 │
│                    └──────────┘                                 │
│                                                                 │
│  MÉTRICAS DE ITERACIÓN:                                        │
│  - Tasa de éxito del prompt                                    │
│  - Tiempo promedio de ejecución                                │
│  - Cantidad de retrabajos necesarios                           │
│  - Tokens consumidos por ejecución                             │
│                                                                 │
└─────────────────────────────────────────────────────────────────┘
```

### Biblioteca de Prompts Organizacional

Las organizaciones deben mantener una biblioteca de prompts probados:

```
prompts-library/
├── by-domain/
│   ├── testing/
│   │   ├── unit-test-generation.md
│   │   ├── integration-test-generation.md
│   │   └── e2e-test-generation.md
│   ├── security/
│   │   ├── vulnerability-analysis.md
│   │   ├── code-audit.md
│   │   └── dependency-check.md
│   └── refactoring/
│       ├── extract-function.md
│       ├── rename-variable.md
│       └── simplify-conditional.md
├── by-risk-level/
│   ├── low-risk-template.md
│   ├── medium-risk-template.md
│   └── high-risk-template.md
├── meta-prompts/
│   ├── generate-prompt-from-pt.md
│   ├── validate-prompt.md
│   └── improve-prompt.md
└── PROMPT-CATALOG.md  # Índice con descripciones y uso recomendado
```

### Integración con el Marco IA-Native

La ingeniería de prompts no es una disciplina aislada; se integra en cada aspecto del marco:

| Componente del Marco | Rol de la Ingeniería de Prompts |
|---------------------|--------------------------------|
| **Policy Tickets** | Template de traducción PT → Prompt |
| **Skills de Gobernanza** | Prompts específicos por skill |
| **Niveles de Autonomía** | Restricciones en prompts según autonomía |
| **Evidencias** | Prompts que producen evidencias verificables |
| **Auditoría** | Prompts almacenados como parte del audit trail |
| **Multi-agente** | Prompts de coordinación entre agentes |

### Conclusión: El Prompt como Contrato

En el marco IA-Native, el prompt no es una simple instrucción: es un **contrato de delegación**. Un prompt bien diseñado:

1. **Explicita la intención** sin ambigüedad
2. **Define los límites** de la acción autorizada
3. **Especifica las evidencias** que prueban el cumplimiento
4. **Conecta con la gobernanza** mediante Policy Tickets
5. **Habilita la trazabilidad** para auditorías futuras

La ingeniería de prompts es, en última instancia, la ingeniería de la comunicación entre humanos e inteligencia artificial. En un mundo donde los agentes ejecutan cambios reales, esta comunicación debe ser tan rigurosa como cualquier otra interfaz de sistema crítico.

---

# PARTE X: CICLO DE VIDA DE DESARROLLO IA-NATIVE (SDLC)

## ★ Visión General del SDLC IA-Native ★

> **"Un marco de gobernanza sin operacionalización en cada etapa del desarrollo es una promesa vacía. El marco IA-Native debe penetrar cada fase del ciclo de vida del software."**

El desarrollo de software en una Software Factory abarca múltiples etapas interdependientes. El marco IA-Native no es solo gobernanza abstracta; es un sistema operativo para cada fase del SDLC (Software Development Life Cycle).

### Mapa del Ciclo de Vida IA-Native

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                    CICLO DE VIDA DE DESARROLLO IA-NATIVE                    │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                             │
│  ┌──────────────┐    ┌──────────────┐    ┌──────────────┐                  │
│  │   ANÁLISIS   │───▶│ PLANIFICACIÓN│───▶│ ARQUITECTURA │                  │
│  │  REQUISITOS  │    │  Y ESTIMACIÓN│    │   Y DISEÑO   │                  │
│  └──────────────┘    └──────────────┘    └──────────────┘                  │
│         │                                        │                          │
│         ▼                                        ▼                          │
│  ┌──────────────────────────────────────────────────────────────┐          │
│  │                      DESARROLLO                              │          │
│  │  ┌──────────┐   ┌──────────┐   ┌──────────┐   ┌──────────┐  │          │
│  │  │ FRONTEND │   │ BACKEND  │   │  MOBILE  │   │   APIs   │  │          │
│  │  └──────────┘   └──────────┘   └──────────┘   └──────────┘  │          │
│  └──────────────────────────────────────────────────────────────┘          │
│                              │                                              │
│                              ▼                                              │
│  ┌──────────────┐    ┌──────────────┐    ┌──────────────┐                  │
│  │   TESTING    │───▶│    CI/CD     │───▶│  PRODUCCIÓN  │                  │
│  │    Y QA      │    │   DEVOPS     │    │  MONITOREO   │                  │
│  └──────────────┘    └──────────────┘    └──────────────┘                  │
│                                                                             │
│  ════════════════════════════════════════════════════════════════════════  │
│  │  GOBERNANZA TRANSVERSAL: Policy Tickets + Skills + Evidencias        │  │
│  ════════════════════════════════════════════════════════════════════════  │
│                                                                             │
└─────────────────────────────────────────────────────────────────────────────┘
```

### Matriz de Autonomía por Etapa

| Etapa | Autonomía IA Típica | Supervisión Humana | Riesgo Base |
|-------|--------------------|--------------------|-------------|
| Análisis de Requisitos | Propuesta + Validación | Alta | Medio |
| Planificación | Asistida | Alta | Medio |
| Arquitectura | Propuesta | Muy Alta | Alto |
| Desarrollo Frontend | Con revisión | Media | Bajo-Medio |
| Desarrollo Backend | Con revisión | Media-Alta | Medio |
| Desarrollo Mobile | Con revisión | Media-Alta | Medio |
| APIs | Con revisión | Alta | Medio-Alto |
| Testing | Completa (generación) | Baja | Bajo |
| CI/CD | Automatizada | Media | Medio |
| Producción/Deploy | Solo propuesta | Muy Alta | Alto |

---

## ★ Análisis de Requisitos con IA ★

> **"La IA puede acelerar la captura de requisitos, pero la validación de lo que el cliente realmente necesita sigue siendo irreductiblemente humana."**

### El Problema del Análisis de Requisitos Tradicional

- 60% de los defectos de software se originan en requisitos mal definidos
- El costo de corregir un requisito en producción es 100x mayor que en análisis
- Los stakeholders frecuentemente no saben expresar lo que necesitan

### Capacidades de la IA en Análisis de Requisitos

| Capacidad | Descripción | Autonomía |
|-----------|-------------|-----------|
| **Extracción de requisitos** | Procesar documentos, transcripciones, emails para identificar requisitos | Completa |
| **Clasificación** | Categorizar requisitos (funcional, no-funcional, restricción) | Completa |
| **Detección de ambigüedades** | Identificar requisitos vagos o contradictorios | Completa |
| **Generación de historias de usuario** | Convertir requisitos en formato estándar | Con revisión |
| **Análisis de impacto** | Evaluar cómo un requisito afecta al sistema existente | Con revisión |
| **Priorización** | Sugerir prioridad basada en dependencias y valor | Propuesta |
| **Validación con stakeholders** | Confirmar entendimiento | Solo humano |

### Workflow de Análisis de Requisitos IA-Native

```
┌─────────────────────────────────────────────────────────────────┐
│               ANÁLISIS DE REQUISITOS IA-NATIVE                  │
├─────────────────────────────────────────────────────────────────┤
│                                                                 │
│  1. CAPTURA (IA: Completa)                                     │
│     ┌──────────────────────────────────────────────────────┐   │
│     │ • Procesar documentos del cliente                    │   │
│     │ • Transcribir y analizar reuniones                   │   │
│     │ • Extraer requisitos de emails/chats                 │   │
│     │ • Generar lista inicial de requisitos                │   │
│     └──────────────────────────────────────────────────────┘   │
│                              ↓                                  │
│  2. ESTRUCTURACIÓN (IA: Completa)                              │
│     ┌──────────────────────────────────────────────────────┐   │
│     │ • Clasificar: Funcional / No-funcional / Restricción │   │
│     │ • Identificar dependencias entre requisitos          │   │
│     │ • Detectar ambigüedades y contradicciones            │   │
│     │ • Generar preguntas de clarificación                 │   │
│     └──────────────────────────────────────────────────────┘   │
│                              ↓                                  │
│  3. REFINAMIENTO (IA: Con revisión humana)                     │
│     ┌──────────────────────────────────────────────────────┐   │
│     │ • Convertir a User Stories (As a... I want... So...) │   │
│     │ • Definir criterios de aceptación                    │   │
│     │ • Estimar complejidad relativa (story points)        │   │
│     │ • HUMANO: Validar con stakeholders                   │   │
│     └──────────────────────────────────────────────────────┘   │
│                              ↓                                  │
│  4. VALIDACIÓN (Humano: Obligatorio)                           │
│     ┌──────────────────────────────────────────────────────┐   │
│     │ • Product Owner aprueba historias                    │   │
│     │ • Stakeholders confirman entendimiento               │   │
│     │ • Tech Lead valida viabilidad técnica                │   │
│     │ • Crear Policy Tickets para desarrollo               │   │
│     └──────────────────────────────────────────────────────┘   │
│                                                                 │
└─────────────────────────────────────────────────────────────────┘
```

### Template: User Story Generada por IA

```yaml
---
id: US-2026-042
generated_by: AI
validated_by: [pending]
status: draft
---

## Historia de Usuario

**Como** [rol del usuario]
**Quiero** [funcionalidad deseada]
**Para** [beneficio/valor de negocio]

## Origen
- Fuente: [Documento/Reunión/Email]
- Texto original: "[cita textual del requisito original]"
- Confianza de interpretación: [Alta/Media/Baja]

## Criterios de Aceptación
- [ ] Dado [contexto], cuando [acción], entonces [resultado esperado]
- [ ] Dado [contexto], cuando [acción], entonces [resultado esperado]
- [ ] Dado [contexto], cuando [acción], entonces [resultado esperado]

## Requisitos No Funcionales Relacionados
- Performance: [si aplica]
- Seguridad: [si aplica]
- Accesibilidad: [si aplica]

## Dependencias
- Depende de: [US-XXX, US-YYY]
- Bloquea a: [US-ZZZ]

## Estimación IA
- Complejidad: [Baja/Media/Alta]
- Story Points sugeridos: [1/2/3/5/8/13]
- Justificación: [razón de la estimación]

## Preguntas de Clarificación
1. [Pregunta generada por IA para resolver ambigüedad]
2. [Pregunta generada por IA para resolver ambigüedad]

## Validación Humana
- [ ] Product Owner aprobó
- [ ] Stakeholder confirmó entendimiento
- [ ] Tech Lead validó viabilidad
- Notas de validación: [comentarios humanos]
```

### Skill de Gobernanza: requirements-analysis

```yaml
---
name: requirements-analysis
description: >
  Políticas para análisis de requisitos asistido por IA.
  Trigger: Inicio de proyecto, nuevas features, cambios de alcance.
metadata:
  scope: [root]
  auto_invoke:
    - "Analizando documentos del cliente"
    - "Generando historias de usuario"
    - "Refinando requisitos"
  risk_level: medio
---

## Patrones Críticos

### SIEMPRE
- Citar la fuente original de cada requisito
- Indicar nivel de confianza en la interpretación
- Generar preguntas de clarificación para ambigüedades
- Requerir validación humana antes de considerar requisito "listo"

### NUNCA
- Asumir requisitos no explicitados
- Omitir requisitos no funcionales
- Considerar un requisito validado sin confirmación del stakeholder
- Estimar sin indicar que es sugerencia de IA
```

---

## ★ Planificación y Estimación con IA ★

> **"La IA puede analizar proyectos históricos para estimar, pero la responsabilidad de comprometerse con fechas es irreductiblemente humana."**

### Capacidades de IA en Planificación

| Capacidad | Descripción | Autonomía |
|-----------|-------------|-----------|
| **Estimación por analogía** | Comparar con proyectos históricos similares | Propuesta |
| **Descomposición de tareas** | Dividir features en tareas atómicas | Con revisión |
| **Detección de dependencias** | Identificar orden y bloqueos | Completa |
| **Análisis de riesgos** | Identificar riesgos basados en patrones | Con revisión |
| **Optimización de recursos** | Sugerir asignación de equipo | Propuesta |
| **Generación de cronograma** | Crear timeline inicial | Propuesta |
| **Compromiso de fechas** | Acordar deadlines | Solo humano |

### Framework de Estimación IA-Native

```
┌─────────────────────────────────────────────────────────────────┐
│                 ESTIMACIÓN IA-NATIVE                            │
├─────────────────────────────────────────────────────────────────┤
│                                                                 │
│  ENTRADA:                                                       │
│  ├── User Stories validadas                                    │
│  ├── Historial de proyectos anteriores                         │
│  ├── Capacidad del equipo (velocidad histórica)                │
│  └── Restricciones conocidas (fechas fijas, recursos)          │
│                                                                 │
│  PROCESO IA:                                                    │
│  ┌─────────────────────────────────────────────────────────┐   │
│  │ 1. Analizar complejidad de cada historia                │   │
│  │ 2. Buscar historias similares en proyectos pasados      │   │
│  │ 3. Aplicar factores de ajuste (tech nuevo, equipo, etc) │   │
│  │ 4. Calcular rangos de estimación (optimista/pesimista)  │   │
│  │ 5. Identificar riesgos que afectan estimación           │   │
│  │ 6. Generar cronograma tentativo                         │   │
│  └─────────────────────────────────────────────────────────┘   │
│                                                                 │
│  SALIDA:                                                        │
│  ├── Estimaciones con rangos (no puntos únicos)                │
│  ├── Nivel de confianza por estimación                         │
│  ├── Riesgos identificados con impacto en timeline             │
│  ├── Cronograma sugerido (NO comprometido)                     │
│  └── Advertencias y supuestos                                  │
│                                                                 │
│  ════════════════════════════════════════════════════════════  │
│  ⚠️ HUMANO DEBE: Revisar, ajustar, y comprometerse            │
│  ════════════════════════════════════════════════════════════  │
│                                                                 │
└─────────────────────────────────────────────────────────────────┘
```

### Template: Plan de Sprint IA-Native

```yaml
---
sprint: Sprint-2026-03
start_date: 2026-02-01
end_date: 2026-02-14
team_capacity: 40 story_points
generated_by: AI
approved_by: [pending]
---

## Objetivo del Sprint
[Objetivo claro y medible]

## Historias Seleccionadas

| ID | Historia | SP | Asignado | Dependencias | Riesgo |
|----|----------|----|---------:|--------------|--------|
| US-042 | [título] | 5 | Dev1 | - | Bajo |
| US-043 | [título] | 8 | Dev2 | US-042 | Medio |
| US-044 | [título] | 3 | Dev1 | - | Bajo |

**Total Story Points:** 35/40 (87.5% capacidad)
**Buffer para imprevistos:** 5 SP (12.5%)

## Estimación de Confianza IA

| Historia | Confianza | Justificación |
|----------|-----------|---------------|
| US-042 | Alta (85%) | Similar a US-012 del proyecto anterior |
| US-043 | Media (60%) | Tecnología nueva, pocos precedentes |
| US-044 | Alta (90%) | Tarea estándar, bien documentada |

## Riesgos del Sprint

| Riesgo | Probabilidad | Impacto | Mitigación |
|--------|--------------|---------|------------|
| Integración API externa | Media | +3 SP | Mockear inicialmente |
| Curva de aprendizaje tech X | Alta | +5 SP | Pair programming |

## Dependencias Externas
- [ ] API de pagos disponible para día 3
- [ ] Diseños finales entregados para día 1
- [ ] Ambiente de staging configurado

## Validación Humana Requerida

- [ ] Tech Lead aprobó estimaciones
- [ ] Product Owner confirmó prioridades
- [ ] Equipo aceptó compromisos
- Ajustes realizados: [documentar cambios]
```

---

## ★ Arquitectura de Software IA-Native ★

> **"La IA puede proponer arquitecturas, pero las decisiones arquitectónicas son irreversibles y costosas. Aquí la supervisión humana es máxima."**

### El Rol de la IA en Decisiones Arquitectónicas

| Tarea | Capacidad IA | Autonomía | Riesgo de Delegación |
|-------|-------------|-----------|---------------------|
| Analizar requisitos no funcionales | Alta | Completa | Bajo |
| Proponer patrones arquitectónicos | Alta | Solo propuesta | Alto |
| Evaluar trade-offs | Media | Solo propuesta | Alto |
| Documentar decisiones (ADRs) | Alta | Con revisión | Medio |
| Generar diagramas | Alta | Completa | Bajo |
| Validar consistencia | Alta | Completa | Bajo |
| Tomar decisión final | N/A | Solo humano | N/A |

### Patrones Arquitectónicos y Cuándo Aplicarlos

```
┌─────────────────────────────────────────────────────────────────┐
│          ÁRBOL DE DECISIÓN ARQUITECTÓNICA                       │
├─────────────────────────────────────────────────────────────────┤
│                                                                 │
│  ¿El sistema requiere alta escalabilidad independiente         │
│  de componentes?                                                │
│  │                                                              │
│  ├── SÍ → ¿Equipo tiene experiencia en sistemas distribuidos? │
│  │        │                                                     │
│  │        ├── SÍ → MICROSERVICIOS                              │
│  │        │        • Event-driven architecture                 │
│  │        │        • API Gateway                               │
│  │        │        • Service mesh                              │
│  │        │                                                     │
│  │        └── NO → MODULAR MONOLITH                            │
│  │                 • Clean Architecture interna                │
│  │                 • Preparado para split futuro               │
│  │                                                              │
│  └── NO → ¿Lógica de negocio compleja?                        │
│           │                                                     │
│           ├── SÍ → CLEAN ARCHITECTURE                          │
│           │        • Capas bien definidas                      │
│           │        • Inversión de dependencias                 │
│           │        • Use cases explícitos                      │
│           │                                                     │
│           └── NO → ARQUITECTURA SIMPLE (MVC/MVP)               │
│                    • Menor overhead                            │
│                    • Más rápido de implementar                 │
│                                                                 │
└─────────────────────────────────────────────────────────────────┘
```

### Clean Architecture en el Marco IA-Native

```
┌─────────────────────────────────────────────────────────────────┐
│                    CLEAN ARCHITECTURE                           │
├─────────────────────────────────────────────────────────────────┤
│                                                                 │
│                    ┌───────────────────┐                       │
│                    │     ENTITIES      │ ← Reglas de negocio   │
│                    │  (Domain Models)  │    más estables       │
│                    └─────────┬─────────┘                       │
│                              │                                  │
│              ┌───────────────┴───────────────┐                 │
│              │         USE CASES             │ ← Lógica de     │
│              │   (Application Business)      │    aplicación   │
│              └───────────────┬───────────────┘                 │
│                              │                                  │
│      ┌───────────────────────┴───────────────────────┐         │
│      │            INTERFACE ADAPTERS                 │         │
│      │  (Controllers, Gateways, Presenters)          │         │
│      └───────────────────────┬───────────────────────┘         │
│                              │                                  │
│  ┌───────────────────────────┴───────────────────────────┐     │
│  │              FRAMEWORKS & DRIVERS                     │     │
│  │  (Web, DB, External APIs, UI, Devices)                │     │
│  └───────────────────────────────────────────────────────┘     │
│                                                                 │
│  ════════════════════════════════════════════════════════════  │
│  REGLA DE DEPENDENCIA: Las flechas apuntan hacia adentro      │
│  (Las capas internas NO conocen las externas)                  │
│  ════════════════════════════════════════════════════════════  │
│                                                                 │
└─────────────────────────────────────────────────────────────────┘
```

### Estructura de Proyecto Clean Architecture

```
src/
├── domain/                    # CAPA: Entities
│   ├── entities/             # Objetos de dominio
│   │   ├── User.ts
│   │   ├── Order.ts
│   │   └── Product.ts
│   ├── value-objects/        # Objetos de valor inmutables
│   │   ├── Email.ts
│   │   ├── Money.ts
│   │   └── Address.ts
│   ├── repositories/         # Interfaces de repositorios
│   │   ├── IUserRepository.ts
│   │   └── IOrderRepository.ts
│   └── services/             # Servicios de dominio
│       └── PricingService.ts
│
├── application/               # CAPA: Use Cases
│   ├── use-cases/            # Casos de uso
│   │   ├── CreateOrder.ts
│   │   ├── ProcessPayment.ts
│   │   └── GetUserOrders.ts
│   ├── dto/                  # Data Transfer Objects
│   │   ├── CreateOrderDTO.ts
│   │   └── OrderResponseDTO.ts
│   └── interfaces/           # Puertos de aplicación
│       ├── IPaymentGateway.ts
│       └── INotificationService.ts
│
├── infrastructure/            # CAPA: Frameworks & Drivers
│   ├── persistence/          # Implementaciones de repositorios
│   │   ├── PostgresUserRepository.ts
│   │   └── RedisCache.ts
│   ├── external/             # Servicios externos
│   │   ├── StripePaymentGateway.ts
│   │   └── SendGridNotification.ts
│   ├── web/                  # Framework web
│   │   ├── express/
│   │   └── middleware/
│   └── config/               # Configuración
│       └── DependencyInjection.ts
│
└── presentation/              # CAPA: Interface Adapters
    ├── controllers/          # Controladores HTTP
    │   ├── OrderController.ts
    │   └── UserController.ts
    ├── presenters/           # Formateadores de respuesta
    └── validators/           # Validación de entrada
```

### Patrones de Diseño por Contexto

| Contexto | Patrones Recomendados | Uso |
|----------|----------------------|-----|
| **Creación de objetos** | Factory, Builder, Singleton | Cuando la creación es compleja |
| **Estructura** | Adapter, Decorator, Facade | Integración con externos |
| **Comportamiento** | Strategy, Observer, Command | Lógica variable, eventos |
| **Arquitectura** | Repository, Unit of Work | Acceso a datos |
| **Concurrencia** | Circuit Breaker, Retry | Resiliencia |
| **Messaging** | Pub/Sub, Event Sourcing | Comunicación asíncrona |

### Template: Architecture Decision Record (ADR)

```markdown
# ADR-001: [Título de la Decisión]

## Estado
[Propuesto | Aceptado | Deprecado | Reemplazado por ADR-XXX]

## Contexto
[Descripción del problema o situación que requiere una decisión]

## Decisión
[La decisión tomada y por qué]

## Alternativas Consideradas

### Alternativa 1: [Nombre]
- **Pros:** [lista]
- **Cons:** [lista]
- **Descartada porque:** [razón]

### Alternativa 2: [Nombre]
- **Pros:** [lista]
- **Cons:** [lista]
- **Descartada porque:** [razón]

## Consecuencias

### Positivas
- [consecuencia positiva 1]
- [consecuencia positiva 2]

### Negativas
- [consecuencia negativa 1]
- [trade-off aceptado]

### Riesgos
- [riesgo identificado y mitigación]

## Análisis IA
- **Generado por:** [modelo]
- **Confianza:** [Alta/Media/Baja]
- **Validado por:** [arquitecto humano]

## Referencias
- [links a documentación relevante]
```

### Skill de Gobernanza: architecture-decision

```yaml
---
name: architecture-decision
description: >
  Políticas para decisiones arquitectónicas asistidas por IA.
  Trigger: Diseño de sistema, cambios estructurales mayores.
metadata:
  scope: [root, backend, frontend]
  auto_invoke:
    - "Diseñando arquitectura de sistema"
    - "Evaluando patrones arquitectónicos"
    - "Creando ADRs"
  risk_level: alto
---

## Patrones Críticos

### SIEMPRE
- Documentar decisión en ADR antes de implementar
- Presentar múltiples alternativas con trade-offs
- Indicar nivel de confianza en la recomendación
- Requerir aprobación de arquitecto senior

### NUNCA
- Tomar decisiones arquitectónicas sin ADR
- Implementar antes de aprobación humana
- Omitir consecuencias negativas o riesgos
- Proponer arquitectura sin considerar requisitos no funcionales
```

---

## ★ Desarrollo Frontend con IA ★

> **"El frontend es la cara visible del sistema. La IA puede acelerar la implementación, pero la experiencia de usuario requiere sensibilidad humana."**

### Stack Frontend Moderno (2026)

| Capa | Tecnologías Principales | Rol de IA |
|------|------------------------|-----------|
| **Framework** | React 19, Next.js 15, Vue 3, Svelte 5 | Generación de componentes |
| **State Management** | Zustand, Jotai, TanStack Query | Sugerir estructura |
| **Styling** | Tailwind CSS 4, CSS Modules | Generar estilos |
| **Testing** | Vitest, Playwright, Testing Library | Generación completa |
| **Build** | Vite 6, Turbopack | Configuración |
| **Types** | TypeScript 5.x | Inferencia, generación |

### Estructura de Proyecto Frontend

```
frontend/
├── src/
│   ├── components/           # Componentes reutilizables
│   │   ├── ui/              # Componentes base (Button, Input, etc.)
│   │   │   ├── Button/
│   │   │   │   ├── Button.tsx
│   │   │   │   ├── Button.test.tsx
│   │   │   │   ├── Button.stories.tsx
│   │   │   │   └── index.ts
│   │   │   └── ...
│   │   └── composed/        # Componentes compuestos
│   │       ├── UserCard/
│   │       └── OrderList/
│   │
│   ├── features/            # Features por dominio
│   │   ├── auth/
│   │   │   ├── components/
│   │   │   ├── hooks/
│   │   │   ├── services/
│   │   │   └── store/
│   │   ├── orders/
│   │   └── products/
│   │
│   ├── hooks/               # Hooks globales
│   │   ├── useAuth.ts
│   │   ├── useApi.ts
│   │   └── useDebounce.ts
│   │
│   ├── services/            # Servicios/API clients
│   │   ├── api.ts
│   │   └── auth.service.ts
│   │
│   ├── store/               # Estado global
│   │   ├── index.ts
│   │   └── slices/
│   │
│   ├── utils/               # Utilidades
│   ├── types/               # Tipos TypeScript
│   ├── constants/           # Constantes
│   └── styles/              # Estilos globales
│
├── public/                   # Assets estáticos
├── tests/                    # Tests E2E
└── .storybook/              # Configuración Storybook
```

### Patrones de Componentes React 19 + TypeScript + Tailwind CSS

```typescript
// ═══════════════════════════════════════════════════════════════
// PATRÓN: Componente con Props Tipadas y Tailwind CSS
// ═══════════════════════════════════════════════════════════════

import { type ReactNode, type ButtonHTMLAttributes } from 'react';
import { cva, type VariantProps } from 'class-variance-authority';
import { cn } from '@/utils/cn';

// Definición de variantes con CVA (Class Variance Authority)
const buttonVariants = cva(
  // Base styles
  'inline-flex items-center justify-center rounded-lg font-medium transition-all duration-200 focus-visible:outline-none focus-visible:ring-2 focus-visible:ring-offset-2 disabled:pointer-events-none disabled:opacity-50',
  {
    variants: {
      variant: {
        primary: 'bg-blue-600 text-white hover:bg-blue-700 focus-visible:ring-blue-500',
        secondary: 'bg-gray-100 text-gray-900 hover:bg-gray-200 focus-visible:ring-gray-500',
        danger: 'bg-red-600 text-white hover:bg-red-700 focus-visible:ring-red-500',
        ghost: 'hover:bg-gray-100 hover:text-gray-900',
        outline: 'border border-gray-300 bg-transparent hover:bg-gray-100',
      },
      size: {
        sm: 'h-8 px-3 text-sm gap-1.5',
        md: 'h-10 px-4 text-sm gap-2',
        lg: 'h-12 px-6 text-base gap-2.5',
      },
    },
    defaultVariants: {
      variant: 'primary',
      size: 'md',
    },
  }
);

interface ButtonProps
  extends ButtonHTMLAttributes<HTMLButtonElement>,
    VariantProps<typeof buttonVariants> {
  isLoading?: boolean;
  leftIcon?: ReactNode;
  rightIcon?: ReactNode;
}

export function Button({
  className,
  variant,
  size,
  isLoading = false,
  leftIcon,
  rightIcon,
  children,
  disabled,
  ...props
}: ButtonProps) {
  return (
    <button
      className={cn(buttonVariants({ variant, size }), className)}
      disabled={disabled || isLoading}
      {...props}
    >
      {isLoading ? (
        <svg
          className="h-4 w-4 animate-spin"
          xmlns="http://www.w3.org/2000/svg"
          fill="none"
          viewBox="0 0 24 24"
        >
          <circle
            className="opacity-25"
            cx="12"
            cy="12"
            r="10"
            stroke="currentColor"
            strokeWidth="4"
          />
          <path
            className="opacity-75"
            fill="currentColor"
            d="M4 12a8 8 0 018-8V0C5.373 0 0 5.373 0 12h4z"
          />
        </svg>
      ) : (
        leftIcon
      )}
      {children}
      {!isLoading && rightIcon}
    </button>
  );
}

// ═══════════════════════════════════════════════════════════════
// PATRÓN: React 19 - use() Hook para Data Fetching
// ═══════════════════════════════════════════════════════════════

import { use, Suspense } from 'react';

// Función que retorna una Promise (para usar con use())
async function fetchUser(userId: string): Promise<User> {
  const response = await fetch(`/api/users/${userId}`);
  if (!response.ok) throw new Error('Failed to fetch user');
  return response.json();
}

// Cache de promesas para evitar waterfalls
const userPromiseCache = new Map<string, Promise<User>>();

function getUserPromise(userId: string): Promise<User> {
  if (!userPromiseCache.has(userId)) {
    userPromiseCache.set(userId, fetchUser(userId));
  }
  return userPromiseCache.get(userId)!;
}

// Componente que usa use() de React 19
function UserProfile({ userId }: { userId: string }) {
  // use() suspende el componente hasta que la promesa se resuelve
  const user = use(getUserPromise(userId));

  return (
    <div className="rounded-xl bg-white p-6 shadow-lg dark:bg-gray-800">
      <div className="flex items-center gap-4">
        <img
          src={user.avatar}
          alt={user.name}
          className="h-16 w-16 rounded-full object-cover ring-2 ring-blue-500"
        />
        <div>
          <h2 className="text-xl font-semibold text-gray-900 dark:text-white">
            {user.name}
          </h2>
          <p className="text-sm text-gray-500 dark:text-gray-400">{user.email}</p>
        </div>
      </div>
    </div>
  );
}

// Uso con Suspense
function UserPage({ userId }: { userId: string }) {
  return (
    <Suspense
      fallback={
        <div className="animate-pulse rounded-xl bg-gray-200 p-6 dark:bg-gray-700">
          <div className="flex items-center gap-4">
            <div className="h-16 w-16 rounded-full bg-gray-300 dark:bg-gray-600" />
            <div className="space-y-2">
              <div className="h-4 w-32 rounded bg-gray-300 dark:bg-gray-600" />
              <div className="h-3 w-48 rounded bg-gray-300 dark:bg-gray-600" />
            </div>
          </div>
        </div>
      }
    >
      <UserProfile userId={userId} />
    </Suspense>
  );
}

// ═══════════════════════════════════════════════════════════════
// PATRÓN: Custom Hook con TanStack Query
// ═══════════════════════════════════════════════════════════════

import { useQuery, useMutation, useQueryClient } from '@tanstack/react-query';

interface User {
  id: string;
  name: string;
  email: string;
  avatar: string;
}

// Hook para obtener usuario
export function useUser(userId: string) {
  return useQuery({
    queryKey: ['user', userId],
    queryFn: () => fetchUser(userId),
    staleTime: 5 * 60 * 1000, // 5 minutos
    gcTime: 10 * 60 * 1000,   // 10 minutos (antes cacheTime)
  });
}

// Hook para actualizar usuario
export function useUpdateUser() {
  const queryClient = useQueryClient();

  return useMutation({
    mutationFn: async ({ userId, data }: { userId: string; data: Partial<User> }) => {
      const response = await fetch(`/api/users/${userId}`, {
        method: 'PATCH',
        headers: { 'Content-Type': 'application/json' },
        body: JSON.stringify(data),
      });
      if (!response.ok) throw new Error('Failed to update user');
      return response.json();
    },
    onSuccess: (updatedUser) => {
      // Invalidar y refetch
      queryClient.invalidateQueries({ queryKey: ['user', updatedUser.id] });
    },
  });
}

// ═══════════════════════════════════════════════════════════════
// PATRÓN: Formulario con React 19 Actions + Tailwind
// ═══════════════════════════════════════════════════════════════

import { useActionState } from 'react';

interface FormState {
  message: string;
  errors?: Record<string, string[]>;
}

async function createUserAction(
  prevState: FormState,
  formData: FormData
): Promise<FormState> {
  const name = formData.get('name') as string;
  const email = formData.get('email') as string;

  // Validación
  const errors: Record<string, string[]> = {};
  if (!name || name.length < 2) {
    errors.name = ['El nombre debe tener al menos 2 caracteres'];
  }
  if (!email || !email.includes('@')) {
    errors.email = ['Email inválido'];
  }

  if (Object.keys(errors).length > 0) {
    return { message: 'Error de validación', errors };
  }

  // Llamada al API
  const response = await fetch('/api/users', {
    method: 'POST',
    headers: { 'Content-Type': 'application/json' },
    body: JSON.stringify({ name, email }),
  });

  if (!response.ok) {
    return { message: 'Error al crear usuario' };
  }

  return { message: 'Usuario creado exitosamente' };
}

function CreateUserForm() {
  const [state, formAction, isPending] = useActionState(createUserAction, {
    message: '',
  });

  return (
    <form action={formAction} className="mx-auto max-w-md space-y-6">
      <div>
        <label
          htmlFor="name"
          className="block text-sm font-medium text-gray-700 dark:text-gray-300"
        >
          Nombre
        </label>
        <input
          type="text"
          id="name"
          name="name"
          className={cn(
            'mt-1 block w-full rounded-lg border px-4 py-2.5 text-gray-900',
            'focus:border-blue-500 focus:ring-2 focus:ring-blue-500/20',
            'dark:border-gray-600 dark:bg-gray-800 dark:text-white',
            state.errors?.name && 'border-red-500 focus:border-red-500 focus:ring-red-500/20'
          )}
        />
        {state.errors?.name && (
          <p className="mt-1 text-sm text-red-600">{state.errors.name[0]}</p>
        )}
      </div>

      <div>
        <label
          htmlFor="email"
          className="block text-sm font-medium text-gray-700 dark:text-gray-300"
        >
          Email
        </label>
        <input
          type="email"
          id="email"
          name="email"
          className={cn(
            'mt-1 block w-full rounded-lg border px-4 py-2.5 text-gray-900',
            'focus:border-blue-500 focus:ring-2 focus:ring-blue-500/20',
            'dark:border-gray-600 dark:bg-gray-800 dark:text-white',
            state.errors?.email && 'border-red-500 focus:border-red-500 focus:ring-red-500/20'
          )}
        />
        {state.errors?.email && (
          <p className="mt-1 text-sm text-red-600">{state.errors.email[0]}</p>
        )}
      </div>

      <Button type="submit" isLoading={isPending} className="w-full">
        {isPending ? 'Creando...' : 'Crear Usuario'}
      </Button>

      {state.message && !state.errors && (
        <p className="rounded-lg bg-green-100 p-3 text-sm text-green-800">
          {state.message}
        </p>
      )}
    </form>
  );
}

// ═══════════════════════════════════════════════════════════════
// PATRÓN: Card Component con Tailwind CSS
// ═══════════════════════════════════════════════════════════════

interface CardProps {
  children: ReactNode;
  className?: string;
}

export function Card({ children, className }: CardProps) {
  return (
    <div
      className={cn(
        'rounded-xl border border-gray-200 bg-white p-6 shadow-sm',
        'dark:border-gray-700 dark:bg-gray-800',
        'transition-shadow hover:shadow-md',
        className
      )}
    >
      {children}
    </div>
  );
}

export function CardHeader({ children, className }: CardProps) {
  return (
    <div className={cn('mb-4 border-b border-gray-100 pb-4 dark:border-gray-700', className)}>
      {children}
    </div>
  );
}

export function CardTitle({ children, className }: CardProps) {
  return (
    <h3 className={cn('text-lg font-semibold text-gray-900 dark:text-white', className)}>
      {children}
    </h3>
  );
}

export function CardContent({ children, className }: CardProps) {
  return <div className={cn('text-gray-600 dark:text-gray-300', className)}>{children}</div>;
}

// ═══════════════════════════════════════════════════════════════
// UTILIDAD: cn() para combinar clases Tailwind
// ═══════════════════════════════════════════════════════════════

// src/utils/cn.ts
import { clsx, type ClassValue } from 'clsx';
import { twMerge } from 'tailwind-merge';

export function cn(...inputs: ClassValue[]) {
  return twMerge(clsx(inputs));
}
```

### Skill de Gobernanza: frontend-development

```yaml
---
name: frontend-development
description: >
  Políticas para desarrollo frontend con React 19, TypeScript y Tailwind CSS.
  Trigger: Creación de componentes, hooks, features.
metadata:
  scope: [root, frontend]
  auto_invoke:
    - "Creando componentes React"
    - "Implementando hooks personalizados"
    - "Configurando state management"
    - "Diseñando con Tailwind CSS"
  risk_level: bajo-medio
---

## Stack Requerido

- React 19 + TypeScript 5.x
- Tailwind CSS 4 con class-variance-authority (CVA)
- TanStack Query para data fetching
- Vitest + Testing Library para tests

## Patrones Críticos

### SIEMPRE
- Tipar todas las props con TypeScript (interfaces, no types para props)
- Usar `use()` de React 19 para data fetching en componentes
- Usar `useActionState()` para formularios con Server Actions
- Aplicar estilos con Tailwind CSS + CVA para variantes
- Usar `cn()` (clsx + tailwind-merge) para combinar clases
- Incluir soporte dark mode con clases `dark:`
- Seguir convención: PascalCase componentes, camelCase hooks

### NUNCA
- Usar `any` en TypeScript (usar `unknown` si necesario)
- Usar CSS-in-JS (preferir Tailwind)
- Mutar estado directamente
- Crear componentes con más de 150 líneas
- Ignorar accesibilidad (aria-*, roles, focus states)
- Usar useEffect para data fetching (usar TanStack Query o use())
- Hardcodear colores (usar design tokens de Tailwind)
```

---

## ★ Desarrollo Backend con IA ★

> **"El backend es el corazón del sistema. Aquí los errores son más costosos y la IA debe operar con mayor supervisión."**

### Stack Backend Moderno (2026)

| Capa | Tecnologías | Rol de IA |
|------|-------------|-----------|
| **Runtime** | Node.js, Deno, Bun, Python, Go | Generación de código |
| **Framework** | Express, Fastify, NestJS, FastAPI | Scaffolding, endpoints |
| **ORM** | Prisma, TypeORM, Drizzle, SQLAlchemy | Queries, migraciones |
| **Cache** | Redis, Memcached | Configuración, patrones |
| **Queue** | BullMQ, RabbitMQ, Kafka | Producers, consumers |
| **Auth** | JWT, OAuth2, Passport | Implementación segura |

### Estructura de Proyecto Backend (Clean Architecture - Python)

```
backend/
├── src/
│   ├── domain/                    # Capa de dominio
│   │   ├── entities/
│   │   │   ├── __init__.py
│   │   │   └── user.py
│   │   ├── value_objects/
│   │   │   ├── __init__.py
│   │   │   └── email.py
│   │   ├── repositories/          # Interfaces (ABC)
│   │   │   ├── __init__.py
│   │   │   └── user_repository.py
│   │   ├── services/              # Domain services
│   │   └── events/                # Domain events
│   │
│   ├── application/               # Capa de aplicación
│   │   ├── use_cases/
│   │   │   ├── user/
│   │   │   │   ├── __init__.py
│   │   │   │   ├── create_user.py
│   │   │   │   ├── get_user.py
│   │   │   │   └── update_user.py
│   │   │   └── order/
│   │   ├── dto/
│   │   │   ├── __init__.py
│   │   │   └── user_dto.py
│   │   ├── mappers/
│   │   └── interfaces/            # Ports
│   │
│   ├── infrastructure/            # Capa de infraestructura
│   │   ├── persistence/
│   │   │   ├── sqlalchemy/
│   │   │   │   ├── __init__.py
│   │   │   │   ├── models.py
│   │   │   │   └── database.py
│   │   │   ├── repositories/      # Implementaciones
│   │   │   │   └── sqlalchemy_user_repository.py
│   │   │   └── redis/
│   │   │       ├── __init__.py
│   │   │       ├── redis_client.py
│   │   │       └── cache_service.py
│   │   ├── messaging/
│   │   │   ├── queues/
│   │   │   └── events/
│   │   ├── external/
│   │   │   ├── payment/
│   │   │   └── notification/
│   │   └── config/
│   │       ├── __init__.py
│   │       ├── settings.py
│   │       └── container.py       # Dependency Injection
│   │
│   ├── presentation/              # Capa de presentación
│   │   ├── api/
│   │   │   ├── __init__.py
│   │   │   ├── routers/
│   │   │   │   ├── __init__.py
│   │   │   │   └── user_router.py
│   │   │   ├── middlewares/
│   │   │   ├── schemas/           # Pydantic schemas
│   │   │   │   └── user_schema.py
│   │   │   └── dependencies.py
│   │   └── graphql/               # Si aplica
│   │       ├── resolvers/
│   │       └── schemas/
│   │
│   └── shared/                    # Código compartido
│       ├── errors/
│       │   └── exceptions.py
│       ├── utils/
│       └── types/
│
├── tests/
│   ├── unit/
│   ├── integration/
│   └── e2e/
│
├── alembic/                       # Migraciones
│   ├── versions/
│   └── env.py
│
├── main.py                        # Entry point FastAPI
├── requirements.txt
└── pyproject.toml
```

### Patrones de API REST (Python + FastAPI)

```python
# ═══════════════════════════════════════════════════════════════
# PATRÓN: Router con validación y manejo de errores (FastAPI)
# ═══════════════════════════════════════════════════════════════

# src/presentation/api/routers/user_router.py

from fastapi import APIRouter, Depends, HTTPException, status
from pydantic import BaseModel, EmailStr

from src.application.use_cases.user.create_user import CreateUserUseCase
from src.application.dto.user_dto import CreateUserDTO
from src.presentation.api.dependencies import get_create_user_use_case
from src.shared.errors.exceptions import ConflictError, ValidationError

router = APIRouter(prefix="/users", tags=["users"])


class CreateUserRequest(BaseModel):
    """Schema de entrada validado por Pydantic"""
    email: EmailStr
    name: str
    password: str


class UserResponse(BaseModel):
    """Schema de respuesta"""
    id: str
    email: str
    name: str
    created_at: str

    class Config:
        from_attributes = True


@router.post("/", response_model=UserResponse, status_code=status.HTTP_201_CREATED)
async def create_user(
    request: CreateUserRequest,
    use_case: CreateUserUseCase = Depends(get_create_user_use_case)
):
    """Crear un nuevo usuario"""
    try:
        dto = CreateUserDTO(
            email=request.email,
            name=request.name,
            password=request.password
        )
        user = await use_case.execute(dto)
        return UserResponse.model_validate(user)
    except ConflictError as e:
        raise HTTPException(status_code=status.HTTP_409_CONFLICT, detail=str(e))
    except ValidationError as e:
        raise HTTPException(status_code=status.HTTP_400_BAD_REQUEST, detail=str(e))


# ═══════════════════════════════════════════════════════════════
# PATRÓN: Use Case con inversión de dependencias
# ═══════════════════════════════════════════════════════════════

# src/application/use_cases/user/create_user.py

from abc import ABC, abstractmethod
from dataclasses import dataclass
from typing import Protocol

from src.domain.entities.user import User
from src.domain.value_objects.email import Email
from src.domain.repositories.user_repository import UserRepository
from src.application.interfaces.email_service import EmailService
from src.application.dto.user_dto import CreateUserDTO
from src.shared.errors.exceptions import ConflictError


class CreateUserUseCase:
    """Caso de uso: Crear usuario"""

    def __init__(
        self,
        user_repository: UserRepository,
        email_service: EmailService
    ):
        self._user_repository = user_repository
        self._email_service = email_service

    async def execute(self, dto: CreateUserDTO) -> User:
        # Validación de dominio
        email = Email.create(dto.email)

        # Verificar unicidad
        existing_user = await self._user_repository.find_by_email(email)
        if existing_user:
            raise ConflictError("User with this email already exists")

        # Crear entidad
        user = User.create(
            email=email,
            name=dto.name,
            password=dto.password
        )

        # Persistir
        await self._user_repository.save(user)

        # Efectos secundarios
        await self._email_service.send_welcome_email(user)

        return user


# ═══════════════════════════════════════════════════════════════
# PATRÓN: Repository con SQLAlchemy
# ═══════════════════════════════════════════════════════════════

# src/infrastructure/persistence/repositories/sqlalchemy_user_repository.py

from typing import Optional
from sqlalchemy.ext.asyncio import AsyncSession
from sqlalchemy import select

from src.domain.entities.user import User
from src.domain.value_objects.email import Email
from src.domain.repositories.user_repository import UserRepository
from src.infrastructure.persistence.sqlalchemy.models import UserModel
from src.application.mappers.user_mapper import UserMapper


class SQLAlchemyUserRepository(UserRepository):
    """Implementación de repositorio con SQLAlchemy"""

    def __init__(self, session: AsyncSession):
        self._session = session

    async def find_by_id(self, user_id: str) -> Optional[User]:
        stmt = select(UserModel).where(UserModel.id == user_id)
        result = await self._session.execute(stmt)
        user_model = result.scalar_one_or_none()

        return UserMapper.to_domain(user_model) if user_model else None

    async def find_by_email(self, email: Email) -> Optional[User]:
        stmt = select(UserModel).where(UserModel.email == email.value)
        result = await self._session.execute(stmt)
        user_model = result.scalar_one_or_none()

        return UserMapper.to_domain(user_model) if user_model else None

    async def save(self, user: User) -> None:
        user_model = UserMapper.to_persistence(user)

        # Upsert pattern
        existing = await self._session.get(UserModel, user.id)
        if existing:
            for key, value in user_model.__dict__.items():
                if not key.startswith('_'):
                    setattr(existing, key, value)
        else:
            self._session.add(user_model)

        await self._session.commit()


# ═══════════════════════════════════════════════════════════════
# PATRÓN: Dependency Injection con FastAPI
# ═══════════════════════════════════════════════════════════════

# src/presentation/api/dependencies.py

from functools import lru_cache
from fastapi import Depends
from sqlalchemy.ext.asyncio import AsyncSession

from src.infrastructure.persistence.sqlalchemy.database import get_session
from src.infrastructure.persistence.repositories.sqlalchemy_user_repository import SQLAlchemyUserRepository
from src.infrastructure.external.notification.email_service import SMTPEmailService
from src.application.use_cases.user.create_user import CreateUserUseCase


async def get_user_repository(session: AsyncSession = Depends(get_session)):
    return SQLAlchemyUserRepository(session)


@lru_cache
def get_email_service():
    return SMTPEmailService()


async def get_create_user_use_case(
    user_repository: SQLAlchemyUserRepository = Depends(get_user_repository),
    email_service: SMTPEmailService = Depends(get_email_service)
) -> CreateUserUseCase:
    return CreateUserUseCase(user_repository, email_service)
```

### Implementación de Redis para Caching (Python)

```python
# ═══════════════════════════════════════════════════════════════
# PATRÓN: Cache Service con Redis
# ═══════════════════════════════════════════════════════════════

# src/infrastructure/persistence/redis/cache_service.py

import json
from typing import TypeVar, Optional, Callable, Awaitable, Any
from dataclasses import dataclass
from redis.asyncio import Redis

T = TypeVar('T')


@dataclass
class CacheOptions:
    """Opciones de configuración de cache"""
    ttl: int = 3600  # Time to live en segundos
    prefix: str = ""


class CacheService:
    """Servicio de cache con Redis"""

    def __init__(self, redis_url: str):
        self._redis = Redis.from_url(redis_url, decode_responses=True)
        self._default_ttl = 3600  # 1 hora

    async def get(self, key: str) -> Optional[Any]:
        """Obtener valor del cache"""
        data = await self._redis.get(key)
        return json.loads(data) if data else None

    async def set(
        self,
        key: str,
        value: Any,
        options: Optional[CacheOptions] = None
    ) -> None:
        """Guardar valor en cache"""
        ttl = options.ttl if options else self._default_ttl
        await self._redis.setex(key, ttl, json.dumps(value, default=str))

    async def delete(self, key: str) -> None:
        """Eliminar valor del cache"""
        await self._redis.delete(key)

    async def invalidate_pattern(self, pattern: str) -> None:
        """Invalidar todas las keys que coincidan con el patrón"""
        keys = []
        async for key in self._redis.scan_iter(match=pattern):
            keys.append(key)
        if keys:
            await self._redis.delete(*keys)

    async def get_or_set(
        self,
        key: str,
        fetcher: Callable[[], Awaitable[T]],
        options: Optional[CacheOptions] = None
    ) -> T:
        """Cache-aside pattern: obtener del cache o ejecutar fetcher"""
        cached = await self.get(key)
        if cached is not None:
            return cached

        fresh = await fetcher()
        await self.set(key, fresh, options)
        return fresh

    async def close(self) -> None:
        """Cerrar conexión"""
        await self._redis.close()


# ═══════════════════════════════════════════════════════════════
# PATRÓN: Decorador de Cache para Use Cases
# ═══════════════════════════════════════════════════════════════

# src/infrastructure/persistence/redis/decorators.py

from functools import wraps
from typing import Callable, Any


def cached(
    key_generator: Callable[..., str],
    ttl: int = 3600
):
    """
    Decorador para cachear resultados de métodos async.

    Uso:
        @cached(lambda self, id: f"user:{id}", ttl=3600)
        async def get_user(self, id: str) -> User:
            ...
    """
    def decorator(func: Callable) -> Callable:
        @wraps(func)
        async def wrapper(self, *args, **kwargs) -> Any:
            # Asume que el objeto tiene un atributo cache_service
            cache_service = getattr(self, '_cache_service', None)
            if cache_service is None:
                return await func(self, *args, **kwargs)

            key = key_generator(self, *args, **kwargs)

            return await cache_service.get_or_set(
                key,
                lambda: func(self, *args, **kwargs),
                CacheOptions(ttl=ttl)
            )
        return wrapper
    return decorator


# ═══════════════════════════════════════════════════════════════
# Uso del decorador en Use Case
# ═══════════════════════════════════════════════════════════════

# src/application/use_cases/user/get_user.py

from typing import Optional
from src.domain.entities.user import User
from src.domain.repositories.user_repository import UserRepository
from src.infrastructure.persistence.redis.cache_service import CacheService
from src.infrastructure.persistence.redis.decorators import cached


class GetUserUseCase:
    """Caso de uso: Obtener usuario por ID con cache"""

    def __init__(
        self,
        user_repository: UserRepository,
        cache_service: CacheService
    ):
        self._user_repository = user_repository
        self._cache_service = cache_service

    @cached(lambda self, user_id: f"user:{user_id}", ttl=3600)
    async def execute(self, user_id: str) -> Optional[User]:
        """Obtener usuario por ID (cacheado por 1 hora)"""
        return await self._user_repository.find_by_id(user_id)


# ═══════════════════════════════════════════════════════════════
# PATRÓN: Configuración de Redis con FastAPI
# ═══════════════════════════════════════════════════════════════

# src/infrastructure/config/settings.py

from pydantic_settings import BaseSettings


class Settings(BaseSettings):
    """Configuración de la aplicación"""

    # Database
    database_url: str = "postgresql+asyncpg://user:pass@localhost/db"

    # Redis
    redis_url: str = "redis://localhost:6379/0"

    # App
    debug: bool = False
    secret_key: str = "your-secret-key"

    class Config:
        env_file = ".env"


settings = Settings()
```

### Skill de Gobernanza: backend-development

```yaml
---
name: backend-development
description: >
  Políticas para desarrollo backend asistido por IA.
  Trigger: APIs, servicios, persistencia, cache.
metadata:
  scope: [root, backend]
  auto_invoke:
    - "Creando endpoints de API"
    - "Implementando use cases"
    - "Configurando base de datos"
    - "Implementando caching"
  risk_level: medio
---

## Patrones Críticos

### SIEMPRE
- Seguir Clean Architecture (separación de capas)
- Validar inputs en el borde del sistema
- Usar DTOs para transferencia de datos
- Implementar manejo de errores consistente
- Loggear operaciones críticas

### NUNCA
- Exponer entidades de dominio directamente en APIs
- Hardcodear credenciales o secrets
- Ignorar validación de inputs
- Hacer queries N+1
- Mutar estado en queries (CQS)
```

---

## ★ Aplicaciones Móviles con IA ★

> **"El desarrollo móvil combina las complejidades del frontend con restricciones únicas de plataforma. La IA debe conocer ambos mundos."**

### Stack Mobile Moderno (2026)

| Enfoque | Tecnología | Casos de Uso | Rol de IA |
|---------|------------|--------------|-----------|
| **Cross-platform** | React Native, Flutter | Apps de negocio, MVPs | Alto |
| **Nativo iOS** | Swift, SwiftUI | Apps premium iOS | Medio |
| **Nativo Android** | Kotlin, Jetpack Compose | Apps premium Android | Medio |
| **PWA** | Web standards | Apps ligeras, instalables | Alto |

### Estructura de Proyecto React Native

```
mobile/
├── src/
│   ├── components/           # Componentes reutilizables
│   │   ├── ui/              # Componentes base
│   │   └── composed/        # Componentes compuestos
│   │
│   ├── screens/             # Pantallas de la app
│   │   ├── auth/
│   │   │   ├── LoginScreen.tsx
│   │   │   └── RegisterScreen.tsx
│   │   ├── home/
│   │   └── profile/
│   │
│   ├── navigation/          # Configuración de navegación
│   │   ├── RootNavigator.tsx
│   │   ├── AuthNavigator.tsx
│   │   └── MainNavigator.tsx
│   │
│   ├── services/            # Servicios y API
│   │   ├── api/
│   │   └── storage/
│   │
│   ├── hooks/               # Custom hooks
│   ├── store/               # Estado global
│   ├── utils/               # Utilidades
│   ├── constants/           # Constantes
│   └── types/               # Tipos TypeScript
│
├── ios/                     # Código nativo iOS
├── android/                 # Código nativo Android
├── __tests__/               # Tests
└── e2e/                     # Tests E2E (Detox)
```

### Consideraciones Específicas de Mobile

```
┌─────────────────────────────────────────────────────────────────┐
│            CONSIDERACIONES MOBILE IA-NATIVE                    │
├─────────────────────────────────────────────────────────────────┤
│                                                                 │
│  PERFORMANCE                                                    │
│  ├── Lazy loading de pantallas                                 │
│  ├── Optimización de listas (FlatList, virtualization)         │
│  ├── Memoization de componentes pesados                        │
│  └── Reducir re-renders innecesarios                           │
│                                                                 │
│  OFFLINE-FIRST                                                  │
│  ├── Persistencia local (AsyncStorage, SQLite)                 │
│  ├── Sincronización con servidor                               │
│  ├── Manejo de conflictos                                      │
│  └── UI para estados offline                                   │
│                                                                 │
│  SEGURIDAD                                                      │
│  ├── Secure storage para tokens                                │
│  ├── Certificate pinning                                       │
│  ├── Biometric authentication                                  │
│  └── Ofuscación de código                                      │
│                                                                 │
│  UX NATIVA                                                      │
│  ├── Gestos nativos                                            │
│  ├── Haptic feedback                                           │
│  ├── Animaciones fluidas (60fps)                               │
│  └── Respeto de guidelines (iOS HIG, Material)                 │
│                                                                 │
│  TESTING                                                        │
│  ├── Unit tests (Jest)                                         │
│  ├── Component tests (Testing Library)                         │
│  ├── E2E tests (Detox, Maestro)                                │
│  └── Testing en dispositivos reales                            │
│                                                                 │
└─────────────────────────────────────────────────────────────────┘
```

---

## ★ Optimización de APIs y Performance ★

> **"Una API mal diseñada es una deuda técnica que crece con cada cliente que la consume. La IA puede ayudar a diseñar APIs robustas desde el inicio."**

### Principios de Diseño de APIs

```
┌─────────────────────────────────────────────────────────────────┐
│              PRINCIPIOS DE API DESIGN                           │
├─────────────────────────────────────────────────────────────────┤
│                                                                 │
│  1. CONSISTENCIA                                                │
│     • Naming conventions uniformes                             │
│     • Estructura de respuesta estándar                         │
│     • Códigos de error consistentes                            │
│                                                                 │
│  2. VERSIONADO                                                  │
│     • URL versioning (/api/v1/) o Header versioning            │
│     • Backward compatibility                                   │
│     • Deprecation policy                                       │
│                                                                 │
│  3. PAGINACIÓN                                                  │
│     • Cursor-based para datasets grandes                       │
│     • Page-based para datasets pequeños                        │
│     • Límites máximos de página                                │
│                                                                 │
│  4. RATE LIMITING                                               │
│     • Por usuario/API key                                      │
│     • Headers informativos (X-RateLimit-*)                     │
│     • Graceful degradation                                     │
│                                                                 │
│  5. CACHING                                                     │
│     • ETags para validación                                    │
│     • Cache-Control headers                                    │
│     • CDN para assets estáticos                                │
│                                                                 │
└─────────────────────────────────────────────────────────────────┘
```

### Estructura de Respuesta API Estándar

```typescript
// ═══════════════════════════════════════════════════════════════
// Respuesta exitosa
// ═══════════════════════════════════════════════════════════════
interface ApiResponse<T> {
  success: true;
  data: T;
  meta?: {
    page?: number;
    limit?: number;
    total?: number;
    nextCursor?: string;
  };
}

// ═══════════════════════════════════════════════════════════════
// Respuesta de error
// ═══════════════════════════════════════════════════════════════
interface ApiError {
  success: false;
  error: {
    code: string;           // Código máquina-readable (USER_NOT_FOUND)
    message: string;        // Mensaje humano-readable
    details?: unknown;      // Detalles adicionales
    requestId: string;      // ID para debugging
  };
}

// ═══════════════════════════════════════════════════════════════
// Códigos de error estándar
// ═══════════════════════════════════════════════════════════════
const ErrorCodes = {
  // 400 Bad Request
  VALIDATION_ERROR: 'VALIDATION_ERROR',
  INVALID_INPUT: 'INVALID_INPUT',

  // 401 Unauthorized
  UNAUTHORIZED: 'UNAUTHORIZED',
  TOKEN_EXPIRED: 'TOKEN_EXPIRED',

  // 403 Forbidden
  FORBIDDEN: 'FORBIDDEN',
  INSUFFICIENT_PERMISSIONS: 'INSUFFICIENT_PERMISSIONS',

  // 404 Not Found
  RESOURCE_NOT_FOUND: 'RESOURCE_NOT_FOUND',

  // 409 Conflict
  CONFLICT: 'CONFLICT',
  DUPLICATE_RESOURCE: 'DUPLICATE_RESOURCE',

  // 429 Too Many Requests
  RATE_LIMITED: 'RATE_LIMITED',

  // 500 Internal Server Error
  INTERNAL_ERROR: 'INTERNAL_ERROR',
} as const;
```

### Implementación de Rate Limiting

```typescript
import { Redis } from 'ioredis';

interface RateLimitConfig {
  windowMs: number;      // Ventana de tiempo en ms
  maxRequests: number;   // Máximo de requests por ventana
}

export class RateLimiter {
  constructor(
    private readonly redis: Redis,
    private readonly config: RateLimitConfig
  ) {}

  async checkLimit(identifier: string): Promise<{
    allowed: boolean;
    remaining: number;
    resetAt: number;
  }> {
    const key = `ratelimit:${identifier}`;
    const now = Date.now();
    const windowStart = now - this.config.windowMs;

    // Usar sorted set para sliding window
    const pipeline = this.redis.pipeline();

    // Eliminar requests fuera de la ventana
    pipeline.zremrangebyscore(key, 0, windowStart);

    // Contar requests en la ventana
    pipeline.zcard(key);

    // Agregar request actual
    pipeline.zadd(key, now, `${now}-${Math.random()}`);

    // Establecer expiración
    pipeline.pexpire(key, this.config.windowMs);

    const results = await pipeline.exec();
    const currentCount = results![1][1] as number;

    const allowed = currentCount < this.config.maxRequests;
    const remaining = Math.max(0, this.config.maxRequests - currentCount - 1);
    const resetAt = now + this.config.windowMs;

    return { allowed, remaining, resetAt };
  }
}

// Middleware Express
export function rateLimitMiddleware(limiter: RateLimiter) {
  return async (req: Request, res: Response, next: NextFunction) => {
    const identifier = req.ip || req.headers['x-api-key'] as string;
    const { allowed, remaining, resetAt } = await limiter.checkLimit(identifier);

    res.setHeader('X-RateLimit-Limit', limiter.config.maxRequests);
    res.setHeader('X-RateLimit-Remaining', remaining);
    res.setHeader('X-RateLimit-Reset', resetAt);

    if (!allowed) {
      return res.status(429).json({
        success: false,
        error: {
          code: 'RATE_LIMITED',
          message: 'Too many requests, please try again later',
          resetAt,
        },
      });
    }

    next();
  };
}
```

### Documentación OpenAPI

```yaml
openapi: 3.0.3
info:
  title: API de Ejemplo
  version: 1.0.0
  description: |
    API para gestión de usuarios y órdenes.

    ## Autenticación
    Esta API usa Bearer tokens (JWT) para autenticación.

    ## Rate Limiting
    - 100 requests por minuto para endpoints públicos
    - 1000 requests por minuto para endpoints autenticados

    ## Versionado
    La versión actual es v1. Versiones anteriores serán deprecadas
    con 6 meses de aviso.

servers:
  - url: https://api.example.com/v1
    description: Producción
  - url: https://staging-api.example.com/v1
    description: Staging

paths:
  /users:
    get:
      summary: Listar usuarios
      tags: [Users]
      parameters:
        - name: page
          in: query
          schema:
            type: integer
            default: 1
        - name: limit
          in: query
          schema:
            type: integer
            default: 20
            maximum: 100
      responses:
        '200':
          description: Lista de usuarios
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/UserListResponse'
        '401':
          $ref: '#/components/responses/Unauthorized'
        '429':
          $ref: '#/components/responses/RateLimited'

components:
  schemas:
    User:
      type: object
      properties:
        id:
          type: string
          format: uuid
        email:
          type: string
          format: email
        name:
          type: string
        createdAt:
          type: string
          format: date-time
      required: [id, email, name, createdAt]

  responses:
    Unauthorized:
      description: No autenticado
      content:
        application/json:
          schema:
            $ref: '#/components/schemas/Error'
    RateLimited:
      description: Rate limit excedido
      headers:
        X-RateLimit-Reset:
          schema:
            type: integer
      content:
        application/json:
          schema:
            $ref: '#/components/schemas/Error'

  securitySchemes:
    bearerAuth:
      type: http
      scheme: bearer
      bearerFormat: JWT
```

---

## ★ DevOps y Despliegue IA-Native ★

> **"El pipeline de CI/CD es la última línea de defensa antes de producción. La IA puede automatizar, pero las decisiones de deploy son irreversibles."**

### Pipeline CI/CD IA-Native

```yaml
# .github/workflows/main.yml
name: CI/CD Pipeline

on:
  push:
    branches: [main, develop]
  pull_request:
    branches: [main]

env:
  NODE_VERSION: '20'
  REGISTRY: ghcr.io
  IMAGE_NAME: ${{ github.repository }}

jobs:
  # ═══════════════════════════════════════════════════════════════
  # VALIDACIÓN DE GOBERNANZA
  # ═══════════════════════════════════════════════════════════════
  governance:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4

      - name: Check Policy Ticket Reference
        run: |
          if [[ "${{ github.event_name }}" == "pull_request" ]]; then
            PR_BODY="${{ github.event.pull_request.body }}"
            if ! echo "$PR_BODY" | grep -qE "PT-[0-9]{4}-[0-9]+"; then
              echo "::error::PR must reference a Policy Ticket (PT-XXXX-XXX)"
              exit 1
            fi
          fi

  # ═══════════════════════════════════════════════════════════════
  # ANÁLISIS DE CÓDIGO
  # ═══════════════════════════════════════════════════════════════
  analyze:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4

      - name: Setup Node
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: 'npm'

      - name: Install Dependencies
        run: npm ci

      - name: Lint
        run: npm run lint

      - name: Type Check
        run: npm run type-check

      - name: Security Audit
        run: npm audit --audit-level=high

  # ═══════════════════════════════════════════════════════════════
  # TESTS
  # ═══════════════════════════════════════════════════════════════
  test:
    runs-on: ubuntu-latest
    needs: [governance, analyze]
    services:
      postgres:
        image: postgres:15
        env:
          POSTGRES_PASSWORD: test
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
      redis:
        image: redis:7
        options: >-
          --health-cmd "redis-cli ping"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5

    steps:
      - uses: actions/checkout@v4

      - name: Setup Node
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: 'npm'

      - name: Install Dependencies
        run: npm ci

      - name: Run Unit Tests
        run: npm run test:unit -- --coverage

      - name: Run Integration Tests
        run: npm run test:integration
        env:
          DATABASE_URL: postgresql://postgres:test@localhost:5432/test
          REDIS_URL: redis://localhost:6379

      - name: Upload Coverage
        uses: codecov/codecov-action@v3
        with:
          files: ./coverage/lcov.info

  # ═══════════════════════════════════════════════════════════════
  # BUILD
  # ═══════════════════════════════════════════════════════════════
  build:
    runs-on: ubuntu-latest
    needs: [test]
    steps:
      - uses: actions/checkout@v4

      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3

      - name: Login to Container Registry
        uses: docker/login-action@v3
        with:
          registry: ${{ env.REGISTRY }}
          username: ${{ github.actor }}
          password: ${{ secrets.GITHUB_TOKEN }}

      - name: Extract metadata
        id: meta
        uses: docker/metadata-action@v5
        with:
          images: ${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}

      - name: Build and push
        uses: docker/build-push-action@v5
        with:
          context: .
          push: true
          tags: ${{ steps.meta.outputs.tags }}
          labels: ${{ steps.meta.outputs.labels }}
          cache-from: type=gha
          cache-to: type=gha,mode=max

  # ═══════════════════════════════════════════════════════════════
  # DEPLOY A STAGING (Automático)
  # ═══════════════════════════════════════════════════════════════
  deploy-staging:
    runs-on: ubuntu-latest
    needs: [build]
    if: github.ref == 'refs/heads/develop'
    environment:
      name: staging
      url: https://staging.example.com
    steps:
      - name: Deploy to Staging
        run: |
          # Deploy automático a staging
          echo "Deploying to staging..."

  # ═══════════════════════════════════════════════════════════════
  # DEPLOY A PRODUCCIÓN (Requiere aprobación)
  # ═══════════════════════════════════════════════════════════════
  deploy-production:
    runs-on: ubuntu-latest
    needs: [build]
    if: github.ref == 'refs/heads/main'
    environment:
      name: production
      url: https://example.com
    steps:
      - name: Deploy to Production
        run: |
          # Deploy a producción (requiere aprobación manual en GitHub)
          echo "Deploying to production..."
```

### Dockerfile Multi-stage

```dockerfile
# ═══════════════════════════════════════════════════════════════
# Stage 1: Dependencies
# ═══════════════════════════════════════════════════════════════
FROM node:20-alpine AS deps
WORKDIR /app

COPY package*.json ./
RUN npm ci --only=production

# ═══════════════════════════════════════════════════════════════
# Stage 2: Build
# ═══════════════════════════════════════════════════════════════
FROM node:20-alpine AS builder
WORKDIR /app

COPY package*.json ./
RUN npm ci

COPY . .
RUN npm run build

# ═══════════════════════════════════════════════════════════════
# Stage 3: Production
# ═══════════════════════════════════════════════════════════════
FROM node:20-alpine AS runner
WORKDIR /app

ENV NODE_ENV=production

RUN addgroup --system --gid 1001 nodejs
RUN adduser --system --uid 1001 appuser

COPY --from=deps /app/node_modules ./node_modules
COPY --from=builder /app/dist ./dist

USER appuser

EXPOSE 3000
CMD ["node", "dist/main.js"]
```

### Infrastructure as Code (Terraform)

```hcl
# main.tf

terraform {
  required_providers {
    aws = {
      source  = "hashicorp/aws"
      version = "~> 5.0"
    }
  }
}

# ═══════════════════════════════════════════════════════════════
# VPC y Networking
# ═══════════════════════════════════════════════════════════════
module "vpc" {
  source = "terraform-aws-modules/vpc/aws"

  name = "${var.project_name}-vpc"
  cidr = "10.0.0.0/16"

  azs             = ["us-east-1a", "us-east-1b"]
  private_subnets = ["10.0.1.0/24", "10.0.2.0/24"]
  public_subnets  = ["10.0.101.0/24", "10.0.102.0/24"]

  enable_nat_gateway = true
  single_nat_gateway = var.environment != "production"
}

# ═══════════════════════════════════════════════════════════════
# ECS Cluster
# ═══════════════════════════════════════════════════════════════
resource "aws_ecs_cluster" "main" {
  name = "${var.project_name}-cluster"

  setting {
    name  = "containerInsights"
    value = "enabled"
  }
}

# ═══════════════════════════════════════════════════════════════
# RDS PostgreSQL
# ═══════════════════════════════════════════════════════════════
module "rds" {
  source = "terraform-aws-modules/rds/aws"

  identifier = "${var.project_name}-db"

  engine               = "postgres"
  engine_version       = "15"
  family               = "postgres15"
  major_engine_version = "15"
  instance_class       = var.environment == "production" ? "db.r6g.large" : "db.t3.micro"

  allocated_storage = 20

  db_name  = var.db_name
  username = var.db_username
  port     = 5432

  vpc_security_group_ids = [module.security_group_rds.security_group_id]
  subnet_ids             = module.vpc.private_subnets

  backup_retention_period = var.environment == "production" ? 7 : 1
  deletion_protection     = var.environment == "production"
}

# ═══════════════════════════════════════════════════════════════
# ElastiCache Redis
# ═══════════════════════════════════════════════════════════════
resource "aws_elasticache_cluster" "redis" {
  cluster_id           = "${var.project_name}-redis"
  engine               = "redis"
  node_type            = var.environment == "production" ? "cache.r6g.large" : "cache.t3.micro"
  num_cache_nodes      = 1
  parameter_group_name = "default.redis7"
  port                 = 6379
  security_group_ids   = [module.security_group_redis.security_group_id]
  subnet_group_name    = aws_elasticache_subnet_group.main.name
}
```

### Skill de Gobernanza: devops-deployment

```yaml
---
name: devops-deployment
description: >
  Políticas para operaciones DevOps y despliegues.
  Trigger: Cambios en CI/CD, infraestructura, deploys.
metadata:
  scope: [root]
  auto_invoke:
    - "Modificando pipelines CI/CD"
    - "Configurando infraestructura"
    - "Desplegando a producción"
  risk_level: alto
---

## Patrones Críticos

### SIEMPRE
- Requerir Policy Ticket para cambios de infraestructura
- Mantener ambientes de staging y producción separados
- Implementar rollback automático en caso de fallo
- Encriptar secrets y credenciales
- Loggear todas las operaciones de deploy

### NUNCA
- Deploy directo a producción sin pasar por staging
- Hardcodear credenciales en código o config
- Modificar infraestructura de producción sin aprobación
- Omitir tests antes de deploy
- Desactivar monitoreo durante deploys
```

---

## ★ Buenas Prácticas y Patrones de Diseño ★

> **"Los patrones de diseño son soluciones probadas a problemas recurrentes. La IA debe conocerlos para aplicarlos correctamente."**

### Principios SOLID

```
┌─────────────────────────────────────────────────────────────────┐
│                    PRINCIPIOS SOLID                             │
├─────────────────────────────────────────────────────────────────┤
│                                                                 │
│  S - Single Responsibility Principle                           │
│      Una clase debe tener una única razón para cambiar         │
│      ✓ Clase UserService solo gestiona usuarios                │
│      ✗ Clase UserService que también envía emails              │
│                                                                 │
│  O - Open/Closed Principle                                     │
│      Abierto para extensión, cerrado para modificación         │
│      ✓ Agregar nuevo PaymentProcessor sin modificar existentes │
│      ✗ Modificar switch/case cada vez que hay nuevo tipo       │
│                                                                 │
│  L - Liskov Substitution Principle                             │
│      Subtipos deben ser sustituibles por sus tipos base        │
│      ✓ Rectangle y Square son ambos Shape                      │
│      ✗ Square que rompe invariantes de Rectangle               │
│                                                                 │
│  I - Interface Segregation Principle                           │
│      Interfaces específicas mejor que una general              │
│      ✓ IReadable, IWritable separadas                          │
│      ✗ IFileHandler con 20 métodos                             │
│                                                                 │
│  D - Dependency Inversion Principle                            │
│      Depender de abstracciones, no de implementaciones         │
│      ✓ UserService depende de IUserRepository                  │
│      ✗ UserService instancia directamente PostgresRepository   │
│                                                                 │
└─────────────────────────────────────────────────────────────────┘
```

### Patrones de Diseño Esenciales

```typescript
// ═══════════════════════════════════════════════════════════════
// PATRÓN: Factory
// Uso: Cuando la creación de objetos es compleja o variable
// ═══════════════════════════════════════════════════════════════

interface PaymentProcessor {
  process(amount: number): Promise<PaymentResult>;
}

class PaymentProcessorFactory {
  static create(type: 'stripe' | 'paypal' | 'mercadopago'): PaymentProcessor {
    switch (type) {
      case 'stripe':
        return new StripeProcessor(config.stripe);
      case 'paypal':
        return new PayPalProcessor(config.paypal);
      case 'mercadopago':
        return new MercadoPagoProcessor(config.mercadopago);
      default:
        throw new Error(`Unknown payment processor: ${type}`);
    }
  }
}

// ═══════════════════════════════════════════════════════════════
// PATRÓN: Strategy
// Uso: Cuando hay múltiples algoritmos intercambiables
// ═══════════════════════════════════════════════════════════════

interface PricingStrategy {
  calculate(basePrice: number, quantity: number): number;
}

class RegularPricing implements PricingStrategy {
  calculate(basePrice: number, quantity: number): number {
    return basePrice * quantity;
  }
}

class BulkPricing implements PricingStrategy {
  calculate(basePrice: number, quantity: number): number {
    const discount = quantity >= 10 ? 0.1 : 0;
    return basePrice * quantity * (1 - discount);
  }
}

class PremiumPricing implements PricingStrategy {
  calculate(basePrice: number, quantity: number): number {
    return basePrice * quantity * 0.85; // 15% descuento siempre
  }
}

class PriceCalculator {
  constructor(private strategy: PricingStrategy) {}

  setStrategy(strategy: PricingStrategy): void {
    this.strategy = strategy;
  }

  calculateTotal(basePrice: number, quantity: number): number {
    return this.strategy.calculate(basePrice, quantity);
  }
}

// ═══════════════════════════════════════════════════════════════
// PATRÓN: Repository
// Uso: Abstracción de acceso a datos
// ═══════════════════════════════════════════════════════════════

interface IRepository<T, ID> {
  findById(id: ID): Promise<T | null>;
  findAll(): Promise<T[]>;
  save(entity: T): Promise<void>;
  delete(id: ID): Promise<void>;
}

interface IUserRepository extends IRepository<User, string> {
  findByEmail(email: Email): Promise<User | null>;
  findActiveUsers(): Promise<User[]>;
}

// ═══════════════════════════════════════════════════════════════
// PATRÓN: Observer / Event Emitter
// Uso: Notificación de cambios a múltiples interesados
// ═══════════════════════════════════════════════════════════════

type EventHandler<T> = (data: T) => void | Promise<void>;

class EventBus {
  private handlers: Map<string, EventHandler<any>[]> = new Map();

  on<T>(event: string, handler: EventHandler<T>): void {
    const handlers = this.handlers.get(event) || [];
    handlers.push(handler);
    this.handlers.set(event, handlers);
  }

  async emit<T>(event: string, data: T): Promise<void> {
    const handlers = this.handlers.get(event) || [];
    await Promise.all(handlers.map(handler => handler(data)));
  }

  off(event: string, handler: EventHandler<any>): void {
    const handlers = this.handlers.get(event) || [];
    this.handlers.set(
      event,
      handlers.filter(h => h !== handler)
    );
  }
}

// Uso
const eventBus = new EventBus();

eventBus.on('user.created', async (user: User) => {
  await emailService.sendWelcome(user);
});

eventBus.on('user.created', async (user: User) => {
  await analyticsService.trackSignup(user);
});

// En el use case
await eventBus.emit('user.created', newUser);

// ═══════════════════════════════════════════════════════════════
// PATRÓN: Decorator
// Uso: Agregar funcionalidad sin modificar clase original
// ═══════════════════════════════════════════════════════════════

interface ILogger {
  log(message: string): void;
}

class ConsoleLogger implements ILogger {
  log(message: string): void {
    console.log(message);
  }
}

class TimestampLoggerDecorator implements ILogger {
  constructor(private logger: ILogger) {}

  log(message: string): void {
    this.logger.log(`[${new Date().toISOString()}] ${message}`);
  }
}

class PrefixLoggerDecorator implements ILogger {
  constructor(private logger: ILogger, private prefix: string) {}

  log(message: string): void {
    this.logger.log(`${this.prefix}: ${message}`);
  }
}

// Uso: composición de decoradores
const logger = new PrefixLoggerDecorator(
  new TimestampLoggerDecorator(
    new ConsoleLogger()
  ),
  'APP'
);
// Output: APP: [2026-01-24T10:30:00.000Z] Hello World
```

### Skill de Gobernanza: code-patterns

```yaml
---
name: code-patterns
description: >
  Políticas para aplicación de patrones de diseño y buenas prácticas.
  Trigger: Refactoring, diseño de nuevos módulos.
metadata:
  scope: [root, frontend, backend]
  auto_invoke:
    - "Aplicando patrones de diseño"
    - "Refactorizando código"
    - "Diseñando arquitectura de módulos"
  risk_level: medio
---

## Patrones Críticos

### SIEMPRE
- Aplicar SOLID en diseño de clases
- Usar composición sobre herencia
- Preferir inmutabilidad
- Documentar decisiones de patrón en ADR si es significativo

### NUNCA
- Sobre-ingenierizar (no aplicar patrones innecesarios)
- Usar herencia profunda (máximo 2 niveles)
- Crear god classes (> 500 líneas)
- Ignorar cohesión (una clase = una responsabilidad)
```

---

## ★ Pull Requests y Code Review IA-Native ★

> **"El Pull Request es el momento de verdad. La IA puede asistir en la revisión, pero la decisión de aprobar es humana."**

### Anatomía de un PR IA-Native

```markdown
## [PT-2026-042] Implementar autenticación OAuth2

### Descripción
Implementación de autenticación OAuth2 con Google y GitHub
como proveedores iniciales.

### Policy Ticket
- **ID:** PT-2026-042
- **Dominio:** security
- **Riesgo:** alto
- **Autonomía aplicada:** propuesta con revisión

### Cambios Realizados
- [ ] Nuevo: OAuth2 service con soporte multi-provider
- [ ] Nuevo: Endpoints /auth/google y /auth/github
- [ ] Modificado: User entity para soportar OAuth providers
- [ ] Nuevo: Tests de integración para flujo OAuth

### Evidencias Producidas
- [ ] Tests unitarios: ✅ 47 tests, 100% pasando
- [ ] Tests integración: ✅ 12 tests, 100% pasando
- [ ] Cobertura: 89% (anterior: 87%)
- [ ] Análisis estático: ✅ Sin nuevos warnings
- [ ] Security scan: ✅ Sin vulnerabilidades

### Checklist de Seguridad (Dominio Alto Riesgo)
- [ ] Tokens almacenados de forma segura
- [ ] Secrets no hardcodeados
- [ ] HTTPS obligatorio
- [ ] Rate limiting implementado
- [ ] Logs sin datos sensibles

### Screenshots / Demos
[Si aplica]

### Notas para Reviewers
- Revisar especialmente el manejo de tokens en `auth.service.ts:45-89`
- La decisión de usar JWT sobre sessions está documentada en ADR-015

### Generado con Asistencia IA
- **Modelo:** Claude Opus 4.5
- **Supervisión:** @tech-lead (aprobó approach)
- **Código generado:** ~60%
- **Código modificado por humano:** ~40%
```

### Automatización de Code Review

```yaml
# .github/workflows/pr-review.yml
name: Automated PR Review

on:
  pull_request:
    types: [opened, synchronize]

jobs:
  automated-review:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Check PR Size
        run: |
          CHANGED_FILES=$(git diff --name-only origin/${{ github.base_ref }}...HEAD | wc -l)
          if [ $CHANGED_FILES -gt 20 ]; then
            echo "::warning::PR has $CHANGED_FILES files. Consider splitting."
          fi

          LINES_CHANGED=$(git diff --stat origin/${{ github.base_ref }}...HEAD | tail -1 | grep -oE '[0-9]+' | head -1)
          if [ $LINES_CHANGED -gt 500 ]; then
            echo "::warning::PR has $LINES_CHANGED lines changed. Consider splitting."
          fi

      - name: Check for TODO/FIXME
        run: |
          if grep -rn "TODO\|FIXME" --include="*.ts" --include="*.tsx" .; then
            echo "::warning::Found TODO/FIXME comments. Ensure they have tickets."
          fi

      - name: Check for Console Logs
        run: |
          if grep -rn "console.log\|console.error" --include="*.ts" --include="*.tsx" src/; then
            echo "::error::Remove console.log statements before merging"
            exit 1
          fi

      - name: Verify Policy Ticket
        run: |
          PR_BODY="${{ github.event.pull_request.body }}"
          if ! echo "$PR_BODY" | grep -qE "PT-[0-9]{4}-[0-9]+"; then
            echo "::error::PR must reference a Policy Ticket"
            exit 1
          fi

      - name: Check Coverage Delta
        run: |
          # Comparar cobertura antes y después
          # Fallar si disminuye más de 2%
          echo "Checking coverage delta..."
```

### Skill de Gobernanza: pull-request-review

```yaml
---
name: pull-request-review
description: >
  Políticas para creación y revisión de Pull Requests.
  Trigger: Creación de PR, revisión de código.
metadata:
  scope: [root]
  auto_invoke:
    - "Creando Pull Request"
    - "Revisando código"
    - "Preparando merge"
  risk_level: medio
---

## Patrones Críticos

### SIEMPRE
- Referenciar Policy Ticket en título y descripción
- Listar evidencias producidas
- Mantener PRs pequeños (< 500 líneas, < 20 archivos)
- Incluir screenshots/demos para cambios UI
- Documentar decisiones no obvias

### NUNCA
- Aprobar PR sin revisar cambios
- Mergear sin checks pasando
- Ignorar comentarios de reviewers
- Forzar merge sin aprobaciones requeridas
- Incluir cambios no relacionados al Policy Ticket
```

---

# PARTE XI: WORKFLOW COMPLETO DE IMPLEMENTACIÓN IA-NATIVE

## ★ Guía Maestra: De Requisitos a Producción ★

> **"Un marco de trabajo sin un workflow claro es una colección de buenas intenciones. Esta sección transforma el marco IA-Native en un proceso ejecutable paso a paso."**

Este capítulo describe el workflow completo de implementación de un proyecto de software bajo el marco IA-Native, desde la captura inicial de requisitos hasta el despliegue en producción y la operación continua. Cada fase incluye procesos detallados, artefactos de documentación, Policy Tickets aplicables, roles, niveles de autonomía y criterios de completitud.

### Mapa del Workflow Completo

```
┌─────────────────────────────────────────────────────────────────────────────────────────┐
│                    WORKFLOW DE IMPLEMENTACIÓN IA-NATIVE                                 │
├─────────────────────────────────────────────────────────────────────────────────────────┤
│                                                                                         │
│  FASE 1          FASE 2          FASE 3          FASE 4          FASE 5          FASE 6│
│  ┌─────┐        ┌─────┐        ┌─────┐        ┌─────┐        ┌─────┐        ┌─────┐   │
│  │REQU-│───────▶│ANÁL-│───────▶│ARQU-│───────▶│DESA-│───────▶│TEST-│───────▶│DESP-│   │
│  │ISTOS│        │ISIS │        │ITEC-│        │RROL-│        │ING &│        │LIEG-│   │
│  │     │        │     │        │TURA │        │LO   │        │QA   │        │UE   │   │
│  └─────┘        └─────┘        └─────┘        └─────┘        └─────┘        └─────┘   │
│     │              │              │              │              │              │       │
│     ▼              ▼              ▼              ▼              ▼              ▼       │
│  ┌─────┐        ┌─────┐        ┌─────┐        ┌─────┐        ┌─────┐        ┌─────┐   │
│  │DOC: │        │DOC: │        │DOC: │        │DOC: │        │DOC: │        │DOC: │   │
│  │SRS  │        │PLAN │        │ADRs │        │CODE │        │TEST │        │RUN- │   │
│  │USER │        │ROAD │        │DIAGR│        │PRs  │        │REPS │        │BOOK │   │
│  │STORY│        │MAP  │        │SPECS│        │     │        │     │        │     │   │
│  └─────┘        └─────┘        └─────┘        └─────┘        └─────┘        └─────┘   │
│                                                                                         │
│  ════════════════════════════════════════════════════════════════════════════════════  │
│  GOBERNANZA TRANSVERSAL: Policy Tickets en cada transición de fase                     │
│  ════════════════════════════════════════════════════════════════════════════════════  │
│                                                                                         │
└─────────────────────────────────────────────────────────────────────────────────────────┘
```

---

## ★ FASE 1: Captura y Validación de Requisitos ★

### 1.1 Descripción del Proceso

La fase de captura de requisitos es el fundamento de todo proyecto. En el marco IA-Native, esta fase combina técnicas tradicionales de ingeniería de requisitos con asistencia de IA para acelerar la extracción, estructuración y validación.

```
┌─────────────────────────────────────────────────────────────────┐
│              FASE 1: CAPTURA DE REQUISITOS                      │
├─────────────────────────────────────────────────────────────────┤
│                                                                 │
│  ENTRADAS                                                       │
│  ├── Documentos del cliente (RFP, emails, presentaciones)      │
│  ├── Transcripciones de reuniones                              │
│  ├── Sistema legacy a reemplazar (si existe)                   │
│  ├── Regulaciones aplicables                                   │
│  └── Restricciones conocidas (presupuesto, tiempo, tecnología) │
│                                                                 │
│  SUBPROCESOS                                                    │
│  ┌──────────────────────────────────────────────────────────┐  │
│  │ 1.1 Recolección de Información Bruta                     │  │
│  │     • Entrevistas con stakeholders                       │  │
│  │     • Análisis de documentación existente                │  │
│  │     • Observación de procesos actuales                   │  │
│  │     • IA: Transcripción y resumen de reuniones           │  │
│  └──────────────────────────────────────────────────────────┘  │
│                         ↓                                       │
│  ┌──────────────────────────────────────────────────────────┐  │
│  │ 1.2 Extracción de Requisitos                             │  │
│  │     • IA: Procesamiento de documentos                    │  │
│  │     • IA: Identificación de requisitos implícitos        │  │
│  │     • IA: Clasificación FR/NFR/Restricciones             │  │
│  │     • Humano: Validación de interpretaciones             │  │
│  └──────────────────────────────────────────────────────────┘  │
│                         ↓                                       │
│  ┌──────────────────────────────────────────────────────────┐  │
│  │ 1.3 Estructuración y Formalización                       │  │
│  │     • IA: Generación de User Stories                     │  │
│  │     • IA: Detección de ambigüedades y conflictos         │  │
│  │     • Humano: Resolución de ambigüedades                 │  │
│  │     • Humano: Priorización con stakeholders              │  │
│  └──────────────────────────────────────────────────────────┘  │
│                         ↓                                       │
│  ┌──────────────────────────────────────────────────────────┐  │
│  │ 1.4 Validación Formal                                    │  │
│  │     • Revisión con Product Owner                         │  │
│  │     • Sign-off de stakeholders                           │  │
│  │     • Baseline de requisitos aprobada                    │  │
│  └──────────────────────────────────────────────────────────┘  │
│                                                                 │
│  SALIDAS                                                        │
│  ├── Documento de Visión y Alcance (01-VISION-OBJETIVOS.md)    │
│  ├── Catálogo de Stakeholders (02-USUARIOS-ROLES.md)           │
│  ├── Especificación de Requisitos (03-REQUISITOS-FUNC.md)      │
│  ├── Requisitos No Funcionales (04-REQUISITOS-NO-FUNC.md)      │
│  ├── Backlog inicial priorizado                                │
│  └── Matriz de trazabilidad requisito↔origen                   │
│                                                                 │
└─────────────────────────────────────────────────────────────────┘
```

### 1.2 Artefactos de Documentación

#### 01-VISION-OBJETIVOS.md

```markdown
# Visión y Objetivos del Proyecto

## Metadata
- **Proyecto:** [Nombre]
- **Cliente:** [Organización]
- **Fecha:** [YYYY-MM-DD]
- **Versión:** 1.0
- **Estado:** [Borrador | En Revisión | Aprobado]

## Problema a Resolver
[Descripción clara del problema de negocio que motiva el proyecto]

## Propuesta de Valor
[Cómo el sistema propuesto resuelve el problema]

## Objetivos del Proyecto

### Objetivos de Negocio
| ID | Objetivo | Métrica de Éxito | Prioridad |
|----|----------|------------------|-----------|
| OBJ-01 | [objetivo] | [métrica medible] | Alta/Media/Baja |

### Objetivos Técnicos
| ID | Objetivo | Criterio de Éxito |
|----|----------|-------------------|
| TEC-01 | [objetivo] | [criterio verificable] |

## Alcance

### Dentro del Alcance (IN)
- [Funcionalidad/módulo incluido]
- [Funcionalidad/módulo incluido]

### Fuera del Alcance (OUT)
- [Funcionalidad/módulo excluido y razón]
- [Funcionalidad/módulo excluido y razón]

## Restricciones
| Tipo | Restricción | Impacto |
|------|-------------|---------|
| Tiempo | [fecha límite] | [impacto en alcance] |
| Presupuesto | [monto] | [impacto en alcance] |
| Tecnología | [restricción] | [impacto en diseño] |
| Regulatorio | [norma aplicable] | [requisitos obligatorios] |

## Supuestos
- [Supuesto 1 y su validación]
- [Supuesto 2 y su validación]

## Riesgos Iniciales
| ID | Riesgo | Probabilidad | Impacto | Mitigación |
|----|--------|--------------|---------|------------|
| RSK-01 | [riesgo] | Alta/Media/Baja | Alto/Medio/Bajo | [estrategia] |

## Aprobaciones
| Rol | Nombre | Firma | Fecha |
|-----|--------|-------|-------|
| Sponsor | [nombre] | _____ | _____ |
| Product Owner | [nombre] | _____ | _____ |
| Tech Lead | [nombre] | _____ | _____ |
```

#### 02-USUARIOS-ROLES.md

```markdown
# Usuarios y Roles del Sistema

## Metadata
- **Proyecto:** [Nombre]
- **Versión:** 1.0
- **Última actualización:** [YYYY-MM-DD]

## Stakeholders

### Mapa de Stakeholders
| Stakeholder | Tipo | Influencia | Interés | Estrategia |
|-------------|------|------------|---------|------------|
| [nombre/rol] | Interno/Externo | Alta/Media/Baja | Alto/Medio/Bajo | [cómo gestionar] |

## Roles del Sistema

### ROL-01: [Nombre del Rol]

**Descripción:** [Qué hace este rol en el sistema]

**Persona:**
- Nombre: [Nombre ficticio representativo]
- Edad: [rango]
- Perfil: [descripción breve]
- Objetivos: [qué busca lograr]
- Frustraciones: [qué problemas tiene actualmente]
- Contexto de uso: [cuándo/cómo usa el sistema]

**Permisos:**
| Módulo | Crear | Leer | Actualizar | Eliminar | Especiales |
|--------|-------|------|------------|----------|------------|
| [módulo] | ✓/✗ | ✓/✗ | ✓/✗ | ✓/✗ | [permisos especiales] |

**Flujos Principales:**
1. [Flujo 1 que este rol ejecuta frecuentemente]
2. [Flujo 2 que este rol ejecuta frecuentemente]

### ROL-02: [Nombre del Rol]
[Repetir estructura...]

## Matriz de Roles y Permisos (Resumen)

| Permiso/Acción | Admin | Coordinador | Usuario | Invitado |
|----------------|-------|-------------|---------|----------|
| Gestionar usuarios | ✓ | ✗ | ✗ | ✗ |
| Ver reportes | ✓ | ✓ | Propios | ✗ |
| [acción] | ... | ... | ... | ... |

## Flujos de Usuario por Rol

### Flujo: [Nombre del Flujo]
```
[Rol] → [Acción 1] → [Sistema responde] → [Acción 2] → [Resultado]
```
```

#### 03-REQUISITOS-FUNCIONALES.md

```markdown
# Requisitos Funcionales

## Metadata
- **Proyecto:** [Nombre]
- **Versión:** 1.0
- **Estado:** [Borrador | Aprobado]

## Módulos del Sistema

### MÓD-01: [Nombre del Módulo]

**Descripción:** [Propósito del módulo]
**Prioridad:** Alta/Media/Baja
**Dependencias:** [Otros módulos de los que depende]

#### Historias de Usuario

##### US-001: [Título de la Historia]

**Historia:**
Como [rol]
Quiero [funcionalidad]
Para [beneficio/valor]

**Prioridad:** Must Have / Should Have / Could Have / Won't Have
**Estimación IA:** [SP sugeridos] | **Estimación Equipo:** [SP acordados]

**Criterios de Aceptación:**
- [ ] Dado [contexto], cuando [acción], entonces [resultado]
- [ ] Dado [contexto], cuando [acción], entonces [resultado]
- [ ] Dado [contexto], cuando [acción], entonces [resultado]

**Reglas de Negocio:**
- RN-001: [Regla de negocio aplicable]
- RN-002: [Regla de negocio aplicable]

**Mockup/Wireframe:** [Link o referencia]

**Notas Técnicas:**
- [Consideración técnica relevante]

**Trazabilidad:**
- Origen: [Documento/reunión donde surgió]
- Requisitos relacionados: [US-XXX, US-YYY]

---

##### US-002: [Título]
[Repetir estructura...]

### MÓD-02: [Nombre del Módulo]
[Repetir estructura...]

## Matriz de Trazabilidad

| Requisito | Origen | Stakeholder | Módulo | Prioridad | Estado |
|-----------|--------|-------------|--------|-----------|--------|
| US-001 | Reunión 2024-01-15 | [nombre] | MÓD-01 | Must Have | Aprobado |
```

#### 04-REQUISITOS-NO-FUNCIONALES.md

```markdown
# Requisitos No Funcionales

## Metadata
- **Proyecto:** [Nombre]
- **Versión:** 1.0

## Categorías de Requisitos No Funcionales

### RNF-PERF: Performance

| ID | Requisito | Métrica | Valor Objetivo | Valor Mínimo |
|----|-----------|---------|----------------|--------------|
| RNF-PERF-01 | Tiempo de respuesta API | Latencia p95 | < 200ms | < 500ms |
| RNF-PERF-02 | Tiempo de carga inicial | LCP | < 2.5s | < 4s |
| RNF-PERF-03 | Usuarios concurrentes | Sin degradación | 100 | 50 |

### RNF-SEC: Seguridad

| ID | Requisito | Estándar/Norma | Verificación |
|----|-----------|----------------|--------------|
| RNF-SEC-01 | Autenticación | OAuth 2.0 / JWT | Test de penetración |
| RNF-SEC-02 | Encriptación en tránsito | TLS 1.3 | Scan SSL |
| RNF-SEC-03 | Encriptación en reposo | AES-256 | Auditoría |

### RNF-SCAL: Escalabilidad

| ID | Requisito | Escenario | Estrategia |
|----|-----------|-----------|------------|
| RNF-SCAL-01 | Horizontal scaling | +50% carga | Auto-scaling grupos |
| RNF-SCAL-02 | Database scaling | 10M registros | Particionamiento |

### RNF-AVAIL: Disponibilidad

| ID | Requisito | SLA | Estrategia |
|----|-----------|-----|------------|
| RNF-AVAIL-01 | Uptime | 99.9% | Multi-AZ, failover |
| RNF-AVAIL-02 | RTO | < 1 hora | Backups automatizados |
| RNF-AVAIL-03 | RPO | < 15 min | Replicación continua |

### RNF-MAINT: Mantenibilidad

| ID | Requisito | Métrica | Objetivo |
|----|-----------|---------|----------|
| RNF-MAINT-01 | Cobertura de tests | % líneas | > 80% |
| RNF-MAINT-02 | Complejidad ciclomática | Por función | < 10 |
| RNF-MAINT-03 | Documentación | Docstrings | 100% públicas |

### RNF-USAB: Usabilidad

| ID | Requisito | Métrica | Objetivo |
|----|-----------|---------|----------|
| RNF-USAB-01 | Accesibilidad | WCAG | 2.1 AA |
| RNF-USAB-02 | Tiempo de aprendizaje | Tarea básica | < 5 min |
| RNF-USAB-03 | Tasa de error usuario | Por sesión | < 2% |
```

### 1.3 Policy Tickets de Fase 1

```yaml
---
id: PT-FASE1-001
phase: "1-Requisitos"
type: "requirements-capture"
domain: analysis
risk_level: medium
---

## Policy Ticket: Captura de Requisitos con IA

### Intención
Utilizar IA para acelerar la extracción y estructuración de requisitos
desde documentos del cliente, transcripciones y sistemas existentes.

### Autonomía por Actividad
| Actividad | Autonomía IA | Validación |
|-----------|--------------|------------|
| Transcripción de reuniones | Completa | Revisión de precisión |
| Extracción de requisitos de docs | Completa | Validación humana obligatoria |
| Clasificación FR/NFR | Completa | Revisión por analista |
| Generación de User Stories | Con revisión | Aprobación de PO |
| Detección de ambigüedades | Completa | Resolución humana |
| Priorización | Solo propuesta | Decisión de stakeholders |
| Validación final | Prohibido | Solo humano |

### Evidencias Requeridas
- [ ] Lista de fuentes procesadas
- [ ] Requisitos extraídos con trazabilidad a origen
- [ ] Ambigüedades detectadas y resueltas
- [ ] User Stories generadas con criterios de aceptación
- [ ] Sign-off de stakeholders en documentos finales

### Criterios de Completitud de Fase
- [ ] 01-VISION-OBJETIVOS.md aprobado
- [ ] 02-USUARIOS-ROLES.md aprobado
- [ ] 03-REQUISITOS-FUNCIONALES.md con todas las US priorizadas
- [ ] 04-REQUISITOS-NO-FUNCIONALES.md con métricas definidas
- [ ] Backlog inicial creado en herramienta de gestión
- [ ] Matriz de trazabilidad completa
```

### 1.4 Roles y Responsabilidades

| Rol | Responsabilidades en Fase 1 |
|-----|----------------------------|
| **Product Owner** | Validar requisitos, priorizar backlog, aprobar documentos |
| **Analista de Negocio** | Facilitar entrevistas, resolver ambigüedades, documentar |
| **Tech Lead** | Validar viabilidad técnica, identificar restricciones |
| **Stakeholders** | Proporcionar información, validar interpretaciones, aprobar |
| **IA** | Transcribir, extraer, clasificar, generar borradores |

### 1.5 Checklist de Salida de Fase 1

```
GATE DE SALIDA - FASE 1: REQUISITOS
═══════════════════════════════════════════════════════════════

□ Documentos Completos
  □ 01-VISION-OBJETIVOS.md firmado por sponsor
  □ 02-USUARIOS-ROLES.md con todas las personas definidas
  □ 03-REQUISITOS-FUNCIONALES.md con US priorizadas
  □ 04-REQUISITOS-NO-FUNCIONALES.md con métricas

□ Validaciones
  □ Stakeholders han revisado y aprobado requisitos
  □ Tech Lead ha validado viabilidad técnica
  □ No hay requisitos con ambigüedades sin resolver
  □ Todos los requisitos tienen trazabilidad a origen

□ Artefactos de Gestión
  □ Backlog creado en [Jira/Azure DevOps/GitHub Projects]
  □ Épicas y User Stories cargadas
  □ Priorización MoSCoW aplicada
  □ Dependencias identificadas

□ Gobernanza
  □ Policy Ticket PT-FASE1-001 completado
  □ Evidencias documentadas
  □ Lecciones aprendidas registradas

APROBACIÓN PARA PASAR A FASE 2:
□ Product Owner: _________________ Fecha: _______
□ Tech Lead: _________________ Fecha: _______
□ Sponsor: _________________ Fecha: _______
```

---

## ★ FASE 2: Análisis, Planificación y Estimación ★

### 2.1 Descripción del Proceso

```
┌─────────────────────────────────────────────────────────────────┐
│              FASE 2: ANÁLISIS Y PLANIFICACIÓN                   │
├─────────────────────────────────────────────────────────────────┤
│                                                                 │
│  ENTRADAS                                                       │
│  ├── Documentos de Fase 1 aprobados                            │
│  ├── Backlog priorizado                                        │
│  ├── Restricciones de tiempo/presupuesto                       │
│  └── Capacidad del equipo                                      │
│                                                                 │
│  SUBPROCESOS                                                    │
│  ┌──────────────────────────────────────────────────────────┐  │
│  │ 2.1 Análisis de Complejidad                              │  │
│  │     • IA: Análisis de historias similares en histórico   │  │
│  │     • IA: Identificación de riesgos técnicos             │  │
│  │     • Equipo: Planning Poker / estimación                │  │
│  └──────────────────────────────────────────────────────────┘  │
│                         ↓                                       │
│  ┌──────────────────────────────────────────────────────────┐  │
│  │ 2.2 Definición de Releases y Sprints                     │  │
│  │     • Agrupación de US en releases                       │  │
│  │     • Definición de MVP                                  │  │
│  │     • Planificación de sprints                           │  │
│  │     • Identificación de dependencias críticas            │  │
│  └──────────────────────────────────────────────────────────┘  │
│                         ↓                                       │
│  ┌──────────────────────────────────────────────────────────┐  │
│  │ 2.3 Gestión de Riesgos                                   │  │
│  │     • IA: Análisis de riesgos por patrones               │  │
│  │     • Equipo: Evaluación y priorización                  │  │
│  │     • Definición de planes de mitigación                 │  │
│  └──────────────────────────────────────────────────────────┘  │
│                         ↓                                       │
│  ┌──────────────────────────────────────────────────────────┐  │
│  │ 2.4 Definición de Recursos                               │  │
│  │     • Asignación de equipo                               │  │
│  │     • Identificación de skills requeridos                │  │
│  │     • Planificación de capacitación si necesaria         │  │
│  └──────────────────────────────────────────────────────────┘  │
│                                                                 │
│  SALIDAS                                                        │
│  ├── Roadmap del proyecto                                      │
│  ├── Plan de releases                                          │
│  ├── Planificación de sprints (al menos Sprint 1 detallado)   │
│  ├── Registro de riesgos                                       │
│  ├── Plan de recursos                                          │
│  └── Cronograma general                                        │
│                                                                 │
└─────────────────────────────────────────────────────────────────┘
```

### 2.2 Artefactos de Documentación

#### PLAN-RELEASES.md

```markdown
# Plan de Releases

## Metadata
- **Proyecto:** [Nombre]
- **Fecha de creación:** [YYYY-MM-DD]
- **Versión:** 1.0

## Visión de Releases

```
Timeline del Proyecto
═══════════════════════════════════════════════════════════════

     Sprint 1-3          Sprint 4-6          Sprint 7-9
    ┌──────────┐        ┌──────────┐        ┌──────────┐
    │  MVP     │───────▶│ Release  │───────▶│ Release  │
    │ v0.1.0   │        │  v1.0.0  │        │  v1.1.0  │
    └──────────┘        └──────────┘        └──────────┘

    Features:           Features:           Features:
    - Auth básica       - Auth completa     - Reportes
    - CRUD core         - Integraciones     - Analytics
    - UI básica         - UI completa       - Optimización
```

## Release 1: MVP (v0.1.0)

**Objetivo:** [Objetivo del MVP]
**Fecha objetivo:** [YYYY-MM-DD]
**Duración:** [X sprints]

### Épicas Incluidas
| Épica | User Stories | Story Points | Prioridad |
|-------|--------------|--------------|-----------|
| EPIC-01: [nombre] | US-001, US-002, US-003 | 21 | Must Have |
| EPIC-02: [nombre] | US-004, US-005 | 13 | Must Have |

### Criterios de Éxito del Release
- [ ] [Criterio medible 1]
- [ ] [Criterio medible 2]
- [ ] [Criterio medible 3]

### Riesgos del Release
| Riesgo | Probabilidad | Impacto | Mitigación |
|--------|--------------|---------|------------|
| [riesgo] | Alta/Media/Baja | Alto/Medio/Bajo | [estrategia] |

## Release 2: v1.0.0
[Repetir estructura...]
```

#### PLAN-SPRINT.md (Template)

```markdown
# Plan de Sprint [N]

## Metadata
- **Sprint:** [número]
- **Fecha inicio:** [YYYY-MM-DD]
- **Fecha fin:** [YYYY-MM-DD]
- **Capacidad:** [X story points]

## Objetivo del Sprint
[Objetivo claro y medible - una frase]

## Backlog del Sprint

### Comprometido
| ID | User Story | SP | Asignado | Dependencias |
|----|------------|----:|----------|--------------|
| US-001 | [título] | 5 | @dev1 | - |
| US-002 | [título] | 8 | @dev2 | US-001 |

**Total comprometido:** [X] SP

### Stretch Goals (si hay capacidad)
| ID | User Story | SP |
|----|------------|----:|
| US-010 | [título] | 3 |

## Riesgos del Sprint
| Riesgo | Mitigación | Owner |
|--------|------------|-------|
| [riesgo] | [acción] | @persona |

## Dependencias Externas
- [ ] [Dependencia 1] - Responsable: [nombre] - Fecha límite: [fecha]
- [ ] [Dependencia 2] - Responsable: [nombre] - Fecha límite: [fecha]

## Definition of Done (Recordatorio)
- [ ] Código completo y revisado
- [ ] Tests unitarios pasando (>80% cobertura)
- [ ] Tests de integración pasando
- [ ] Documentación actualizada
- [ ] PR aprobado y mergeado
- [ ] Desplegado en staging
- [ ] PO ha validado criterios de aceptación
```

#### REGISTRO-RIESGOS.md

```markdown
# Registro de Riesgos

## Metadata
- **Proyecto:** [Nombre]
- **Última actualización:** [YYYY-MM-DD]

## Matriz de Riesgos Activos

| ID | Riesgo | Categoría | Prob. | Impacto | Score | Estado | Owner | Mitigación |
|----|--------|-----------|-------|---------|-------|--------|-------|------------|
| RSK-001 | [descripción] | Técnico | Alta | Alto | 9 | Activo | @nombre | [estrategia] |
| RSK-002 | [descripción] | Recursos | Media | Medio | 4 | Mitigando | @nombre | [estrategia] |

## Escala de Evaluación

**Probabilidad:** Alta (3), Media (2), Baja (1)
**Impacto:** Alto (3), Medio (2), Bajo (1)
**Score:** Probabilidad × Impacto

## Detalle de Riesgos

### RSK-001: [Título del Riesgo]

**Descripción:** [Descripción detallada]
**Categoría:** Técnico / Recursos / Externo / Negocio
**Fecha identificación:** [YYYY-MM-DD]
**Owner:** @persona

**Análisis:**
- Probabilidad: [Alta/Media/Baja] - Justificación: [razón]
- Impacto: [Alto/Medio/Bajo] - Justificación: [razón]

**Estrategia:** Mitigar / Aceptar / Transferir / Evitar

**Plan de Mitigación:**
1. [Acción 1] - Responsable: @nombre - Fecha: [fecha]
2. [Acción 2] - Responsable: @nombre - Fecha: [fecha]

**Plan de Contingencia:** (si el riesgo se materializa)
1. [Acción inmediata]
2. [Acción de recuperación]

**Indicadores de Alerta:**
- [Señal temprana 1]
- [Señal temprana 2]

**Historial:**
| Fecha | Evento | Acción Tomada |
|-------|--------|---------------|
| [fecha] | [qué pasó] | [qué se hizo] |
```

### 2.3 Policy Tickets de Fase 2

```yaml
---
id: PT-FASE2-001
phase: "2-Planificación"
type: "planning-estimation"
domain: management
risk_level: medium
---

## Policy Ticket: Planificación y Estimación con IA

### Intención
Utilizar IA para asistir en la estimación de esfuerzo, identificación
de riesgos y generación de planes de sprint basados en datos históricos.

### Autonomía por Actividad
| Actividad | Autonomía IA | Validación |
|-----------|--------------|------------|
| Análisis de US similares en histórico | Completa | Información |
| Sugerencia de story points | Solo propuesta | Equipo decide |
| Identificación de dependencias | Completa | Revisión por Tech Lead |
| Detección de riesgos por patrones | Completa | Evaluación humana |
| Generación de cronograma | Solo propuesta | Aprobación de PM |
| Compromiso de fechas | Prohibido | Solo humano |

### Evidencias Requeridas
- [ ] Roadmap aprobado
- [ ] Plan de releases documentado
- [ ] Al menos Sprint 1 planificado en detalle
- [ ] Registro de riesgos inicial
- [ ] Estimaciones validadas por el equipo

### Criterios de Completitud de Fase
- [ ] MVP definido con alcance claro
- [ ] Todas las US tienen estimación del equipo
- [ ] Dependencias críticas identificadas
- [ ] Riesgos evaluados y con planes de mitigación
- [ ] Recursos asignados
- [ ] Stakeholders informados del plan
```

### 2.4 Checklist de Salida de Fase 2

```
GATE DE SALIDA - FASE 2: PLANIFICACIÓN
═══════════════════════════════════════════════════════════════

□ Documentos Completos
  □ PLAN-RELEASES.md con todos los releases definidos
  □ PLAN-SPRINT-01.md detallado y listo para ejecutar
  □ REGISTRO-RIESGOS.md con riesgos identificados
  □ Cronograma general aprobado

□ Estimaciones
  □ Todas las US del MVP tienen story points
  □ Equipo ha validado estimaciones (no solo IA)
  □ Buffer de contingencia incluido (15-20%)
  □ Velocidad del equipo establecida o estimada

□ Recursos
  □ Equipo asignado con roles claros
  □ Skills gaps identificados
  □ Plan de capacitación si necesario
  □ Herramientas y accesos listos

□ Gestión
  □ Backlog ordenado por prioridad en herramienta
  □ Sprint 1 listo para comenzar
  □ Ceremonias agendadas
  □ Canales de comunicación establecidos

□ Gobernanza
  □ Policy Ticket PT-FASE2-001 completado
  □ Evidencias documentadas

APROBACIÓN PARA PASAR A FASE 3:
□ Product Owner: _________________ Fecha: _______
□ Tech Lead: _________________ Fecha: _______
□ Project Manager: _________________ Fecha: _______
```

---

## ★ FASE 3: Arquitectura y Diseño Técnico ★

### 3.1 Descripción del Proceso

```
┌─────────────────────────────────────────────────────────────────┐
│              FASE 3: ARQUITECTURA Y DISEÑO                      │
├─────────────────────────────────────────────────────────────────┤
│                                                                 │
│  ENTRADAS                                                       │
│  ├── Requisitos funcionales y no funcionales                   │
│  ├── Restricciones técnicas identificadas                      │
│  ├── Plan de releases                                          │
│  └── Experiencia del equipo                                    │
│                                                                 │
│  SUBPROCESOS                                                    │
│  ┌──────────────────────────────────────────────────────────┐  │
│  │ 3.1 Selección de Stack Tecnológico                       │  │
│  │     • IA: Análisis de opciones vs requisitos             │  │
│  │     • Equipo: Evaluación y selección                     │  │
│  │     • Documentar decisiones en ADR                       │  │
│  └──────────────────────────────────────────────────────────┘  │
│                         ↓                                       │
│  ┌──────────────────────────────────────────────────────────┐  │
│  │ 3.2 Diseño de Arquitectura                               │  │
│  │     • Definición de capas y componentes                  │  │
│  │     • Patrones arquitectónicos                           │  │
│  │     • Diagramas C4 (Contexto, Contenedores, Componentes) │  │
│  │     • IA: Generación de diagramas y documentación        │  │
│  └──────────────────────────────────────────────────────────┘  │
│                         ↓                                       │
│  ┌──────────────────────────────────────────────────────────┐  │
│  │ 3.3 Diseño de Datos                                      │  │
│  │     • Modelo de entidades                                │  │
│  │     • Esquema de base de datos                           │  │
│  │     • Estrategia de migración                            │  │
│  │     • IA: Generación de esquemas y validación            │  │
│  └──────────────────────────────────────────────────────────┘  │
│                         ↓                                       │
│  ┌──────────────────────────────────────────────────────────┐  │
│  │ 3.4 Diseño de UI/UX                                      │  │
│  │     • Wireframes y flujos                                │  │
│  │     • Sistema de diseño                                  │  │
│  │     • Prototipos interactivos                            │  │
│  └──────────────────────────────────────────────────────────┘  │
│                         ↓                                       │
│  ┌──────────────────────────────────────────────────────────┐  │
│  │ 3.5 Definición de Estándares                             │  │
│  │     • Guías de código                                    │  │
│  │     • Patrones a seguir                                  │  │
│  │     • Convenciones de naming                             │  │
│  │     • Configuración de linters                           │  │
│  └──────────────────────────────────────────────────────────┘  │
│                                                                 │
│  SALIDAS                                                        │
│  ├── 05-ARQUITECTURA-STACK.md                                  │
│  ├── 06-MODELO-DATOS.md                                        │
│  ├── 07-DISENO-UI-UX.md                                        │
│  ├── 08-SISTEMA-DISENO-ESTILOS.md                              │
│  ├── 09-PATRONES-CODIGO.md                                     │
│  ├── ADRs (Architecture Decision Records)                      │
│  ├── Diagramas C4                                              │
│  └── Prototipos/Wireframes                                     │
│                                                                 │
└─────────────────────────────────────────────────────────────────┘
```

### 3.2 Artefactos de Documentación

#### 05-ARQUITECTURA-STACK.md

```markdown
# Arquitectura y Stack Tecnológico

## Metadata
- **Proyecto:** [Nombre]
- **Versión:** 1.0
- **Arquitecto:** [Nombre]
- **Fecha:** [YYYY-MM-DD]

## Stack Tecnológico

### Frontend
| Componente | Tecnología | Versión | Justificación |
|------------|------------|---------|---------------|
| Framework | React | 18.x | [razón de elección] |
| Lenguaje | TypeScript | 5.x | [razón] |
| Styling | Tailwind CSS | 3.x | [razón] |
| State | Zustand | 4.x | [razón] |
| Build | Vite | 5.x | [razón] |

### Backend
| Componente | Tecnología | Versión | Justificación |
|------------|------------|---------|---------------|
| Runtime | Python | 3.11 | [razón] |
| Framework | FastAPI | 0.100+ | [razón] |
| ORM | SQLAlchemy | 2.x | [razón] |
| Migrations | Alembic | 1.x | [razón] |

### Datos
| Componente | Tecnología | Versión | Justificación |
|------------|------------|---------|---------------|
| Base de datos | PostgreSQL | 15.x | [razón] |
| Cache | Redis | 7.x | [razón] |
| Search | [si aplica] | - | [razón] |

### Infraestructura
| Componente | Tecnología | Justificación |
|------------|------------|---------------|
| Contenedores | Docker | [razón] |
| Orquestación | Docker Compose / K8s | [razón] |
| CI/CD | GitHub Actions | [razón] |
| Cloud | AWS / GCP / Azure | [razón] |

## Arquitectura del Sistema

### Diagrama de Contexto (C4 Level 1)
```
[Diagrama mostrando sistema, usuarios y sistemas externos]
```

### Diagrama de Contenedores (C4 Level 2)
```
[Diagrama mostrando contenedores/servicios principales]
```

### Diagrama de Componentes (C4 Level 3)
```
[Diagrama detallado de componentes por servicio]
```

## Decisiones Arquitectónicas (ADRs)

| ADR | Título | Estado | Fecha |
|-----|--------|--------|-------|
| ADR-001 | [título] | Aceptado | [fecha] |
| ADR-002 | [título] | Aceptado | [fecha] |

Ver carpeta `/docs/adrs/` para detalle completo.

## Estructura de Carpetas

```
proyecto/
├── frontend/
│   └── src/
│       ├── components/
│       ├── features/
│       ├── hooks/
│       ├── services/
│       ├── store/
│       └── types/
├── backend/
│   └── src/
│       ├── domain/
│       ├── application/
│       ├── infrastructure/
│       └── presentation/
├── docs/
│   ├── adrs/
│   └── diagrams/
└── docker/
```
```

#### 06-MODELO-DATOS.md

```markdown
# Modelo de Datos

## Metadata
- **Proyecto:** [Nombre]
- **Versión:** 1.0
- **Fecha:** [YYYY-MM-DD]

## Diagrama Entidad-Relación

```
[Diagrama ER]
```

## Entidades

### Usuario

**Descripción:** Representa a los usuarios del sistema

| Campo | Tipo | Nullable | Default | Descripción |
|-------|------|----------|---------|-------------|
| id | UUID | No | gen_random_uuid() | PK |
| email | VARCHAR(255) | No | - | Único, índice |
| password_hash | VARCHAR(255) | No | - | Bcrypt hash |
| name | VARCHAR(100) | No | - | Nombre completo |
| role | ENUM | No | 'user' | admin/coord/user |
| created_at | TIMESTAMP | No | NOW() | - |
| updated_at | TIMESTAMP | No | NOW() | Auto-update |

**Índices:**
- `idx_usuario_email` (email) - Único
- `idx_usuario_role` (role)

**Relaciones:**
- Usuario 1:N Entrega (un usuario tiene muchas entregas)

### [Entidad 2]
[Repetir estructura...]

## Migraciones

### Estrategia
- Herramienta: Alembic
- Naming: `YYYYMMDD_HHMMSS_descripcion.py`
- Rollback obligatorio para cada migración

### Migración Inicial
```sql
-- Ejemplo de migración inicial
CREATE TABLE usuarios (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    email VARCHAR(255) NOT NULL UNIQUE,
    ...
);
```
```

### 3.3 Policy Tickets de Fase 3

```yaml
---
id: PT-FASE3-001
phase: "3-Arquitectura"
type: "architecture-design"
domain: architecture
risk_level: high
---

## Policy Ticket: Diseño Arquitectónico con IA

### Intención
Utilizar IA para asistir en el diseño de arquitectura, generación
de diagramas y documentación técnica, con validación humana obligatoria.

### Autonomía por Actividad
| Actividad | Autonomía IA | Validación |
|-----------|--------------|------------|
| Análisis de opciones de stack | Completa | Información para decisión |
| Propuesta de arquitectura | Solo propuesta | Arquitecto decide |
| Generación de diagramas | Con revisión | Validación de precisión |
| Escritura de ADRs | Con revisión | Aprobación de Tech Lead |
| Diseño de modelo de datos | Con revisión | Validación por DBA/Senior |
| Decisiones arquitectónicas | Prohibido | Solo humano |

### Evidencias Requeridas
- [ ] Stack documentado con justificaciones
- [ ] Diagramas C4 (al menos niveles 1 y 2)
- [ ] ADRs para decisiones significativas
- [ ] Modelo de datos con esquema
- [ ] Estructura de carpetas definida

### Criterios de Completitud de Fase
- [ ] 05-ARQUITECTURA-STACK.md aprobado
- [ ] 06-MODELO-DATOS.md aprobado
- [ ] 07-DISENO-UI-UX.md aprobado
- [ ] 08-SISTEMA-DISENO-ESTILOS.md aprobado
- [ ] 09-PATRONES-CODIGO.md aprobado
- [ ] Todos los ADRs críticos documentados
- [ ] Equipo ha revisado y aceptado arquitectura
```

### 3.4 Checklist de Salida de Fase 3

```
GATE DE SALIDA - FASE 3: ARQUITECTURA
═══════════════════════════════════════════════════════════════

□ Documentos Técnicos
  □ 05-ARQUITECTURA-STACK.md completo y aprobado
  □ 06-MODELO-DATOS.md con todas las entidades
  □ 07-DISENO-UI-UX.md con wireframes/prototipos
  □ 08-SISTEMA-DISENO-ESTILOS.md con tokens definidos
  □ 09-PATRONES-CODIGO.md con convenciones

□ Decisiones Arquitectónicas
  □ Stack tecnológico seleccionado y justificado
  □ ADRs documentados para decisiones clave
  □ Patrones arquitectónicos definidos (Clean Arch, etc.)
  □ Trade-offs documentados y aceptados

□ Diseño de Datos
  □ Modelo ER completo
  □ Esquema de BD listo para implementar
  □ Estrategia de migraciones definida
  □ Índices y optimizaciones planificadas

□ Diseño de UI/UX
  □ Wireframes para flujos principales
  □ Sistema de diseño documentado
  □ Componentes base identificados
  □ Accesibilidad considerada (WCAG 2.1 AA)

□ Infraestructura
  □ Ambientes definidos (dev, staging, prod)
  □ Estrategia de despliegue clara
  □ CI/CD pipeline diseñado

□ Gobernanza
  □ Policy Ticket PT-FASE3-001 completado
  □ Equipo ha revisado y aceptado arquitectura

APROBACIÓN PARA PASAR A FASE 4:
□ Tech Lead / Arquitecto: _________________ Fecha: _______
□ Product Owner: _________________ Fecha: _______
```

---

## ★ FASE 4: Desarrollo ★

### 4.1 Descripción del Proceso

```
┌─────────────────────────────────────────────────────────────────┐
│                    FASE 4: DESARROLLO                           │
├─────────────────────────────────────────────────────────────────┤
│                                                                 │
│  CICLO DE SPRINT                                                │
│  ┌──────────────────────────────────────────────────────────┐  │
│  │                                                          │  │
│  │  ┌─────────┐    ┌─────────┐    ┌─────────┐    ┌───────┐ │  │
│  │  │ SPRINT  │───▶│  DEV    │───▶│  CODE   │───▶│ SPRINT│ │  │
│  │  │ PLANNING│    │ (Daily) │    │ REVIEW  │    │ REVIEW│ │  │
│  │  └─────────┘    └─────────┘    └─────────┘    └───────┘ │  │
│  │       │              │              │              │     │  │
│  │       ▼              ▼              ▼              ▼     │  │
│  │  ┌─────────┐    ┌─────────┐    ┌─────────┐    ┌───────┐ │  │
│  │  │ Policy  │    │   PRs   │    │ Merge   │    │ RETRO │ │  │
│  │  │ Tickets │    │ + Tests │    │ + Deploy│    │       │ │  │
│  │  └─────────┘    └─────────┘    └─────────┘    └───────┘ │  │
│  │                                                          │  │
│  └──────────────────────────────────────────────────────────┘  │
│                                                                 │
│  FLUJO DE DESARROLLO DIARIO                                     │
│  ┌──────────────────────────────────────────────────────────┐  │
│  │                                                          │  │
│  │  1. Tomar US asignada del sprint                        │  │
│  │  2. Crear Policy Ticket (o usar existente)              │  │
│  │  3. Crear branch desde develop                          │  │
│  │  4. Desarrollo con asistencia IA                        │  │
│  │  5. Tests (unit + integration)                          │  │
│  │  6. PR con referencia a Policy Ticket                   │  │
│  │  7. Code Review                                         │  │
│  │  8. Merge + Deploy a staging                            │  │
│  │  9. Validación de PO                                    │  │
│  │                                                          │  │
│  └──────────────────────────────────────────────────────────┘  │
│                                                                 │
│  DESARROLLO POR CAPA                                            │
│  ┌──────────────────────────────────────────────────────────┐  │
│  │                                                          │  │
│  │  ┌──────────┐  ┌──────────┐  ┌──────────┐  ┌──────────┐ │  │
│  │  │ BACKEND  │  │ FRONTEND │  │  MOBILE  │  │   APIs   │ │  │
│  │  │          │  │          │  │          │  │          │ │  │
│  │  │ Domain   │  │Components│  │ Screens  │  │Endpoints │ │  │
│  │  │ Use Cases│  │ Features │  │ Services │  │ OpenAPI  │ │  │
│  │  │ Repos    │  │ Hooks    │  │ State    │  │ Versión  │ │  │
│  │  │ Infra    │  │ Services │  │ Native   │  │ Gateway  │ │  │
│  │  └──────────┘  └──────────┘  └──────────┘  └──────────┘ │  │
│  │                                                          │  │
│  └──────────────────────────────────────────────────────────┘  │
│                                                                 │
└─────────────────────────────────────────────────────────────────┘
```

### 4.2 Workflow Detallado de Desarrollo

#### Paso 1: Inicio de User Story

```markdown
## Checklist: Inicio de User Story

□ US asignada en el sprint actual
□ Criterios de aceptación claros
□ Dependencias resueltas o no bloqueantes
□ Diseño/wireframe disponible (si aplica)
□ Dudas resueltas con PO

## Crear Policy Ticket

```yaml
---
id: PT-DEV-[sprint]-[número]
us_reference: US-XXX
developer: @nombre
created: YYYY-MM-DD
---

## Policy Ticket de Desarrollo

### User Story
[Copiar título de la US]

### Alcance Técnico
- Archivos a crear: [lista]
- Archivos a modificar: [lista]
- Tests requeridos: [unit/integration/e2e]

### Autonomía IA
| Tarea | Autonomía |
|-------|-----------|
| Scaffolding inicial | Completa |
| Lógica de negocio | Con revisión |
| Tests | Completa |
| Estilos/UI | Completa |

### Evidencias
- [ ] Tests unitarios (>80% cobertura nueva)
- [ ] Tests de integración
- [ ] Lint sin errores
- [ ] Build exitoso
- [ ] PR creado con referencia
```

□ Policy Ticket creado y vinculado
```

#### Paso 2: Desarrollo con IA

```markdown
## Workflow de Desarrollo con IA

### 2.1 Crear Branch
```bash
git checkout develop
git pull origin develop
git checkout -b feature/US-XXX-descripcion-corta
```

### 2.2 Desarrollo Iterativo

```
┌─────────────────────────────────────────────────────────────────┐
│              CICLO DE DESARROLLO CON IA                         │
├─────────────────────────────────────────────────────────────────┤
│                                                                 │
│  ┌─────────────────────────────────────────────────────────┐   │
│  │ 1. DEFINIR INTENCIÓN                                    │   │
│  │    "Implementar endpoint POST /api/users con validación"│   │
│  └─────────────────────────────────────────────────────────┘   │
│                         ↓                                       │
│  ┌─────────────────────────────────────────────────────────┐   │
│  │ 2. IA GENERA CÓDIGO                                     │   │
│  │    • Siguiendo patrones de 09-PATRONES-CODIGO.md        │   │
│  │    • Respetando arquitectura de 05-ARQUITECTURA.md      │   │
│  │    • Con skill de gobernanza cargado                    │   │
│  └─────────────────────────────────────────────────────────┘   │
│                         ↓                                       │
│  ┌─────────────────────────────────────────────────────────┐   │
│  │ 3. DESARROLLADOR REVISA                                 │   │
│  │    • Lógica correcta                                    │   │
│  │    • Patrones respetados                                │   │
│  │    • Edge cases cubiertos                               │   │
│  │    • Seguridad considerada                              │   │
│  └─────────────────────────────────────────────────────────┘   │
│                         ↓                                       │
│  ┌─────────────────────────────────────────────────────────┐   │
│  │ 4. ITERAR SI NECESARIO                                  │   │
│  │    • Pedir correcciones específicas                     │   │
│  │    • Agregar casos no considerados                      │   │
│  │    • Refinar implementación                             │   │
│  └─────────────────────────────────────────────────────────┘   │
│                         ↓                                       │
│  ┌─────────────────────────────────────────────────────────┐   │
│  │ 5. TESTS                                                │   │
│  │    • IA genera tests unitarios                          │   │
│  │    • IA genera tests de integración                     │   │
│  │    • Desarrollador valida cobertura y casos             │   │
│  └─────────────────────────────────────────────────────────┘   │
│                         ↓                                       │
│  ┌─────────────────────────────────────────────────────────┐   │
│  │ 6. COMMIT ATÓMICO                                       │   │
│  │    git commit -m "feat(users): add POST /api/users      │   │
│  │                                                          │   │
│  │    PT-DEV-03-001                                        │   │
│  │    US-XXX"                                              │   │
│  └─────────────────────────────────────────────────────────┘   │
│                                                                 │
└─────────────────────────────────────────────────────────────────┘
```

### 2.3 Convención de Commits

```
<type>(<scope>): <description>

[body opcional]

PT-XXX-XXX
US-XXX (si aplica)

---
Tipos: feat, fix, refactor, test, docs, style, chore
Scope: módulo o componente afectado
```
```

#### Paso 3: Pull Request

```markdown
## Template de Pull Request

### Título
`[PT-DEV-03-001] feat(users): implementar endpoint POST /api/users`

### Descripción

#### Policy Ticket
- **ID:** PT-DEV-03-001
- **User Story:** US-XXX - [título]
- **Dominio:** backend
- **Riesgo:** medio

#### Cambios Realizados
- [x] Nuevo endpoint POST /api/users
- [x] Validación de input con Pydantic
- [x] Use case CreateUser implementado
- [x] Repository con persistencia Postgres
- [x] Tests unitarios (85% cobertura)
- [x] Tests de integración

#### Archivos Modificados
```
backend/src/
├── domain/entities/User.py (nuevo)
├── application/use_cases/CreateUser.py (nuevo)
├── infrastructure/repositories/UserRepository.py (nuevo)
├── presentation/controllers/UserController.py (nuevo)
└── tests/
    ├── unit/test_create_user.py (nuevo)
    └── integration/test_user_endpoint.py (nuevo)
```

#### Evidencias
- [x] Tests pasan localmente
- [x] Lint sin errores
- [x] Build exitoso
- [x] Cobertura: 85% (mínimo: 80%)

#### Screenshots (si aplica UI)
[Captura de pantalla]

#### Notas para Reviewers
- Revisar validación de email en línea 45
- Decisión de usar UUID vs incremental documentada en ADR-005

#### Checklist Pre-merge
- [ ] Self-review completado
- [ ] Tests pasan en CI
- [ ] Documentación actualizada
- [ ] No hay console.log/print de debug

---
*Generado con asistencia IA bajo Policy Ticket PT-DEV-03-001*
```

#### Paso 4: Code Review

```markdown
## Checklist de Code Review

### Aspectos Funcionales
□ Cumple con criterios de aceptación de la US
□ Lógica de negocio correcta
□ Edge cases manejados
□ Validaciones apropiadas

### Aspectos Técnicos
□ Sigue patrones de 09-PATRONES-CODIGO.md
□ Respeta arquitectura (Clean Architecture)
□ No viola principios SOLID
□ Naming conventions respetadas
□ Sin código duplicado innecesario

### Calidad de Código
□ Legible y mantenible
□ Funciones < 50 líneas
□ Archivos < 500 líneas
□ Complejidad ciclomática razonable
□ Docstrings/TSDoc donde corresponde

### Seguridad
□ No hay vulnerabilidades obvias
□ Inputs validados
□ No expone datos sensibles
□ Manejo de errores apropiado

### Tests
□ Cobertura > 80%
□ Casos edge probados
□ Tests son legibles y mantenibles
□ No hay tests flaky

### Documentación
□ ADR si hay decisión significativa
□ Comentarios solo donde necesarios
□ README actualizado si aplica

### Resultado
□ APROBAR - Listo para merge
□ SOLICITAR CAMBIOS - [listar cambios]
□ COMENTAR - Sugerencias no bloqueantes
```

### 4.3 Policy Tickets de Fase 4

```yaml
---
id: PT-FASE4-TEMPLATE
phase: "4-Desarrollo"
type: "development"
---

## Policy Ticket Template: Desarrollo de User Story

### Información
- Sprint: [número]
- User Story: [US-XXX]
- Desarrollador: [@nombre]
- Reviewer: [@nombre]

### Alcance
**En scope:**
- [archivo/módulo 1]
- [archivo/módulo 2]

**Fuera de scope:**
- [no tocar X]
- [no modificar Y]

### Autonomía IA por Capa

| Capa | Autonomía | Supervisión |
|------|-----------|-------------|
| Domain entities | Con revisión | Validar lógica de negocio |
| Use cases | Con revisión | Validar flujo y errores |
| Repositories | Completa | Verificar queries |
| Controllers | Completa | Verificar validaciones |
| Tests unitarios | Completa | Verificar cobertura |
| Tests integración | Con revisión | Validar escenarios |
| UI Components | Completa | Verificar UX |
| Estilos | Completa | Verificar consistencia |

### Evidencias Requeridas
- [ ] Tests unitarios pasan
- [ ] Tests integración pasan
- [ ] Cobertura > 80%
- [ ] Lint sin errores
- [ ] Build exitoso
- [ ] PR creado con template
- [ ] Review aprobado
- [ ] Merge a develop
- [ ] Deploy a staging
- [ ] PO validó en staging

### Criterios de Rechazo
- Cobertura < 80%
- Tests fallando
- Lint errors
- Violación de patrones documentados
- Vulnerabilidades de seguridad
- PR sin Policy Ticket referenciado
```

### 4.4 Checklist por Sprint

```
CHECKLIST DE SPRINT
═══════════════════════════════════════════════════════════════

SPRINT PLANNING
□ Objetivo del sprint definido
□ US seleccionadas y estimadas
□ Policy Tickets creados para US complejas
□ Dependencias identificadas
□ Capacidad del equipo confirmada

DURANTE EL SPRINT
□ Daily standup (foco en bloqueantes)
□ PRs creados con template
□ Code reviews en < 24h
□ CI/CD pasando
□ Staging actualizado

SPRINT REVIEW
□ Demo de funcionalidades completadas
□ PO valida criterios de aceptación
□ Feedback documentado
□ US marcadas como Done

SPRINT RETROSPECTIVE
□ Qué funcionó bien
□ Qué puede mejorar
□ Acciones concretas para próximo sprint
□ Actualizar documentación si necesario

MÉTRICAS DEL SPRINT
□ Velocity: [SP completados] / [SP comprometidos]
□ Bugs introducidos: [número]
□ Cobertura promedio: [%]
□ Lead time promedio: [horas/días]
```

---

## ★ FASE 5: Testing e Integración ★

### 5.1 Descripción del Proceso

```
┌─────────────────────────────────────────────────────────────────┐
│                FASE 5: TESTING E INTEGRACIÓN                    │
├─────────────────────────────────────────────────────────────────┤
│                                                                 │
│  PIRÁMIDE DE TESTING                                            │
│                                                                 │
│              ┌───────────┐                                      │
│              │   E2E     │  ← Pocos, costosos, lentos           │
│              │   Tests   │    Selenium, Cypress, Playwright     │
│             ─┴───────────┴─                                     │
│            ┌───────────────┐                                    │
│            │  Integration  │  ← Moderados                       │
│            │    Tests      │    API tests, DB tests             │
│           ─┴───────────────┴─                                   │
│          ┌───────────────────┐                                  │
│          │    Unit Tests     │  ← Muchos, rápidos, baratos      │
│          │                   │    Jest, Pytest                  │
│         ─┴───────────────────┴─                                 │
│                                                                 │
│  TIPOS DE TESTING                                               │
│  ┌──────────────────────────────────────────────────────────┐  │
│  │ • Unit Tests: Funciones y clases aisladas               │  │
│  │ • Integration Tests: Componentes conectados             │  │
│  │ • API Tests: Endpoints y contratos                      │  │
│  │ • E2E Tests: Flujos completos de usuario                │  │
│  │ • Performance Tests: Carga y estrés                     │  │
│  │ • Security Tests: SAST, DAST, penetration               │  │
│  │ • Accessibility Tests: WCAG compliance                  │  │
│  └──────────────────────────────────────────────────────────┘  │
│                                                                 │
│  FLUJO DE INTEGRACIÓN                                           │
│                                                                 │
│  Feature Branch → PR → Checks → Code Review → Merge             │
│                         │                                       │
│                         ▼                                       │
│              ┌─────────────────────┐                           │
│              │ CI Pipeline Checks  │                           │
│              ├─────────────────────┤                           │
│              │ □ Lint              │                           │
│              │ □ Type Check        │                           │
│              │ □ Unit Tests        │                           │
│              │ □ Integration Tests │                           │
│              │ □ Build             │                           │
│              │ □ Security Scan     │                           │
│              │ □ Coverage Check    │                           │
│              └─────────────────────┘                           │
│                                                                 │
└─────────────────────────────────────────────────────────────────┘
```

### 5.2 Artefactos de Testing

#### 10-ESTRATEGIA-TESTING.md

```markdown
# Estrategia de Testing

## Metadata
- **Proyecto:** [Nombre]
- **Versión:** 1.0

## Objetivos de Calidad

| Métrica | Objetivo | Mínimo Aceptable |
|---------|----------|------------------|
| Cobertura de código | 85% | 80% |
| Tests unitarios pasando | 100% | 100% |
| Tests integración pasando | 100% | 100% |
| Bugs críticos en prod | 0 | 0 |
| Tiempo de ejecución CI | < 10 min | < 15 min |

## Estrategia por Capa

### Backend

| Tipo | Herramienta | Cobertura | Responsable |
|------|-------------|-----------|-------------|
| Unit | Pytest | 90% domain/application | Desarrollador + IA |
| Integration | Pytest + TestClient | 80% endpoints | Desarrollador + IA |
| API Contract | Schemathesis | 100% OpenAPI | CI automático |

### Frontend

| Tipo | Herramienta | Cobertura | Responsable |
|------|-------------|-----------|-------------|
| Unit | Vitest | 80% utils/hooks | Desarrollador + IA |
| Component | Testing Library | 70% componentes | Desarrollador + IA |
| E2E | Playwright | Flujos críticos | QA + IA |

### Flujos E2E Críticos

1. **Registro y Login**
   - Registro exitoso
   - Login exitoso
   - Recuperación de contraseña

2. **[Flujo de negocio principal]**
   - [Paso 1]
   - [Paso 2]
   - [Paso 3]

## Ambientes de Testing

| Ambiente | Propósito | Datos | Refresh |
|----------|-----------|-------|---------|
| Local | Dev testing | Fixtures | Manual |
| CI | Automated tests | Fixtures | Cada PR |
| Staging | QA/UAT | Copia prod anonimizada | Semanal |

## Gestión de Datos de Test

### Fixtures
- Ubicación: `/tests/fixtures/`
- Formato: JSON/YAML
- Versionados con código

### Factory Pattern
```python
# tests/factories/user_factory.py
class UserFactory:
    @staticmethod
    def create(**overrides):
        defaults = {
            "email": f"test_{uuid4()}@example.com",
            "name": "Test User",
            "role": "user"
        }
        return User(**{**defaults, **overrides})
```

## Proceso de QA

```
Desarrollo → PR → CI Checks → Code Review → Merge → Staging → QA Manual → Prod
                                                         │
                                                         ▼
                                              ┌─────────────────────┐
                                              │ QA Checklist        │
                                              ├─────────────────────┤
                                              │ □ Criterios de AC   │
                                              │ □ Edge cases        │
                                              │ □ Regresión básica  │
                                              │ □ Cross-browser     │
                                              │ □ Responsive        │
                                              │ □ Accesibilidad     │
                                              └─────────────────────┘
```
```

### 5.3 Policy Tickets de Testing

```yaml
---
id: PT-FASE5-001
phase: "5-Testing"
type: "quality-assurance"
domain: testing
risk_level: low
---

## Policy Ticket: Generación de Tests con IA

### Intención
Utilizar IA para generar tests unitarios, de integración y E2E,
con validación humana de casos y cobertura.

### Autonomía
| Actividad | Autonomía IA | Validación |
|-----------|--------------|------------|
| Generación de unit tests | Completa | Revisar cobertura |
| Generación de integration tests | Completa | Revisar escenarios |
| Generación de E2E tests | Con revisión | Validar flujos críticos |
| Identificación de edge cases | Completa | Confirmar relevancia |
| Fixtures y factories | Completa | Revisar datos |
| Análisis de cobertura | Completa | Información |

### Evidencias
- [ ] Cobertura > objetivo definido
- [ ] Todos los tests pasan
- [ ] Edge cases identificados y cubiertos
- [ ] Flujos críticos E2E funcionando
```

---

## ★ FASE 6: Despliegue y Operación ★

### 6.1 Descripción del Proceso

```
┌─────────────────────────────────────────────────────────────────┐
│              FASE 6: DESPLIEGUE Y OPERACIÓN                     │
├─────────────────────────────────────────────────────────────────┤
│                                                                 │
│  PIPELINE DE DESPLIEGUE                                         │
│                                                                 │
│  develop ──────▶ staging ──────▶ production                    │
│      │              │               │                           │
│      ▼              ▼               ▼                           │
│  ┌───────┐     ┌───────┐      ┌───────┐                        │
│  │ Auto  │     │ Auto  │      │Manual │                        │
│  │Deploy │     │Deploy │      │Approve│                        │
│  └───────┘     └───────┘      └───────┘                        │
│                                    │                            │
│                                    ▼                            │
│                           ┌─────────────┐                       │
│                           │  Checklist  │                       │
│                           │  Pre-Deploy │                       │
│                           └─────────────┘                       │
│                                    │                            │
│                                    ▼                            │
│                           ┌─────────────┐                       │
│                           │   Deploy    │                       │
│                           │   + Smoke   │                       │
│                           └─────────────┘                       │
│                                    │                            │
│                                    ▼                            │
│                           ┌─────────────┐                       │
│                           │  Monitoreo  │                       │
│                           │  Post-Deploy│                       │
│                           └─────────────┘                       │
│                                                                 │
│  ARTEFACTOS DE OPERACIÓN                                        │
│  ┌──────────────────────────────────────────────────────────┐  │
│  │ • 11-INFRAESTRUCTURA-DEPLOY.md                           │  │
│  │ • docker-compose.yml (ambientes)                         │  │
│  │ • Terraform/IaC files                                    │  │
│  │ • RUNBOOK.md (procedimientos operativos)                 │  │
│  │ • INCIDENT-RESPONSE.md                                   │  │
│  │ • Dashboards de monitoreo                                │  │
│  │ • Alertas configuradas                                   │  │
│  └──────────────────────────────────────────────────────────┘  │
│                                                                 │
└─────────────────────────────────────────────────────────────────┘
```

### 6.2 Artefactos de Despliegue

#### 11-INFRAESTRUCTURA-DEPLOY.md

```markdown
# Infraestructura y Despliegue

## Metadata
- **Proyecto:** [Nombre]
- **Versión:** 1.0
- **Última actualización:** [YYYY-MM-DD]

## Ambientes

| Ambiente | URL | Propósito | Datos | Auto-deploy |
|----------|-----|-----------|-------|-------------|
| Development | localhost | Dev local | Fixtures | N/A |
| Staging | staging.example.com | QA/UAT | Copia anon. | Sí (develop) |
| Production | example.com | Producción | Real | No (manual) |

## Arquitectura de Infraestructura

```
┌─────────────────────────────────────────────────────────────────┐
│                         PRODUCCIÓN                              │
├─────────────────────────────────────────────────────────────────┤
│                                                                 │
│  ┌─────────────┐    ┌─────────────┐    ┌─────────────┐        │
│  │   CDN       │───▶│   Nginx     │───▶│  Frontend   │        │
│  │ (CloudFlare)│    │ (Proxy)     │    │  (Static)   │        │
│  └─────────────┘    └─────────────┘    └─────────────┘        │
│                            │                                    │
│                            ▼                                    │
│                     ┌─────────────┐                            │
│                     │   Backend   │                            │
│                     │  (FastAPI)  │                            │
│                     │  x2 replicas│                            │
│                     └─────────────┘                            │
│                       │         │                              │
│              ┌────────┘         └────────┐                     │
│              ▼                           ▼                     │
│       ┌─────────────┐             ┌─────────────┐             │
│       │  PostgreSQL │             │    Redis    │             │
│       │   (RDS)     │             │(ElastiCache)│             │
│       └─────────────┘             └─────────────┘             │
│                                                                 │
└─────────────────────────────────────────────────────────────────┘
```

## Docker Compose

```yaml
# docker-compose.prod.yml
version: '3.8'

services:
  frontend:
    build:
      context: ./frontend
      dockerfile: Dockerfile.prod
    ports:
      - "3000:80"
    depends_on:
      - backend

  backend:
    build:
      context: ./backend
      dockerfile: Dockerfile.prod
    ports:
      - "8000:8000"
    environment:
      - DATABASE_URL=${DATABASE_URL}
      - REDIS_URL=${REDIS_URL}
    depends_on:
      - db
      - redis

  db:
    image: postgres:15-alpine
    volumes:
      - postgres_data:/var/lib/postgresql/data
    environment:
      - POSTGRES_PASSWORD=${DB_PASSWORD}

  redis:
    image: redis:7-alpine
    volumes:
      - redis_data:/data

volumes:
  postgres_data:
  redis_data:
```

## Proceso de Deploy a Producción

### Pre-Deploy Checklist

```
PRE-DEPLOY CHECKLIST
═══════════════════════════════════════════════════════════════

□ Código
  □ Todos los PRs del release mergeados
  □ CI/CD verde en develop
  □ Release branch creado y taggeado
  □ CHANGELOG.md actualizado

□ Testing
  □ Tests pasan en staging
  □ QA sign-off obtenido
  □ Performance tests ejecutados
  □ Security scan limpio

□ Base de Datos
  □ Migraciones probadas en staging
  □ Rollback de migraciones verificado
  □ Backup de producción reciente

□ Infraestructura
  □ Capacidad verificada
  □ Certificados SSL vigentes
  □ Secrets actualizados si necesario

□ Comunicación
  □ Stakeholders notificados
  □ Ventana de mantenimiento acordada (si aplica)
  □ Runbook revisado

□ Rollback
  □ Plan de rollback documentado
  □ Versión anterior disponible
  □ Tiempo estimado de rollback: [X min]

APROBACIÓN PARA DEPLOY:
□ Tech Lead: _________________ Fecha: _______
□ QA Lead: _________________ Fecha: _______
□ Product Owner: _________________ Fecha: _______
```

### Proceso de Deploy

```bash
# 1. Crear release tag
git checkout develop
git pull
git tag -a v1.2.0 -m "Release 1.2.0"
git push origin v1.2.0

# 2. GitHub Actions ejecuta pipeline de producción
# (automático con tag)

# 3. Aprobación manual en GitHub (environment protection)

# 4. Deploy ejecutado

# 5. Smoke tests automáticos

# 6. Monitoreo post-deploy (30 min)
```

### Post-Deploy Checklist

```
POST-DEPLOY CHECKLIST
═══════════════════════════════════════════════════════════════

□ Verificación Inmediata (0-5 min)
  □ Aplicación responde
  □ Health check endpoints OK
  □ Login funciona
  □ Flujo crítico principal funciona

□ Monitoreo (5-30 min)
  □ Métricas normales (CPU, memoria, latencia)
  □ Sin errores anómalos en logs
  □ Tasas de error < baseline
  □ Alertas silenciosas

□ Validación (30-60 min)
  □ Smoke tests manuales completos
  □ Stakeholders notificados del éxito
  □ Documentación actualizada

□ Si hay problemas:
  □ Ejecutar runbook de troubleshooting
  □ Evaluar necesidad de rollback
  □ Comunicar a stakeholders
```

## Runbook de Operaciones

### RUNBOOK.md

```markdown
# Runbook de Operaciones

## Accesos

| Sistema | URL | Credenciales |
|---------|-----|--------------|
| AWS Console | console.aws.amazon.com | Ver 1Password |
| Logs (CloudWatch) | [link] | IAM role |
| Monitoreo (Datadog) | [link] | SSO |
| Base de datos | [endpoint] | Ver Secrets Manager |

## Comandos Frecuentes

### Ver logs en tiempo real
```bash
# Backend
aws logs tail /ecs/backend --follow

# Errores de última hora
aws logs filter-log-events \
  --log-group-name /ecs/backend \
  --filter-pattern "ERROR" \
  --start-time $(date -d '1 hour ago' +%s000)
```

### Reiniciar servicio
```bash
aws ecs update-service \
  --cluster production \
  --service backend \
  --force-new-deployment
```

### Escalar servicio
```bash
aws ecs update-service \
  --cluster production \
  --service backend \
  --desired-count 4
```

### Rollback
```bash
# 1. Obtener task definition anterior
aws ecs describe-services \
  --cluster production \
  --services backend

# 2. Actualizar a versión anterior
aws ecs update-service \
  --cluster production \
  --service backend \
  --task-definition backend:123  # versión anterior
```

## Troubleshooting

### Problema: Alta latencia

1. Verificar métricas de CPU/memoria
2. Verificar conexiones a BD
3. Verificar cache Redis
4. Revisar queries lentas
5. Escalar si necesario

### Problema: Errores 5xx

1. Revisar logs de errores
2. Identificar endpoint afectado
3. Verificar dependencias externas
4. Verificar migraciones
5. Evaluar rollback

### Problema: Base de datos lenta

1. Verificar conexiones activas
2. Identificar queries lentas (pg_stat_statements)
3. Verificar índices
4. Verificar locks
5. Considerar read replica
```
```

### 6.3 Policy Tickets de Despliegue

```yaml
---
id: PT-FASE6-001
phase: "6-Despliegue"
type: "deployment"
domain: infrastructure
risk_level: high
---

## Policy Ticket: Despliegue a Producción

### Intención
Desplegar release v[X.Y.Z] a producción con todas las validaciones
y aprobaciones requeridas.

### Pre-condiciones
- [ ] QA sign-off obtenido
- [ ] Todos los checks CI verdes
- [ ] Staging validado por PO
- [ ] Pre-deploy checklist completado

### Autonomía
| Actividad | Autonomía IA | Aprobación |
|-----------|--------------|------------|
| Build de imágenes | Completa | Automático |
| Deploy a staging | Completa | Automático |
| Deploy a producción | Prohibido | Manual obligatoria |
| Rollback | Prohibido | Manual obligatoria |
| Smoke tests | Completa | Verificación |
| Monitoreo | Completa | Alertas a humanos |

### Responsables
- Tech Lead: @nombre (aprueba deploy)
- QA Lead: @nombre (sign-off calidad)
- On-call: @nombre (monitoreo post-deploy)

### Plan de Rollback
1. Trigger: [condiciones que disparan rollback]
2. Comando: [comando de rollback]
3. Tiempo estimado: [X minutos]
4. Verificación: [cómo confirmar rollback exitoso]

### Evidencias Post-Deploy
- [ ] Health checks OK
- [ ] Smoke tests pasando
- [ ] Métricas estables por 30 min
- [ ] Sin alertas disparadas
- [ ] CHANGELOG.md actualizado
```

### 6.4 Checklist Final de Release

```
GATE DE SALIDA - RELEASE A PRODUCCIÓN
═══════════════════════════════════════════════════════════════

□ Calidad
  □ Todos los tests pasan
  □ Cobertura > 80%
  □ 0 bugs críticos conocidos
  □ QA sign-off

□ Seguridad
  □ Security scan limpio
  □ Dependencias actualizadas
  □ Secrets rotados si necesario

□ Documentación
  □ CHANGELOG.md actualizado
  □ API docs actualizados
  □ Runbook actualizado

□ Operaciones
  □ Pre-deploy checklist completado
  □ Backup de BD verificado
  □ Plan de rollback listo
  □ On-call asignado

□ Comunicación
  □ Release notes preparadas
  □ Stakeholders notificados

POST-DEPLOY
□ Smoke tests OK
□ Métricas estables 30 min
□ Stakeholders confirmados
□ Celebrar 🎉

═══════════════════════════════════════════════════════════════
RELEASE v[X.Y.Z] COMPLETADO

Fecha: _____________
Firmado: _____________
```

---

## ★ Resumen: Documentos por Fase ★

```
MATRIZ DE DOCUMENTACIÓN POR FASE
═══════════════════════════════════════════════════════════════

FASE 1: REQUISITOS
├── 01-VISION-OBJETIVOS.md
├── 02-USUARIOS-ROLES.md
├── 03-REQUISITOS-FUNCIONALES.md
├── 04-REQUISITOS-NO-FUNCIONALES.md
├── Backlog en herramienta de gestión
└── PT-FASE1-001 (Policy Ticket)

FASE 2: PLANIFICACIÓN
├── PLAN-RELEASES.md
├── PLAN-SPRINT-[N].md (por sprint)
├── REGISTRO-RIESGOS.md
├── Cronograma general
└── PT-FASE2-001 (Policy Ticket)

FASE 3: ARQUITECTURA
├── 05-ARQUITECTURA-STACK.md
├── 06-MODELO-DATOS.md
├── 07-DISENO-UI-UX.md
├── 08-SISTEMA-DISENO-ESTILOS.md
├── 09-PATRONES-CODIGO.md
├── /docs/adrs/ADR-XXX.md (por decisión)
├── Diagramas C4
└── PT-FASE3-001 (Policy Ticket)

FASE 4: DESARROLLO
├── Policy Tickets por US (PT-DEV-XX-XXX)
├── PRs con template estándar
├── Commits con convención
├── Code reviews documentados
└── Sprint retrospectives

FASE 5: TESTING
├── 10-ESTRATEGIA-TESTING.md
├── /tests/ (código de tests)
├── Reportes de cobertura
├── QA sign-offs
└── PT-FASE5-001 (Policy Ticket)

FASE 6: DESPLIEGUE
├── 11-INFRAESTRUCTURA-DEPLOY.md
├── RUNBOOK.md
├── INCIDENT-RESPONSE.md
├── docker-compose.[env].yml
├── /terraform/ (IaC)
├── CHANGELOG.md
└── PT-FASE6-001 (Policy Ticket)

═══════════════════════════════════════════════════════════════
```

---

# PARTE XII: CONCLUSIONES

## Conclusiones Generales

El desarrollo de software se encuentra en un punto de inflexión. La incorporación de inteligencia artificial con capacidad de acción directa no representa una simple mejora de productividad ni una herramienta más en el arsenal del desarrollador. Constituye un cambio estructural que altera la forma en que se concibe, organiza y controla el trabajo de construcción de sistemas digitales.

A lo largo de este documento se ha mostrado que las metodologías y prácticas tradicionales, en particular los enfoques ágiles, fueron diseñadas para coordinar trabajo humano. Sus artefactos, ceremonias y mecanismos de control presuponen capacidades y límites propios de las personas. Cuando estos supuestos dejan de cumplirse, el marco metodológico comienza a mostrar fisuras que no pueden resolverse mediante ajustes superficiales.

El marco de trabajo IA-Native propuesto surge como una respuesta a esta brecha. Su aporte central no reside en una herramienta específica ni en una técnica aislada, sino en la redefinición de la unidad básica del proceso: del trabajo entendido como tarea a la acción entendida como decisión de delegación gobernada. Este desplazamiento conceptual permite abordar de manera explícita cuestiones que en los enfoques tradicionales permanecen implícitas o informales, como la autorización de la acción, la clasificación del riesgo, la exigencia de evidencia y la asunción clara de responsabilidades.

La introducción del Policy Ticket como instrumento operativo materializa este enfoque y ofrece a las organizaciones un medio concreto para gobernar la actuación de la inteligencia artificial. Lejos de frenar la innovación, este instrumento permite escalar el uso de la IA de forma sostenible, evitando los efectos negativos de una adopción descontrolada. La experiencia muestra que la velocidad sin gobierno conduce a la pérdida de comprensión del sistema y al aumento del riesgo; el marco IA-Native propone un equilibrio deliberado entre velocidad y control.

El análisis de la transformación de roles, la simulación de un sprint completo y la discusión crítica de riesgos y trade-offs refuerzan la idea de que la adopción de IA en el desarrollo de software es, ante todo, una decisión organizacional y cultural. No se trata únicamente de qué tecnología se utiliza, sino de cómo se toman decisiones, cómo se distribuyen responsabilidades y cómo se aprende de la práctica real.

Asimismo, la proyección del marco en contextos educativos y de formación profesional subraya su alcance más amplio. Preparar a los profesionales para trabajar en entornos IA-Native implica enseñar no solo a programar, sino a gobernar procesos complejos mediados por sistemas artificiales. Este cambio en la formación resulta imprescindible para asegurar la continuidad y la calidad de la disciplina en los próximos años.

En síntesis, el marco de trabajo IA-Native ofrece una estructura coherente para enfrentar un desafío emergente de gran magnitud. No pretende ser una solución definitiva ni universal, sino un punto de partida sólido para repensar el desarrollo de software en un contexto donde la inteligencia artificial deja de ser un asistente y se convierte en un actor con capacidad de acción. Su valor reside en hacer explícito lo que ya está ocurriendo en la práctica y en proporcionar herramientas conceptuales y operativas para gobernar ese cambio con responsabilidad y criterio profesional.

---

## Regla Final del Framework

**La IA puede producir cambios, pero GitHub decide si esos cambios existen.**

Si un cambio no puede pasar por un PR gobernado, branch protection, checks automáticos, y aprobación humana proporcional al riesgo, entonces no es un cambio válido, aunque funcione.

**Si un cambio no puede ser gobernado, no debe ser integrado.**
